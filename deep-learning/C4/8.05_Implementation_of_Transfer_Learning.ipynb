{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15F8jGyUdtrG"
   },
   "source": [
    "## __Transfer Learning__\n",
    "- Transfer learning refers to a technique in machine learning where a pre-trained model, typically trained on a large dataset, is used as a starting point for solving a different but related task.\n",
    "- It involves using models trained on one problem as a starting point for a related problem.\n",
    "- It is flexible, allowing the use of pre-trained models directly, as feature extraction preprocessing, and integrated into entirely new models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgCmoesorvHh"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing the required libraries\n",
    "2. Adding classifier layers\n",
    "3. Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKMViz1Ngp8Y"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- The **from tensorflow.keras.utils import load_img** is used to load an image file from the file system.\n",
    "\n",
    "- The **from tensorflow.keras.utils import img_to_array** is used to convert an image loaded with load_img into a NumPy array.\n",
    "\n",
    "- The **from keras.applications.vgg16 import preprocess_input** is used to preprocess the input image array before feeding it to the VGG16 model. VGG16 expects the input images to be preprocessed in a specific way.\n",
    "\n",
    "- The **from keras.applications.vgg16 import VGG16** is used to import the VGG16 model architecture. VGG16 is a popular convolutional neural network model pre-trained on the ImageNet dataset for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5woHmJrJdtrK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_860Ql8kwv-"
   },
   "source": [
    "### Step 2: Adding Classifier Layers\n",
    "- It demonstrates how to load a pre-trained VGG16 model without its classifier layers and then add new custom classifier layers on top of it.\n",
    "- The new model is defined by connecting the output of the pre-trained VGG16 model to a flatten layer, followed by a dense layer with 1024 units and ReLU activation, and finally a dense layer with 10 units and softmax activation for multi-class classification.\n",
    "- The model summary provides an overview of the architecture and layer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h95JEG-VdtrM",
    "outputId": "943b0380-f982-4dcc-ba94-fd71ce47573e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              42468352  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,193,290\n",
      "Trainable params: 57,193,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(10, activation='softmax')(class1)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3tLfn7WdtrN"
   },
   "source": [
    "**Observation**\n",
    "- Running the example defines the new model ready for training and summarizes the model architecture.\n",
    "- We have flattened the output of the last pooling layer and added our new fully connected layers.\n",
    "-The weights of the VGG16 model and the weights for the new model will all be trained together on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DWAS8owlYbP"
   },
   "source": [
    "### Step 3: Preprocessing and Feature Extraction\n",
    "- The image is loaded from a file and preprocessed to meet the input requirements of the VGG16 model (resizing, converting to a numpy array, and reshaping).\n",
    "\n",
    "- The modified model is used to predict and extract features from the input image, resulting in a feature vector with a specific shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pn9hQjZCdtrN",
    "outputId": "e1f032bc-a048-493d-dba5-8d9dd2a65205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467096/553467096 [==============================] - 5s 0us/step\n",
      "1/1 [==============================] - 1s 722ms/step\n",
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "image = load_img('dog.jpg', target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "model = VGG16()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "features = model.predict(image)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jwvk_m95mR7D"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "- The VGG16 model weights are downloaded and loaded successfully, and the extracted features from the input image have a shape of (1, 4096)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfLnQBrHdtrO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
