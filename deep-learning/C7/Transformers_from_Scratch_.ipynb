{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/C7/Transformers_from_Scratch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btxy_uFPT_zi",
        "outputId": "2e35cc03-02e3-41fe-8b5c-bb0e82904148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Collecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.51.1)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.29.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (14.0.6)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-23.1.4 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow_text-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcLl_PPeHeE4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import (Add, Dense, Layer, Dropout, Embedding, LayerNormalization,\n",
        "                                     MultiHeadAttention, TextVectorization, StringLookup)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "import tensorflow_text as tf_text\n",
        "_AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJrVGWbcILp6",
        "outputId": "811da5b4-871f-428a-f503-0c00083f7a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-08 16:06:27--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2023-01-08 16:08:41--  (try: 2)  https://www.manythings.org/anki/fra-eng.zip\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2023-01-08 16:10:53--  (try: 3)  https://www.manythings.org/anki/fra-eng.zip\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6720195 (6.4M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   6.41M  13.2MB/s    in 0.5s    \n",
            "\n",
            "2023-01-08 16:12:05 (13.2 MB/s) - ‘fra-eng.zip’ saved [6720195/6720195]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  DATA_FNAME = 'fra.txt'\n",
        "  BATCH_SIZE = 256\n",
        "\n",
        "  SOURCE_VOCAB_SIZE = 10000\n",
        "  TARGET_VOCAB_SIZE = 10000\n",
        "\n",
        "  MAX_POS_ENCODING = 256 # define the max positions in source and target\n",
        "\n",
        "  ENCODER_NUM_LAYERS = 2 # number of layers for encoders\n",
        "  DECODER_NUM_LAYERS = 2 # number of layers for encoders\n",
        "\n",
        "  #Define the dimensions of models\n",
        "  D_MODEL = 128\n",
        "\n",
        "  #Number of units in FFNN\n",
        "  DFF = 256\n",
        "\n",
        "  #define the number of heads\n",
        "  NUM_HEADS = 4\n",
        "  DROP_RATE = 0.1\n",
        "\n",
        "  #Number of epochs to train\n",
        "  EPOCHS = 25\n",
        "\n",
        "  #Define the output directory\n",
        "  OUTPUT_DIR = 'output'\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "0q5YXi_-IpaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exp1 = Config()\n",
        "# exp1.EPOCHS = 30"
      ],
      "metadata": {
        "id": "GHIty9jBK2u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Attention Layers"
      ],
      "metadata": {
        "id": "smu1E63qLOjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().init()\n",
        "    # Layer Definitions\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = LayerNormalization()\n",
        "    self.add = Add()\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attentionOuputs = self.mha(query=x,\n",
        "                               key = x,\n",
        "                               value=x)\n",
        "    x = self.add([x,attentionOuputs])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self,x,context):\n",
        "    (attentionOutputs,attentionScores) = self.mha(query=x,\n",
        "                                                  key=context,\n",
        "                                                  value=context,\n",
        "                                                  return_attention_scores=True)\n",
        "    x = self.add([x,attentionOutputs])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class CausalAttention(BaseAttention): # Masked Attention\n",
        "  def call(self,x):\n",
        "    attentionOutputs = self.mha(query=x,\n",
        "                                key=x,\n",
        "                                value=x,\n",
        "                                use_causal_mask=True)\n",
        "    x = self.add([x,attentionOutputs])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Vl1AsjUELMjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def load_data(file_name):\n",
        "  with open(file_name, 'rt') as textFile:\n",
        "    lines = textFile.readlines()\n",
        "    pairs = [line.split(\"\\t\")[:-1] for line in lines]\n",
        "    random.shuffle(pairs)\n",
        "    source = [src for src,_ in pairs]\n",
        "    target = [trgt for _,trgt in pairs]\n",
        "  return (source,target)\n",
        "\n",
        "def splitting_dataset(source,target):\n",
        "  trainSize = int(len(source) * 0.8)\n",
        "  valsize = int(len(source) * 0.1)\n",
        "\n",
        "  (trainSource,trainTarget) = (source[:trainSize],target[:trainSize])\n",
        "  (valSource,valTarget) = (source[trainSize:trainSize+valsize], target[trainSize:trainSize+valsize])\n",
        "  (testSource,testTarget) = (source[trainSize+valsize :], target[trainSize+valsize :])\n",
        "\n",
        "  return (\n",
        "      (trainSource,trainTarget),\n",
        "      (valSource,valTarget),\n",
        "      (testSource,testTarget)\n",
        "  )"
      ],
      "metadata": {
        "id": "2aY9xEp2LA4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source,target = load_data(file_name=config.DATA_FNAME)"
      ],
      "metadata": {
        "id": "xhDFNTXaQE7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mu4vbUqRQoD",
        "outputId": "e98e10a9-228b-494f-9054-e9c923ac36d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is my last project.',\n",
              " 'When I heard the news, I wanted to cry.',\n",
              " 'I have to tell you something.',\n",
              " \"It's too late to go now.\",\n",
              " 'Tom got married to a widow.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK6bpW4MRaIZ",
        "outputId": "fdf9d801-d201-497f-9089-86b9d6f794b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"C'est mon dernier projet.\",\n",
              " \"Quand j'ai appris la nouvelle, j'ai eu envie de pleurer.\",\n",
              " 'Je dois vous dire quelque chose.',\n",
              " 'Il est maintenant trop tard pour se mettre en route.',\n",
              " 'Tom a épousé une veuve.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(splits, batchSize,sourceTextProcessor, targetTextProcessor, train=False):\n",
        "  (source,target) = splits\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((source,target))\n",
        "\n",
        "  def prepare_batch(source,target):\n",
        "    source = sourceTextProcessor(source)\n",
        "    targetBuffer = targetTextProcessor(target)\n",
        "    targetInput = targetBuffer[:,:-1]\n",
        "    targetOutput = targetBuffer[:,1:]\n",
        "    return (source,targetInput), targetOutput\n",
        "\n",
        "  if train:\n",
        "    dataset = (\n",
        "        dataset.shuffle(dataset.cardinality().numpy())\n",
        "        .batch(batchSize)\n",
        "        .map(prepare_batch,_AUTO)\n",
        "        .prefetch(_AUTO)\n",
        "    )\n",
        "  else:\n",
        "    dataset = dataset.batch(batchSize).map(prepare_batch,_AUTO).prefetch(_AUTO)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "b42zTmsrRoZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "    # split accented characters\n",
        "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
        "    text = tf.strings.lower(text)\n",
        "\n",
        "    # keep space, a to z, and selected punctuations\n",
        "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,]\", \"\")\n",
        "\n",
        "    # add spaces around punctuation\n",
        "    text = tf.strings.regex_replace(text, \"[.?!,]\", r\" \\0 \")\n",
        "\n",
        "    # strip whitespace and add [START] and [END] tokens\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join([\"[START]\", text, \"[END]\"], separator=\" \")\n",
        "\n",
        "    # return the processed text\n",
        "    return text"
      ],
      "metadata": {
        "id": "enPiomFTTgAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "source,target = load_data(file_name=config.DATA_FNAME)\n",
        "\n",
        "# Split the dataset\n",
        "(train,val,test) = splitting_dataset(source=source,target=target)\n",
        "\n",
        "# Create Text processing layer\n",
        "\n",
        "sourceTextProcessor = TextVectorization(standardize = tf_lower_and_split_punct ,\n",
        "                                        max_tokens=config.SOURCE_VOCAB_SIZE)\n",
        "sourceTextProcessor.adapt(train[0]) # source text data\n",
        "\n",
        "TargetTextProcessor = TextVectorization(standardize = tf_lower_and_split_punct ,\n",
        "                                        max_tokens=config.TARGET_VOCAB_SIZE)\n",
        "TargetTextProcessor.adapt(train[1]) # target text data"
      ],
      "metadata": {
        "id": "_bvze20gUl6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDs = make_dataset(\n",
        "    splits=train,\n",
        "    batchSize=config.BATCH_SIZE,\n",
        "    train=True,\n",
        "    sourceTextProcessor=sourceTextProcessor,\n",
        "    targetTextProcessor=TargetTextProcessor\n",
        ")\n",
        "\n",
        "validDs = make_dataset(\n",
        "    splits=val,\n",
        "    batchSize=config.BATCH_SIZE,\n",
        "    train=False,\n",
        "    sourceTextProcessor=sourceTextProcessor,\n",
        "    targetTextProcessor=TargetTextProcessor\n",
        ")\n",
        "\n",
        "testDs = make_dataset(\n",
        "    splits=test,\n",
        "    batchSize=config.BATCH_SIZE,\n",
        "    train=False,\n",
        "    sourceTextProcessor=sourceTextProcessor,\n",
        "    targetTextProcessor=TargetTextProcessor\n",
        ")"
      ],
      "metadata": {
        "id": "aM-tkZoCV0cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opRW6M4aWjFA",
        "outputId": "05fc7b8b-b357-4b8d-cdfc-6947d0207760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=((TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None)), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Layer):\n",
        "  def __init__(self,dff, dModel, dropoutRate=0.1, **kwargs ):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    #Sequential\n",
        "    self.seq = Sequential([\n",
        "        Dense(units=dff, activation='relu'),\n",
        "        Dense(units=dModel),\n",
        "        Dropout(rate=dropoutRate)\n",
        "    ])\n",
        "\n",
        "    self.add = Add()\n",
        "    self.layernorm = LayerNormalization()\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.add([x,self.seq(x)])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "eP-B4nHBXd0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "ltV_8uTyfrNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(Layer):\n",
        "\n",
        "  def __init__(self, dModel, numHeads, dff, dropoutRate=0.1, **kwargs):\n",
        "\n",
        "    super.__init__(**kwargs)\n",
        "\n",
        "    self.globalSelfAttention = GlobalSelfAttention(\n",
        "        num_heads = numHeads,\n",
        "        key_dim = dModel//numHeads,\n",
        "        dropout = dropoutRate\n",
        "    )\n",
        "\n",
        "    self.ffn = FeedForward(dff=dff, dModel = dModel, dropoutRate = dropoutRate)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.globalSelfAttention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(Layer):\n",
        "  def __init__(self,\n",
        "               numLayers,\n",
        "               dModel,\n",
        "               numHeads,\n",
        "               sourceVocabSize,\n",
        "               maximumPositionEncoding,\n",
        "               dff, dropoutRate, **kwargs):\n",
        "    super.__init__(**kwargs)\n",
        "    self.dModel = dModel\n",
        "    self.numLayers = numLayers\n",
        "\n",
        "    self.positionalEmbedding = PositionalEmbedding(vocabSize = sourceVocabSize, dModel = dModel,\n",
        "                                                   maximumPositionEncoding = maximumPositionEncoding)\n",
        "\n",
        "    self.encoderLayers = [\n",
        "        EncoderLayer(dModel = dModel, dff = dff, numHeads = numHeads, dropoutRate = dropoutRate) for _ in range(numLayers)\n",
        "    ]\n",
        "\n",
        "    self.dropout = Dropout(rate=dropoutRate)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.positionalEmbedding(x)\n",
        "    x = self.dropout(x)\n",
        "    for encoderLayer in self.encoderLayers:\n",
        "      x = encoderLayer(x=x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yWwgNRFxfqih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length,depth):\n",
        "  depth = depth/2\n",
        "  positions = np.arange(length)[:,np.newaxis]\n",
        "  depths = np.arange(depth)[np.newaxis,:]/depth\n",
        "\n",
        "  angleRates = 1/(10000**depths)\n",
        "  angleRads = positions * angleRates\n",
        "  posEncoding = np.concatenate([np.sin(angleRads), np.cos(angleRads)],axis=-1)\n",
        "  return tf.cast(posEncoding,dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(Layer):\n",
        "  def __init__(self,vocabSize,dModel, maximomPositonEncoding,**kwargs):\n",
        "    super.__init__(**kwargs)\n",
        "\n",
        "    self.embedding = Embedding(\n",
        "        input_dim = vocabSize, output_dim = dModel,mask_zero=True\n",
        "    )\n",
        "\n",
        "    self.posEncoding = position_encoding(length = maximumPositionEncoding,\n",
        "                                         depth=dModel)\n",
        "    self.dModel = dModel\n",
        "\n",
        "  def call(self,x):\n",
        "    seqLen = tf.shape(x)[-1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.DModel,tf.float32))\n",
        "    x += self.posEncoding[tf.newaxis, :seqLen, : ]\n",
        "    return x"
      ],
      "metadata": {
        "id": "GzwDRhWjhDo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQcUz4WJlJ83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}