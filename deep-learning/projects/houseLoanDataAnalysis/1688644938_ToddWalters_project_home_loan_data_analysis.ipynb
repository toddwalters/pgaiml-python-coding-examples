{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/projects/houseLoanDataAnalysis/1688644938_ToddWalters_project_home_loan_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY8iESXrFnl4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXqTWV3iFnl4"
   },
   "source": [
    "# <a id='toc1_'></a>[**Loan Default Prediction using Deep Learning**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ7XucjeFnl5"
   },
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "For a safe and secure lending experience, it's important to analyze the past data. In this project, you have to build a deep learning model to predict the chance of default for future loans using the historical data. As you will see, this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The main objective of this project is to create a deep learning model that can accurately predict whether an applicant will be able to repay a loan based on historical data. This involves:\n",
    "\n",
    "1. Analyzing and preprocessing the given dataset\n",
    "2. Handling imbalanced data\n",
    "3. Building and training a deep learning model\n",
    "4. Evaluating the model using appropriate metrics\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The dataset contains historical loan application data. It includes various features about loan applicants and a target variable indicating whether the loan was repaid or defaulted. The data is highly imbalanced, which presents an additional challenge for model training and evaluation.\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "1. Load the dataset\n",
    "2. Check for null values in the dataset\n",
    "3. Analyze the distribution of the target variable (loan default rate)\n",
    "4. Balance the dataset\n",
    "5. Visualize the balanced/imbalanced data\n",
    "6. Preprocess and encode the features\n",
    "7. Build and train a deep learning model\n",
    "8. Evaluate the model using Sensitivity and ROC AUC metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHz_5EEfFnl5"
   },
   "source": [
    "## <a id='toc1_5_'></a>[**1.0 Load The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqVKXKtFnl5"
   },
   "source": [
    "### <a id='toc1_5_1_'></a>[**1.1 Setup: Import Necessary Libraries**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtYfv5plFnl5"
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn tensorflow category_encoders imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kxMwRpFnl6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = '/Users/toddwalters/Development/data/1688644938_dataset/1688644938_loan_data.csv'\n",
    "# Uncomment the following line to run in Google Colab\n",
    "# dataset_file = '/content/drive/MyDrive/Colab Notebooks/datasets/1688644938_dataset/1688644938_loan_data.csv'\n",
    "\n",
    "imputed_dataset_file = '/Users/toddwalters/Development/data/1688644938_dataset/1688644938_loan_data_imputed.csv'\n",
    "# Uncomment the following line to run in Google Colab\n",
    "# imputed_dataset_file = '/content/drive/MyDrive/Colab Notebooks/datasets/1688644938_dataset/1688644938_loan_data_imputed.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNDE8nqrFnl6"
   },
   "source": [
    "### <a id='toc1_5_2_'></a>[**1.2 Loading The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq_ODKMLFnl6"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0LPdLeSH338"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows and basic information about the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3wvj5HEH338"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzpqC3fOH338"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyyZjEjPH338"
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsyYHyw2H338"
   },
   "outputs": [],
   "source": [
    "# print(f\"\\nThe df.describe output is:\\n\")\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(df.describe().transpose())\n",
    "\n",
    "print(f\"\\nThe df.describe output is:\\n\")\n",
    "print(df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6FK_4qtH338"
   },
   "source": [
    "## <a id='toc1_6_'></a>[**2.0  Check for null values in the dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCPEGQMlH338"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull values in the dataset:\")\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3kHiD8VH339"
   },
   "source": [
    "### <a id='toc1_6_1_'></a>[**2.1  Drop Features With More Than 100K Null Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePwj7DzjH339"
   },
   "outputs": [],
   "source": [
    "# Dropping features with large amounts of missing data (any feature missing more than 100K of null values)\n",
    "df.drop(columns=[\n",
    "    'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRIxMBsiH339"
   },
   "source": [
    "### <a id='toc1_6_3_'></a>[**2.3 KNN Imputation of Numerical Features**](#toc0_)\n",
    "\n",
    "1. **`create_availability_flags` Function**\n",
    "  \n",
    "    - **Input**: DataFrame and a list of column names\n",
    "    - **Process**:\n",
    "      - For each specified column, creates a new column with the suffix \"_FLAG\"\n",
    "      - Flag is 1 if the original value is not null, 0 if null\n",
    "    - **Purpose**: Tracks which values were originally missing before imputation\n",
    "\n",
    "2. **Creation of Flags for Credit Bureau Features**\n",
    "\n",
    "    - Defines a list of credit bureau features\n",
    "    - Calls `create_availability_flags` to create flag columns for these features\n",
    "    - Displays the resulting flag columns using `df.info()`\n",
    "\n",
    "3. **`knn_impute_features` Function**\n",
    "\n",
    "    - Scales input features using StandardScaler\n",
    "    - Performs KNN imputation using sklearn's KNNImputer\n",
    "    - Scales imputed values back to original range\n",
    "    - Updates original DataFrame with imputed values\n",
    "\n",
    "4. **KNN Imputation Process**\n",
    "\n",
    "    - Creates a list of all features to impute (credit bureau features and others)\n",
    "    - Displays DataFrame info before imputation\n",
    "    - Performs KNN imputation using `knn_impute_features` function\n",
    "    - Saves imputed DataFrame to CSV file\n",
    "    - Displays DataFrame info after imputation\n",
    "\n",
    "5. **Summary Statistics**\n",
    "\n",
    "   - For each imputed feature, prints summary statistics using `describe()`\n",
    "\n",
    "6. **Visualization**\n",
    "\n",
    "    - Creates a grid of subplots, one for each imputed feature\n",
    "    - Calculates number of rows and columns based on feature count\n",
    "    - For each feature:\n",
    "      - Plots a histogram showing distribution after imputation\n",
    "    - Removes any unused subplots\n",
    "    - Displays the resulting plot\n",
    "\n",
    "**Overall Approach**\n",
    "\n",
    "This code provides a comprehensive method for handling missing data:\n",
    "\n",
    "1. Flags originally missing values (Section 2.2)\n",
    "2. Imputes missing values using KNN\n",
    "3. Provides summary statistics of imputed data\n",
    "4. Visualizes distribution of imputed features\n",
    "\n",
    "This approach ensures transparency in the imputation process and aids in understanding the impact of imputation on data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Onh_a9ipH339"
   },
   "source": [
    "#### <a id='toc1_6_3_1_'></a>[**2.3.1 Create Feature Flag Categories**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgiyTPy7H339"
   },
   "outputs": [],
   "source": [
    "def create_availability_flags(df, columns):\n",
    "    \"\"\"\n",
    "    Create binary flags indicating data availability for specified columns.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param columns: list of column names to create flags for\n",
    "    :return: DataFrame with added flag columns\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        flag_col_name = f\"{col}_FLAG\"\n",
    "        df[flag_col_name] = (~df[col].isnull()).astype(int)\n",
    "    return df\n",
    "\n",
    "# List of AMT_REQ_CREDIT_BUREAU features\n",
    "credit_bureau_features = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "]\n",
    "# Create availability flags\n",
    "df = create_availability_flags(df, credit_bureau_features)\n",
    "\n",
    "# Display info for flag columns\n",
    "print(\"\\nFlag columns:\")\n",
    "print(df[[col + '_FLAG' for col in credit_bureau_features]].info())\n",
    "print(f\"\\nFlag Feature Head:\")\n",
    "print(df[[col + '_FLAG' for col in credit_bureau_features]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJVlSluNH339"
   },
   "source": [
    "#### <a id='toc1_6_3_2_'></a>[**2.3.2 KNN Feature Imputation**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the features which were imputed by this function are not normalized in the returned dataframe. Let me explain why:\n",
    "\n",
    "1. The function does use normalization (specifically, standardization using `StandardScaler`) as part of the imputation process. This is done because KNN imputation works better with scaled data.\n",
    "\n",
    "2. However, after the imputation is performed, the function explicitly converts the data back to its original scale using `scaler.inverse_transform(imputed_features)`.\n",
    "\n",
    "3. The DataFrame is then updated with these inverse-transformed (i.e., original scale) values.\n",
    "\n",
    "So, the process goes like this:\n",
    "\n",
    "    Original data → Scaled data → Imputed scaled data → Inverse scaled (original scale) imputed data\n",
    "\n",
    "The final step ensures that the imputed values are on the same scale as the original data. This is important because it maintains the interpretability of the features and ensures consistency with any non-imputed values in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXXaBYfjH339"
   },
   "outputs": [],
   "source": [
    "def knn_impute_features(df, features, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Perform KNN imputation on specified columns.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param features: list of column names to impute\n",
    "    :param n_neighbors: number of neighbors to use for KNN imputation\n",
    "    :return: DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Prepare data for KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Scale the features\n",
    "    scaled_features = scaler.fit_transform(df[features])\n",
    "\n",
    "    # Perform KNN imputation\n",
    "    imputed_features = imputer.fit_transform(scaled_features)\n",
    "\n",
    "    # Convert back to original scale\n",
    "    imputed_features = scaler.inverse_transform(imputed_features)\n",
    "\n",
    "    # Update the DataFrame with imputed values\n",
    "    for i, col in enumerate(features):\n",
    "        df[col] = imputed_features[:, i]\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of all features that need KNN imputation\n",
    "all_features_to_impute = credit_bureau_features + [\n",
    "    'EXT_SOURCE_3',\n",
    "    'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "    'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "    'EXT_SOURCE_2'\n",
    "    # Add any other features that need imputation here\n",
    "]\n",
    "\n",
    "# Display info before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Check if the imputed file exists\n",
    "if os.path.exists(imputed_dataset_file):\n",
    "    print(\"Found pre-existing data imputation file.  Loading imputed data from file...\")\n",
    "    df = pd.read_csv(imputed_dataset_file)\n",
    "else:\n",
    "    print(\"Performing KNN imputation...\")\n",
    "    # Perform KNN imputation\n",
    "    df = knn_impute_features(df, all_features_to_impute)\n",
    "\n",
    "# Save the imputed dataframe if needed\n",
    "df.to_csv(imputed_dataset_file, index=False)\n",
    "\n",
    "print(\"\\nImputation complete. The specified features have been updated in the main dataframe.\\n\")\n",
    "\n",
    "# Display info after imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in all_features_to_impute:\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())\n",
    "\n",
    "# Calculate the number of rows and columns needed for the subplots\n",
    "n_features = len(all_features_to_impute)\n",
    "n_cols = min(3, n_features)  # Max 3 columns\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axs = axs.ravel() if n_features > 1 else [axs]\n",
    "\n",
    "# Plot histograms for each feature\n",
    "for i, feature in enumerate(all_features_to_impute):\n",
    "    axs[i].hist(df[feature], bins=50)\n",
    "    axs[i].set_title(f'Distribution of {feature}\\nafter imputation')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNcnxkerH339"
   },
   "source": [
    "### <a id='toc1_6_4_'></a>[**2.4 Imputation of Missing Categorical Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgYOWZ-hH339"
   },
   "outputs": [],
   "source": [
    "def impute_categorical(df, column_name, strategy='proportional', missing_value=None, new_category_name='Unknown', distribute_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Impute missing values in a categorical column using various strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column_name: str, name of the column to impute\n",
    "    - strategy: str, one of 'proportional', 'new_category', 'mode'\n",
    "    - missing_value: value to be considered as missing (if None, will use pd.isnull())\n",
    "    - new_category_name: str, name of the new category if using 'new_category' strategy\n",
    "    - distribute_ratio: float, ratio of missing values to distribute when using 'proportional' strategy\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify missing values\n",
    "    if missing_value is None:\n",
    "        missing_mask = df[column_name].isnull()\n",
    "    else:\n",
    "        missing_mask = df[column_name] == missing_value\n",
    "\n",
    "    missing_count = missing_mask.sum()\n",
    "\n",
    "    if strategy == 'proportional':\n",
    "        # Calculate the distribution of non-missing values\n",
    "        value_counts = df[~missing_mask][column_name].value_counts()\n",
    "        total_non_missing = value_counts.sum()\n",
    "\n",
    "        # Calculate the number of values to distribute\n",
    "        distribute_count = int(missing_count * distribute_ratio)\n",
    "        other_count = missing_count - distribute_count\n",
    "\n",
    "        # Calculate the number of new values for each category\n",
    "        new_values = (value_counts / total_non_missing * distribute_count).round().astype(int)\n",
    "\n",
    "        # Adjust to ensure we have exactly the right number\n",
    "        while new_values.sum() + other_count < missing_count:\n",
    "            new_values[new_values.idxmax()] += 1\n",
    "        while new_values.sum() + other_count > missing_count:\n",
    "            new_values[new_values.idxmax()] -= 1\n",
    "\n",
    "        # Create a list of all new values\n",
    "        all_new_values = []\n",
    "        for category, count in new_values.items():\n",
    "            all_new_values.extend([category] * count)\n",
    "\n",
    "        # Add the new category for remaining values\n",
    "        all_new_values.extend([new_category_name] * other_count)\n",
    "\n",
    "        # Shuffle and assign new values\n",
    "        np.random.shuffle(all_new_values)\n",
    "        df.loc[missing_mask, column_name] = all_new_values\n",
    "\n",
    "    elif strategy == 'new_category':\n",
    "        # Simply replace all missing values with the new category name\n",
    "        df.loc[missing_mask, column_name] = new_category_name\n",
    "\n",
    "    elif strategy == 'mode':\n",
    "        # Replace missing values with the most frequent category\n",
    "        mode_value = df[~missing_mask][column_name].mode().iloc[0]\n",
    "        df.loc[missing_mask, column_name] = mode_value\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose 'proportional', 'new_category', or 'mode'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB9W-kniH339"
   },
   "source": [
    "#### <a id='toc1_6_4_1'></a>[**2.4.1 Identify Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCNZ5ClDH33-"
   },
   "outputs": [],
   "source": [
    "def display_object_features_with_nulls(df):\n",
    "    \"\"\"\n",
    "    Display object features with non-zero null values, their null counts, and percentages.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Get null counts for object features only\n",
    "    object_features = df.select_dtypes(include=['object'])\n",
    "    null_counts = object_features.isnull().sum()\n",
    "    \n",
    "    # Filter for non-zero nulls and sort\n",
    "    non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if non_zero_nulls.empty:\n",
    "        print(\"No object features found with null values.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_rows = len(df)\n",
    "    percentages = (non_zero_nulls / total_rows) * 100\n",
    "    \n",
    "    # Combine information\n",
    "    combined_info = pd.DataFrame({\n",
    "        'Null Count': non_zero_nulls,\n",
    "        'Null Percentage': percentages,\n",
    "        'Data Type': df[non_zero_nulls.index].dtypes\n",
    "    })\n",
    "    \n",
    "    print(\"Object features with non-zero null values:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.float_format', '{:.2f}%'.format):\n",
    "        print(combined_info)\n",
    "    \n",
    "    # Optional: Visualize the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    combined_info['Null Percentage'].plot(kind='bar')\n",
    "    plt.title(\"Percentage of Null Values in Object Features\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Percentage of Null Values\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "display_object_features_with_nulls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GIeIehLH33-"
   },
   "outputs": [],
   "source": [
    "print(f\"List of Features of Type Object:\\n\")\n",
    "print(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "print(f\"\\nUnique values in OCCUPATION_TYPE: {df['OCCUPATION_TYPE'].unique()}\")\n",
    "print(f\"\\nUnique values in NAME_TYPE_SUITE: {df['NAME_TYPE_SUITE'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgpG1c45H33-"
   },
   "source": [
    "#### <a id='toc1_6_4_2_'></a>[**2.4.2 Investigate Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75-FWq-IH33-"
   },
   "source": [
    "##### <a id='toc1_6_4_2_1_'></a>[**2.4.2.1 Imputation of Missing Categorical Data Within `OCCUPATION_TYPE` Feature**](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Fill missing values with 'Missing' to include them in the histogram\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "df = df.copy()\n",
    "\n",
    "df['OCCUPATION_TYPE'].fillna('Missing', inplace=True)\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['OCCUPATION_TYPE']\n",
    "\n",
    "# Plotting\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given this distribution, here's an approach to distribute the missing values across the other categories while maintaining the overall structure of the data:\n",
    "\n",
    "1. We'll use a proportional distribution method, but with a slight modification to account for the large number of missing values.\n",
    "2. Instead of directly distributing all missing values, we'll distribute a portion of them (e.g., 80%) across the existing categories based on their current proportions. This helps maintain the general distribution while not overly inflating the existing categories.\n",
    "3. The remaining portion (e.g., 20%) will be assigned to a new category called \"Other\" or \"Unspecified\". This accounts for the possibility that some of these missing values might genuinely be unknown or not fit into existing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UhdT92NH33-"
   },
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation for OCCUPATION_TYPE\n",
    "df = impute_categorical(df, 'OCCUPATION_TYPE', strategy='proportional',\n",
    "                        missing_value='Missing', new_category_name='Other',\n",
    "                        distribute_ratio=0.8)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['OCCUPATION_TYPE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_An4_EaqH33-"
   },
   "source": [
    "##### <a id='toc1_6_4_2_2_'></a>[**2.4.2.2 Imputation of Missing Categorical Date Within `NAME_TYPE_SUITE` Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Fill missing values with 'Missing' to include them in the histogram\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "df = df.copy()\n",
    "\n",
    "df['NAME_TYPE_SUITE'].fillna('Missing', inplace=True)\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['NAME_TYPE_SUITE']\n",
    "\n",
    "# Plotting\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this distribution, here's an approach to distribute the missing values across the other categories while maintaining the overall structure of the data:\n",
    "\n",
    "1. Using the 'proportional' strategy to distribute the missing values across existing categories based on their current proportions.\n",
    "2. Setting distribute_ratio=0.95, which means 95% of the missing values will be distributed proportionally among existing categories, and only 5% will be assigned to the new 'Other' category.\n",
    "3. This approach will:\n",
    "\n",
    "    - Maintain the overall distribution of the data.\n",
    "    - Assign most of the missing values to 'Unaccompanied', reflecting the dominant trend in the data.\n",
    "    - Still allow for some diversity by assigning smaller portions to other categories.\n",
    "    - Create a small 'Other' category to account for any truly unknown or unique cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tJ5ACQ8H33-"
   },
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation\n",
    "df = impute_categorical(df, 'NAME_TYPE_SUITE', strategy='proportional',\n",
    "                        missing_value='Missing', new_category_name='Other_C',\n",
    "                        distribute_ratio=0.95)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['NAME_TYPE_SUITE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD7TaXVnH33-"
   },
   "source": [
    "### <a id='toc1_6_3_'></a>[**2.3 Mean/Median Imputation for Numerical Features**](#toc0_)\n",
    "\n",
    "1. Defines a function impute_mean_median that:\n",
    "\n",
    "    - Takes a DataFrame and a dictionary specifying features and their imputation methods.\n",
    "    - Imputes each feature using the specified method (mean or median).\n",
    "    - Keeps track of the imputation process for each feature.\n",
    "    - Returns the imputed DataFrame and a summary of the imputation process.\n",
    "\n",
    "2. Specifies the features to impute and their respective methods in a dictionary.\n",
    "3. Performs the imputation using the `impute_mean_median` function.\n",
    "4. Displays a summary of the imputation process, showing:\n",
    "\n",
    "    - The feature name\n",
    "    - The imputation method used\n",
    "    - The value used for imputation\n",
    "    - The number of null values before and after imputation\n",
    "\n",
    "5. Creates a visualization showing the distribution of each imputed feature after imputation.\n",
    "6. Optionally prints summary statistics for each imputed feature.\n",
    "\n",
    "This approach allows for flexibility in choosing the imputation method for each feature while providing transparency about the imputation process. The visualization and summary statistics help in understanding the impact of imputation on the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mvhDgHcH33-"
   },
   "outputs": [],
   "source": [
    "def impute_mean_median(df, features_dict):\n",
    "    \"\"\"\n",
    "    Impute specified features using either mean or median.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param features_dict: dictionary with feature names as keys and 'mean' or 'median' as values\n",
    "    :return: DataFrame with imputed values and a summary of imputation\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    for feature, method in features_dict.items():\n",
    "        original_null_count = df[feature].isnull().sum()\n",
    "\n",
    "        if method == 'mean':\n",
    "            impute_value = df[feature].mean()\n",
    "        elif method == 'median':\n",
    "            impute_value = df[feature].median()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method for {feature}. Use 'mean' or 'median'.\")\n",
    "\n",
    "        df[feature].fillna(impute_value, inplace=True)\n",
    "\n",
    "        summary.append({\n",
    "            'Feature': feature,\n",
    "            'Method': method,\n",
    "            'Imputed Value': impute_value,\n",
    "            'Null Count Before': original_null_count,\n",
    "            'Null Count After': df[feature].isnull().sum()\n",
    "        })\n",
    "\n",
    "    return df, pd.DataFrame(summary)\n",
    "\n",
    "# Specify features and imputation methods\n",
    "features_to_impute = {\n",
    "    'AMT_GOODS_PRICE': 'median',\n",
    "    'AMT_ANNUITY': 'median',\n",
    "    'CNT_FAM_MEMBERS': 'median',\n",
    "    'DAYS_LAST_PHONE_CHANGE': 'median'\n",
    "}\n",
    "\n",
    "# Perform imputation\n",
    "df, imputation_summary = impute_mean_median(df, features_to_impute)\n",
    "\n",
    "# Display imputation summary\n",
    "print(\"Imputation Summary:\")\n",
    "print(imputation_summary)\n",
    "\n",
    "# Visualize distributions before and after imputation\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, feature in enumerate(features_to_impute.keys()):\n",
    "    axs[i].hist(df[feature], bins=50, alpha=0.7, label='After Imputation')\n",
    "    axs[i].set_title(f'Distribution of {feature}')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in features_to_impute.keys():\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaAxtJbzH33-"
   },
   "source": [
    "### <a id='toc1_6_4_'></a>[**2.4  Re-Check For Null Values In The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRI1RZXGH33-"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nFeatures with non-zero Null values and their data types:\")\n",
    "null_counts = df.isnull().sum()\n",
    "non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "# Get data types for features with non-zero null values\n",
    "data_types = df.dtypes[non_zero_nulls.index]\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    # Combine the non-zero nulls with their data types for display\n",
    "    combined_info = pd.DataFrame({'Null Counts': non_zero_nulls, 'Data Type': data_types})\n",
    "    print(combined_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv79ZXhNFnl6"
   },
   "source": [
    "#### <a id='toc1_6_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code checks for and displays the count of null values in each column of the dataset.\n",
    "\n",
    "#### <a id='toc1_6_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the extent and distribution of missing data is crucial because:\n",
    "\n",
    "1. It affects the quality and reliability of our analysis and model predictions.\n",
    "2. It guides our data preprocessing strategy, including decisions on imputation or feature dropping.\n",
    "3. Missing data patterns might provide insights into data collection processes or inherent characteristics of certain variables.\n",
    "\n",
    "#### <a id='toc1_6_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "1. Many features have no missing values, including key variables like **TARGET** (***our prediction goal***) and basic applicant information.\n",
    "2. Several features have a significant number of missing values:\n",
    "\n",
    "    - **OWN_CAR_AGE**: 202,929 missing values\n",
    "    - **OCCUPATION_TYPE**: 96,391 missing values\n",
    "   - **EXT_SOURCE_1**: 173,378 missing values\n",
    "    - **EXT_SOURCE_3**: 60,965 missing values\n",
    "\n",
    "3. Most features related to building characteristics (e.g., **APARTMENTS_AVG, BASEMENTAREA_AVG**, etc.) have a large number of missing values, ranging from about 150,000 to 215,000.\n",
    "4. Credit bureau request features (**AMT_REQ_CREDIT_BUREAU_***) all have 41,519 missing values each.\n",
    "5. Some potentially important features like **AMT_ANNUITY** (*12 missing*) and **AMT_GOODS_PRICE** (ˆ) have a small number of null values.\n",
    "\n",
    "#### <a id='toc1_6_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "1. The dataset has a mixed pattern of missing values, with some features being complete and others having significant gaps.\n",
    "2. The high number of missing values in building-related features suggests these might not be applicable or available for all loan applications.\n",
    "3. The consistent number of missing values in credit bureau request features indicates a systematic reason for their absence, possibly related to data availability or collection processes.\n",
    "4. Core features for loan assessment (e.g., income, credit amount, target variable) are largely complete, which is positive for our analysis.\n",
    "\n",
    "#### <a id='toc1_6_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "1. For features with a small number of missing values (e.g., **AMT_ANNUITY, AMT_GOODS_PRICE**): consider using imputation techniques like mean, median, or advanced methods like KNN imputation.\n",
    "2. For features with a large number of missing values:\n",
    "    1. If they're deemed crucial (like **EXT_SOURCE** variables), consider advanced imputation techniques or creating a \"missing\" category.\n",
    "    2. If they're less important or redundant (like many of the building-related features), consider dropping them or creating aggregate features that combine information from related columns.\n",
    "3. For credit bureau request features: consider creating a binary flag indicating whether this information was available, in addition to any imputation strategy.\n",
    "4. Analyze the impact of missing values on the target variable to ensure that dropping or imputing doesn't introduce bias into the model.\n",
    "5. Document all decisions made regarding handling of missing data, as this will be crucial for model interpretation and future data processing.\n",
    "\n",
    "#### <a id='toc1_6_5_'></a>[Decisions](#toc0_)\n",
    "\n",
    "1.  I used mean or median imputation on the following Features with null values:\n",
    "\n",
    "    > 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'CNT_FAM_MEMBERS', 'DAYS_LAST_PHONE_CHANGE'\n",
    "\n",
    "2. I used a proportional imputation technique combined with creation of a \"Other\" category with the following categorical features with missing data:\n",
    "\n",
    "    > 'OCCUPATION_TYPE', NAME_TYPE_SUITE'\n",
    "\n",
    "3.  I dropped any feature with more than 100K null values:\n",
    "\n",
    "    > 'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    > 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    > 'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    > 'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    > 'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    > 'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    > 'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    > 'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    > 'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    > 'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    > 'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    > 'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    > 'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    > 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    > 'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "\n",
    "4. I created a feature flag category for each credit bureau request features that indicates whether this information was available\n",
    "\n",
    "5. I used KNN imputation on the following features that have null values:\n",
    "\n",
    "    > 'EXT_SOURCE_3', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[**3.0 Pre-process and encode the features**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gib4VfnNRbv"
   },
   "source": [
    "### <a id='toc1_7_1_'></a>[**3.1 Categorical Data Preprocessing Strategy**](#toc0_)\n",
    "\n",
    "1. **Binary Encoding**\n",
    "\n",
    "  For features with only two categories, use simple binary encoding:\n",
    "\n",
    "  - `FLAG_OWN_CAR`: {'N': 0, 'Y': 1}\n",
    "  - `FLAG_OWN_REALTY`: {'N': 0, 'Y': 1}\n",
    "\n",
    "2. **Ordinal Encoding**\n",
    "\n",
    "  For features with a clear ordinal relationship:\n",
    "\n",
    "  - **`NAME_EDUCATION_TYPE`**:\n",
    "    1. 'Lower secondary'\n",
    "    2. 'Secondary / secondary special'\n",
    "    3. 'Incomplete higher'\n",
    "    4. 'Higher education'\n",
    "    5. 'Academic degree'\n",
    "\n",
    "3. **One-Hot Encoding**\n",
    "\n",
    "  For nominal categories with low to medium cardinality:\n",
    "\n",
    "  - **`NAME_CONTRACT_TYPE`**\n",
    "  - **`NAME_TYPE_SUITE`**\n",
    "  - **`NAME_INCOME_TYPE`**\n",
    "  - **`NAME_FAMILY_STATUS`**\n",
    "  - **`NAME_HOUSING_TYPE`**\n",
    "  - **`WEEKDAY_APPR_PROCESS_START`**\n",
    "\n",
    "4. **Handling High Cardinality**\n",
    "\n",
    "  For features with many categories:\n",
    "\n",
    "  - **`OCCUPATION_TYPE`**: Consider grouping similar occupations or using target encoding.\n",
    "  - **`ORGANIZATION_TYPE`**: Group into broader categories or use target encoding.\n",
    "\n",
    "5. **Special Cases**\n",
    "\n",
    "  - **`CODE_GENDER`**:\n",
    "    - One-hot encode, but consider investigating and potentially removing 'XNA' entries.\n",
    "\n",
    "6. **Feature Engineering**\n",
    "\n",
    "  Consider creating new features:\n",
    "\n",
    "  - **`IS_WEEKEND`**: Binary feature derived from **`WEEKDAY_APPR_PROCESS_START`**.\n",
    "  - **`FAMILY_SIZE`**: Combine information from **`NAME_FAMILY_STATUS`** and **`NAME_TYPE_SUITE`**.\n",
    "\n",
    "7. **Handling Rare Categories**\n",
    "\n",
    "  For categories with very few occurrences:\n",
    "\n",
    "  - Group rare categories into an 'Other' category.\n",
    "  - Threshold: Consider grouping categories that appear in less than 1% of the data.\n",
    "\n",
    "8. **Dealing with Unknown Values**\n",
    "\n",
    "  - For 'Unknown' or 'XNA' values:\n",
    "    - If few, consider removing these entries.\n",
    "    - Otherwise, treat as a separate category in one-hot encoding.\n",
    "    - For ordinal features, assign a appropriate value (e.g., the median or a value outside the range)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXecMNHkQt-u"
   },
   "source": [
    "### <a id='toc1_7_1_1_'></a>[**3.1.1 Binary Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzqS6yIfQuuB"
   },
   "outputs": [],
   "source": [
    "df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].map({'N': 0, 'Y': 1})\n",
    "df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cnqPRtqQvIf"
   },
   "source": [
    "### <a id='toc1_7_1_2_'></a>[**3.1.2 Ordinal Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOaq72RaRMwR"
   },
   "outputs": [],
   "source": [
    "education_order = ['Lower secondary', 'Secondary / secondary special', 'Incomplete higher', 'Higher education', 'Academic degree']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
    "df['NAME_EDUCATION_TYPE'] = ordinal_encoder.fit_transform(df[['NAME_EDUCATION_TYPE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_Ebe1IYRPNV"
   },
   "source": [
    "### <a id='toc1_7_1_3_'></a>[**3.1.3 One-Hot Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km2bOSriRPtA"
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0aSjOagReqU"
   },
   "source": [
    "### <a id='toc1_7_1_4'></a>[**3.1.4 Handling High Cardinality**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8E0UmyApRfZU"
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder()\n",
    "df['OCCUPATION_TYPE_encoded'] = te.fit_transform(df['OCCUPATION_TYPE'], df['TARGET'])\n",
    "df['ORGANIZATION_TYPE_encoded'] = te.fit_transform(df['ORGANIZATION_TYPE'], df['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZf2UJE1RpjO"
   },
   "source": [
    "### <a id='toc1_7_1_5'></a>[**3.1.5 Special Cases**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_unknown_values(df, unknown_value):\n",
    "    \"\"\"\n",
    "    Identify categorical features that contain undefined values.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param unknown_value: The value to search for (default is 'Unknown')\n",
    "    :return: Dictionary with features and their undefined value counts\n",
    "    \"\"\"\n",
    "    # Select only object (string) and category dtype columns\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    unknown_features = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        # Count 'Unknown' values\n",
    "        unknown_count = (df[col] == unknown_value).sum()\n",
    "        \n",
    "        if unknown_count > 0:\n",
    "            # Calculate percentage\n",
    "            percentage = (unknown_count / len(df)) * 100\n",
    "            unknown_features[col] = {\n",
    "                'count': unknown_count,\n",
    "                'percentage': percentage\n",
    "            }\n",
    "    \n",
    "    return unknown_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_5_1'></a>[**3.1.5.1 Investigate Features With `XNA` Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqDWfgvuRqF6"
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "unknown_features = identify_unknown_values(df, unknown_value='XNA')\n",
    "\n",
    "# Print results\n",
    "if unknown_features:\n",
    "    print(\"Features with 'XNA' values:\")\n",
    "    for feature, info in unknown_features.items():\n",
    "        print(f\"{feature}: {info['count']} 'XNA' values ({info['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"No features found with 'Unknown' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "- Based on the fact that XNA represents 18% of the ORGANIZATION_TYPE Feature I will leave it as a Categorical Value associated with that feature.\n",
    "- Since there are only 4 entries that have `XNA` as a value in `CODE_GENDER` I will remove those lines from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_5_2'></a>[**3.1.5.2 Clean-up `CODE_GENDER` And Binary Encode The Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_binary_encode_gender(df, column='CODE_GENDER', remove_value='XNA', encoding_map={'M': 0, 'F': 1}, inplace=False):\n",
    "    \"\"\"\n",
    "    Remove specified value from gender column and apply binary encoding.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param column: name of the gender column\n",
    "    :param remove_value: value to be removed (e.g., 'XNA')\n",
    "    :param encoding_map: dictionary mapping original values to encoded values\n",
    "    :return: DataFrame with cleaned and encoded gender\n",
    "    \"\"\"\n",
    "    # Step 1: Print initial shape and distribution\n",
    "    print(f\"Initial shape of the dataset: {df.shape}\")\n",
    "    print(f\"\\nInitial {column} distribution:\")\n",
    "    print(df[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Step 2: Remove rows with specified value\n",
    "    df_cleaned = df[df[column] != remove_value].copy()\n",
    "    \n",
    "    # Step 3: Apply binary encoding\n",
    "    df_cleaned[column] = df_cleaned[column].map(encoding_map)\n",
    "    \n",
    "    # Step 4: Verify the changes\n",
    "    print(f\"\\nShape of the dataset after removing '{remove_value}': {df_cleaned.shape}\")\n",
    "    print(f\"\\n{column} distribution after encoding:\")\n",
    "    print(df_cleaned[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Check for any unexpected values\n",
    "    unexpected = df_cleaned[~df_cleaned[column].isin(encoding_map.values())]\n",
    "    if len(unexpected) > 0:\n",
    "        print(f\"\\nWarning: Unexpected values found in {column}:\")\n",
    "        print(unexpected[column].value_counts())\n",
    "    else:\n",
    "        print(f\"\\nAll {column} values successfully encoded.\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_cleaned[column].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {column} after Encoding')\n",
    "    plt.xlabel(f'{column} ({\", \".join([f\"{v}: {k}\" for k, v in encoding_map.items()])})')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    if inplace:\n",
    "        df[column] = df_cleaned[column]\n",
    "        return None\n",
    "    else:\n",
    "        return df_cleaned\n",
    "\n",
    "# Usage:\n",
    "# To get a new DataFrame:\n",
    "df_processed = clean_and_binary_encode_gender(df)\n",
    "\n",
    "# Update the dataset file with the cleaned and encoded data\n",
    "clean_and_binary_encode_gender(df, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoiAMUaqRqhG"
   },
   "source": [
    "### <a id='toc1_7_1_6'></a>[**3.1.6 Feature Engineering**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rOtEmzhRrAM"
   },
   "outputs": [],
   "source": [
    "df['IS_WEEKEND'] = df['WEEKDAY_APPR_PROCESS_START'].isin(['SATURDAY', 'SUNDAY']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpOJphPFSCo8"
   },
   "source": [
    "### <a id='toc1_7_1_7'></a>[**3.1.7 Handling Rare Categories**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf1pD4SqSDE-"
   },
   "outputs": [],
   "source": [
    "def group_rare_categories(series, threshold=0.01):\n",
    "    value_counts = series.value_counts(normalize=True)\n",
    "    mask = value_counts > threshold\n",
    "    return series.map(lambda x: x if x in value_counts[mask].index else 'Other')\n",
    "\n",
    "df['OCCUPATION_TYPE'] = group_rare_categories(df['OCCUPATION_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3mgQq4MSa1q"
   },
   "source": [
    "### <a id='toc1_7_1_8'></a>[**3.1.8 Dealing with Unknown Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_8_1'></a>[**3.1.8.1 Investigate Features With 'Unknown' As A Value**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZnwMocJSbRd"
   },
   "outputs": [],
   "source": [
    "unknown_features = identify_unknown_values(df, unknown_value='Unknown')\n",
    "\n",
    "# Print results\n",
    "if unknown_features:\n",
    "    print(\"Features with 'Unknown' values:\")\n",
    "    for feature, info in unknown_features.items():\n",
    "        print(f\"{feature}: {info['count']} 'Unknown' values ({info['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"No features found with 'Unknown' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_8_2'></a>[**3.1.8.2 Clean-up `NAME_FAMILY_STATUS` And One-Hot Encode The Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_encode_family_status(df, column='NAME_FAMILY_STATUS', inplace=False):\n",
    "    \"\"\"\n",
    "    Remove 'Unknown' values from NAME_FAMILY_STATUS and apply one-hot encoding.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param column: name of the family status column\n",
    "    :param inplace: if True, modify the DataFrame in-place; if False, return a new DataFrame\n",
    "    :return: DataFrame with cleaned and encoded family status (if inplace=False)\n",
    "    \"\"\"\n",
    "    # Step 1: Print initial shape and distribution\n",
    "    print(\"Initial shape of the dataset:\", df.shape)\n",
    "    print(\"\\nInitial NAME_FAMILY_STATUS distribution:\")\n",
    "    print(df[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Step 2: Remove rows where NAME_FAMILY_STATUS is 'Unknown'\n",
    "    df_cleaned = df[df[column] != 'Unknown'].copy()\n",
    "    \n",
    "    # Step 3: Apply one-hot encoding\n",
    "    df_encoded = pd.get_dummies(df_cleaned, columns=[column], prefix='FAMILY_STATUS')\n",
    "    \n",
    "    # Step 4: Verify the changes\n",
    "    print(\"\\nShape of the dataset after removing 'Unknown':\", df_encoded.shape)\n",
    "    print(\"\\nColumns created after one-hot encoding:\")\n",
    "    new_columns = [col for col in df_encoded.columns if col.startswith('FAMILY_STATUS_')]\n",
    "    print(new_columns)\n",
    "    \n",
    "    # Optional: Visualize the distribution of family status before and after\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    df[column].value_counts().plot(kind='bar', ax=ax1, title='Before Cleaning')\n",
    "    ax1.set_xlabel('Family Status')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    df_cleaned[column].value_counts().plot(kind='bar', ax=ax2, title='After Cleaning')\n",
    "    ax2.set_xlabel('Family Status')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if inplace:\n",
    "        # Update the original DataFrame\n",
    "        df.drop(index=df[df[column] == 'Unknown'].index, inplace=True)\n",
    "        new_columns = [col for col in df_encoded.columns if col.startswith('FAMILY_STATUS_')]\n",
    "        df[new_columns] = df_encoded[new_columns]\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "    else:\n",
    "        return df_encoded\n",
    "\n",
    "# Usage:\n",
    "# To get a new DataFrame:\n",
    "# df_processed = clean_and_encode_family_status(df)\n",
    "\n",
    "# To update the original DataFrame:\n",
    "clean_and_encode_family_status(df, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_9_'></a>[**3.1.9 Remove Object Type Features**](#toc0_)\n",
    "\n",
    "Now that all of the Categorical Features have been Encoded we can remove those Features from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features with large amounts of missing data (any feature missing more than 100K of null values)\n",
    "df.drop(columns=[\n",
    "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
    "    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
    "    'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_1_10'></a>[**3.1.10 Correlation Heatmap**](#toc0_)\n",
    "\n",
    "**Interpretation and Next Steps**\n",
    "\n",
    "1. Look for highly correlated feature pairs (correlation close to 1 or -1).\n",
    "2. Consider removing one feature from highly correlated pairs to reduce multicollinearity.\n",
    "3. Identify features strongly correlated with your target variable (if included in the heatmap).\n",
    "4. Use these insights to guide your feature selection process.\n",
    "5. Be cautious about interpreting correlations for features that were originally categorical and then encoded.\n",
    "\n",
    "> Remember, correlation doesn't imply causation, but it's a valuable tool for understanding your data and informing your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df, figsize=(20, 16)):\n",
    "    # Select only numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eymMopkQH34C"
   },
   "source": [
    "#### <a id='toc1_7_11_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code preprocesses the data by encoding categorical variables, splitting the data into training and testing sets, and scaling the features.\n",
    "\n",
    "#### <a id='toc1_7_12_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Proper preprocessing ensures that the data is in a suitable format for the deep learning model and that the model's performance can be accurately evaluated.\n",
    "\n",
    "#### <a id='toc1_7_13_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The transformation of categorical variables into numerical format\n",
    "- The split of data into training and testing sets\n",
    "- The scaling of features to a common range\n",
    "\n",
    "#### <a id='toc1_7_14_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The data is now properly encoded, split, and scaled, ready for model training.\n",
    "\n",
    "#### <a id='toc1_7_15_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Ensure that the same preprocessing steps are applied to any new data used for predictions\n",
    "- Consider using cross-validation for a more robust evaluation of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[**4.0 Analyze the distribution of the target variable (loan default rate)**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the percentage of defaults\n",
    "default_rate = df['TARGET'].mean() * 100\n",
    "print(f\"\\nPercentage of defaults: {default_rate:.2f}%\")\n",
    "\n",
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='TARGET', data=df)\n",
    "plt.title('Distribution of Loan Defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code calculates the percentage of loan defaults and visualizes the distribution of the target variable.\n",
    "\n",
    "#### <a id='toc1_7_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the class distribution is crucial for binary classification problems, as imbalanced datasets can lead to biased models.\n",
    "\n",
    "#### <a id='toc1_7_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The percentage of defaults in the dataset\n",
    "- Visual representation of the class imbalance\n",
    "\n",
    "#### <a id='toc1_7_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "This analysis reveals whether we're dealing with a balanced or imbalanced dataset.\n",
    "\n",
    "#### <a id='toc1_7_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- If the dataset is heavily imbalanced, consider using techniques like SMOTE, undersampling, or adjusting class weights\n",
    "- If relatively balanced, proceed with caution and monitor for potential bias in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**5.0 Balance The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_9_1_'></a>[**5.1 SMOTE**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('TARGET', axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_8_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code applies the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset by creating synthetic examples of the minority class.\n",
    "\n",
    "#### <a id='toc1_8_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Balancing the dataset helps prevent the model from being biased towards the majority class, which is crucial for fair and accurate predictions, especially in loan default scenarios.\n",
    "\n",
    "#### <a id='toc1_8_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The change in the number of samples after applying SMOTE\n",
    "- The new ratio of default to non-default cases\n",
    "\n",
    "#### <a id='toc1_8_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "SMOTE has created a balanced dataset, which should help in training a more fair and accurate model.\n",
    "\n",
    "#### <a id='toc1_8_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Proceed with caution and validate the model's performance on both balanced and imbalanced test sets\n",
    "- Consider experimenting with other balancing techniques if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[**6.0 Visualize The Balanced/Imbalanced Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Distribution of Loan Defaults After SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_9_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code visualizes the distribution of the target variable after applying SMOTE.\n",
    "\n",
    "#### <a id='toc1_9_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Visualizing the balanced dataset confirms the effectiveness of the SMOTE technique and provides a clear comparison with the original imbalanced distribution.\n",
    "\n",
    "#### <a id='toc1_9_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The new distribution of default and non-default cases\n",
    "- Comparison with the original imbalanced distribution\n",
    "\n",
    "#### <a id='toc1_9_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The visualization confirms that SMOTE has successfully balanced the dataset.\n",
    "\n",
    "#### <a id='toc1_9_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Use this balanced dataset for model training\n",
    "- Keep the original imbalanced distribution in mind when interpreting model performance on real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL2-RpL4H34C"
   },
   "source": [
    "## <a id='11_'></a>[**7.0 Build And Train A Deep Learning Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_1_'></a>[**7.1 Test/Train Split and Data Normalization**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_2_'></a>[**7.1 Build And Train A Deep Learning Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC-x9UuCH34D"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW4QKokeH34D"
   },
   "source": [
    "#### <a id='toc1_11_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code defines a deep learning model architecture, compiles the model with appropriate loss function and optimizer, and trains the model on the preprocessed data.\n",
    "\n",
    "#### <a id='toc1_11_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Building and training the model is the core of the project, where the patterns in the data are learned to make predictions on loan defaults.\n",
    "\n",
    "#### <a id='toc1_11_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The model architecture (number of layers, neurons, activation functions)\n",
    "- The training process (number of epochs, batch size)\n",
    "- The training and validation accuracy/loss over epochs\n",
    "\n",
    "#### <a id='toc1_11_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The model has been trained on the balanced dataset and should be capable of predicting loan defaults.\n",
    "\n",
    "#### <a id='toc1_11_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Monitor the training process for signs of overfitting or underfitting\n",
    "- Experiment with different architectures or hyperparameters if the performance is not satisfactory\n",
    "- Consider using techniques like early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='12_'></a>[**8.0 Evaluate the model using Sensitivity and ROC AUC metrics**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
