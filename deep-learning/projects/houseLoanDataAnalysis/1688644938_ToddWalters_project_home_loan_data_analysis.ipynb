{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/projects/automatingPortOperations/1714053668_ToddWalters_project_automating_port_operations.ipynb\" target=\"_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bY8iESXrFnl4",
    "outputId": "127ead91-357b-45ab-ada8-9eaedadcb38f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXqTWV3iFnl4"
   },
   "source": [
    "# <a id='toc1_'></a>[**Loan Default Prediction using Deep Learning**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ7XucjeFnl5"
   },
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "For a safe and secure lending experience, it's important to analyze the past data. In this project, you have to build a deep learning model to predict the chance of default for future loans using the historical data. As you will see, this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The main objective of this project is to create a deep learning model that can accurately predict whether an applicant will be able to repay a loan based on historical data. This involves:\n",
    "\n",
    "1. Analyzing and preprocessing the given dataset\n",
    "2. Handling imbalanced data\n",
    "3. Building and training a deep learning model\n",
    "4. Evaluating the model using appropriate metrics\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The dataset contains historical loan application data. It includes various features about loan applicants and a target variable indicating whether the loan was repaid or defaulted. The data is highly imbalanced, which presents an additional challenge for model training and evaluation.\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "1. Load the dataset\n",
    "2. Check for null values in the dataset\n",
    "3. Analyze the distribution of the target variable (loan default rate)\n",
    "4. Balance the dataset\n",
    "5. Visualize the balanced/imbalanced data\n",
    "6. Preprocess and encode the features\n",
    "7. Build and train a deep learning model\n",
    "8. Evaluate the model using Sensitivity and ROC AUC metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHz_5EEfFnl5"
   },
   "source": [
    "## <a id='toc1_5_'></a>[**1.0 Load The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqVKXKtFnl5"
   },
   "source": [
    "### <a id='toc1_5_1_'></a>[**1.1 Setup: Import Necessary Libraries**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtYfv5plFnl5"
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kxMwRpFnl6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNDE8nqrFnl6"
   },
   "source": [
    "### <a id='toc1_5_2_'></a>[**1.2 Loading The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yq_ODKMLFnl6",
    "outputId": "7233cc95-bfde-4f17-a2aa-9f53e103cb75"
   },
   "outputs": [],
   "source": [
    "# Load the dataset (replace 'loan_data.csv' with your actual filename)\n",
    "df = pd.read_csv('/Users/toddwalters/Development/data/1688644938_dataset/1688644938_loan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows and basic information about the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nThe df.describe output is:\\n\")\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(df.describe().transpose())\n",
    "\n",
    "print(f\"\\nThe df.describe output is:\\n\")\n",
    "print(df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[**2.0  Check for null values in the dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull values in the dataset:\")\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[**2.1  Drop Features With More Than 100K Null Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features with large amounts of missing data (any feature missing more than 100K of null values)\n",
    "df.drop(columns=[\n",
    "    'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_3_'></a>[**2.3 KNN Imputation of Numerical Features**](#toc0_)\n",
    "\n",
    "1. **`create_availability_flags` Function**\n",
    "  \n",
    "    - **Input**: DataFrame and a list of column names\n",
    "    - **Process**:\n",
    "      - For each specified column, creates a new column with the suffix \"_FLAG\"\n",
    "      - Flag is 1 if the original value is not null, 0 if null\n",
    "    - **Purpose**: Tracks which values were originally missing before imputation\n",
    "\n",
    "2. **Creation of Flags for Credit Bureau Features**\n",
    "\n",
    "    - Defines a list of credit bureau features\n",
    "    - Calls `create_availability_flags` to create flag columns for these features\n",
    "    - Displays the resulting flag columns using `df.info()`\n",
    "\n",
    "3. **`knn_impute_features` Function**\n",
    "\n",
    "    - Scales input features using StandardScaler\n",
    "    - Performs KNN imputation using sklearn's KNNImputer\n",
    "    - Scales imputed values back to original range\n",
    "    - Updates original DataFrame with imputed values\n",
    "\n",
    "4. **KNN Imputation Process**\n",
    "\n",
    "    - Creates a list of all features to impute (credit bureau features and others)\n",
    "    - Displays DataFrame info before imputation\n",
    "    - Performs KNN imputation using `knn_impute_features` function\n",
    "    - Saves imputed DataFrame to CSV file\n",
    "    - Displays DataFrame info after imputation\n",
    "\n",
    "5. **Summary Statistics**\n",
    "\n",
    "   - For each imputed feature, prints summary statistics using `describe()`\n",
    "\n",
    "6. **Visualization**\n",
    "\n",
    "    - Creates a grid of subplots, one for each imputed feature\n",
    "    - Calculates number of rows and columns based on feature count\n",
    "    - For each feature:\n",
    "      - Plots a histogram showing distribution after imputation\n",
    "    - Removes any unused subplots\n",
    "    - Displays the resulting plot\n",
    "\n",
    "**Overall Approach**\n",
    "\n",
    "This code provides a comprehensive method for handling missing data:\n",
    "\n",
    "1. Flags originally missing values (Section 2.2)\n",
    "2. Imputes missing values using KNN\n",
    "3. Provides summary statistics of imputed data\n",
    "4. Visualizes distribution of imputed features\n",
    "\n",
    "This approach ensures transparency in the imputation process and aids in understanding the impact of imputation on data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_3_1_'></a>[**2.3.1 Create Feature Flag Categories**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_availability_flags(df, columns):\n",
    "    \"\"\"\n",
    "    Create binary flags indicating data availability for specified columns.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param columns: list of column names to create flags for\n",
    "    :return: DataFrame with added flag columns\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        flag_col_name = f\"{col}_FLAG\"\n",
    "        df[flag_col_name] = (~df[col].isnull()).astype(int)\n",
    "    return df\n",
    "\n",
    "# List of AMT_REQ_CREDIT_BUREAU features\n",
    "credit_bureau_features = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "]\n",
    "# Create availability flags\n",
    "df = create_availability_flags(df, credit_bureau_features)\n",
    "\n",
    "# Display info for flag columns\n",
    "print(\"\\nFlag columns:\")\n",
    "print(df[[col + '_FLAG' for col in credit_bureau_features]].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_3_2_'></a>[**2.3.2 KNN Feature Impuation**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute_features(df, features, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Perform KNN imputation on specified columns.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param features: list of column names to impute\n",
    "    :param n_neighbors: number of neighbors to use for KNN imputation\n",
    "    :return: DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Prepare data for KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    scaled_features = scaler.fit_transform(df[features])\n",
    "    \n",
    "    # Perform KNN imputation\n",
    "    imputed_features = imputer.fit_transform(scaled_features)\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    imputed_features = scaler.inverse_transform(imputed_features)\n",
    "    \n",
    "    # Update the DataFrame with imputed values\n",
    "    for i, col in enumerate(features):\n",
    "        df[col] = imputed_features[:, i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# List of all features that need KNN imputation\n",
    "all_features_to_impute = credit_bureau_features + [\n",
    "    'EXT_SOURCE_3',\n",
    "    'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "    'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "    'EXT_SOURCE_2'\n",
    "    # Add any other features that need imputation here\n",
    "]\n",
    "\n",
    "# Display info before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Perform KNN imputation\n",
    "df = knn_impute_features(df, all_features_to_impute)\n",
    "\n",
    "# Save the imputed dataframe if needed\n",
    "df.to_csv('/Users/toddwalters/Development/data/1688644938_dataset/loan_data_imputed.csv', index=False)\n",
    "\n",
    "print(\"\\nImputation complete. The specified features have been updated in the main dataframe.\\n\")\n",
    "\n",
    "# Display info after imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in all_features_to_impute:\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())\n",
    "\n",
    "# Calculate the number of rows and columns needed for the subplots\n",
    "n_features = len(all_features_to_impute)\n",
    "n_cols = min(3, n_features)  # Max 3 columns\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axs = axs.ravel() if n_features > 1 else [axs]\n",
    "\n",
    "# Plot histograms for each feature\n",
    "for i, feature in enumerate(all_features_to_impute):\n",
    "    axs[i].hist(df[feature], bins=50)\n",
    "    axs[i].set_title(f'Distribution of {feature}\\nafter imputation')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_4_'></a>[**2.4 Imputation of Missing Categorical Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical(df, column_name, strategy='proportional', missing_value=None, new_category_name='Unknown', distribute_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Impute missing values in a categorical column using various strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column_name: str, name of the column to impute\n",
    "    - strategy: str, one of 'proportional', 'new_category', 'mode'\n",
    "    - missing_value: value to be considered as missing (if None, will use pd.isnull())\n",
    "    - new_category_name: str, name of the new category if using 'new_category' strategy\n",
    "    - distribute_ratio: float, ratio of missing values to distribute when using 'proportional' strategy\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify missing values\n",
    "    if missing_value is None:\n",
    "        missing_mask = df[column_name].isnull()\n",
    "    else:\n",
    "        missing_mask = df[column_name] == missing_value\n",
    "    \n",
    "    missing_count = missing_mask.sum()\n",
    "    \n",
    "    if strategy == 'proportional':\n",
    "        # Calculate the distribution of non-missing values\n",
    "        value_counts = df[~missing_mask][column_name].value_counts()\n",
    "        total_non_missing = value_counts.sum()\n",
    "        \n",
    "        # Calculate the number of values to distribute\n",
    "        distribute_count = int(missing_count * distribute_ratio)\n",
    "        other_count = missing_count - distribute_count\n",
    "        \n",
    "        # Calculate the number of new values for each category\n",
    "        new_values = (value_counts / total_non_missing * distribute_count).round().astype(int)\n",
    "        \n",
    "        # Adjust to ensure we have exactly the right number\n",
    "        while new_values.sum() + other_count < missing_count:\n",
    "            new_values[new_values.idxmax()] += 1\n",
    "        while new_values.sum() + other_count > missing_count:\n",
    "            new_values[new_values.idxmax()] -= 1\n",
    "        \n",
    "        # Create a list of all new values\n",
    "        all_new_values = []\n",
    "        for category, count in new_values.items():\n",
    "            all_new_values.extend([category] * count)\n",
    "        \n",
    "        # Add the new category for remaining values\n",
    "        all_new_values.extend([new_category_name] * other_count)\n",
    "        \n",
    "        # Shuffle and assign new values\n",
    "        np.random.shuffle(all_new_values)\n",
    "        df.loc[missing_mask, column_name] = all_new_values\n",
    "        \n",
    "    elif strategy == 'new_category':\n",
    "        # Simply replace all missing values with the new category name\n",
    "        df.loc[missing_mask, column_name] = new_category_name\n",
    "        \n",
    "    elif strategy == 'mode':\n",
    "        # Replace missing values with the most frequent category\n",
    "        mode_value = df[~missing_mask][column_name].mode().iloc[0]\n",
    "        df.loc[missing_mask, column_name] = mode_value\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose 'proportional', 'new_category', or 'mode'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_4_1'></a>[**2.4.1 Identify Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nFeatures with non-zero Null values and their data types:\")\n",
    "null_counts = df.isnull().sum()\n",
    "non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "# Get data types for features with non-zero null values\n",
    "data_types = df.dtypes[non_zero_nulls.index]\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    # Combine the non-zero nulls with their data types for display\n",
    "    combined_info = pd.DataFrame({'Null Counts': non_zero_nulls, 'Data Type': data_types})\n",
    "    print(combined_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of Features of Type Object:\\n\")\n",
    "print(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# print(f\"\\nUnique values in OCCUPATION_TYPE: {df['OCCUPATION_TYPE'].unique()}\")\n",
    "# print(f\"\\nUnique values in NAME_TYPE_SUITE: {df['NAME_TYPE_SUITE'].unique()}\")\n",
    "\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nUnique values in {column}: {df[column].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_4_2'></a>[**2.4.2 Investigate Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Fill missing values with 'Missing' to include them in the histogram\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "df = df.copy()\n",
    "\n",
    "df['OCCUPATION_TYPE'].fillna('Missing', inplace=True)\n",
    "df['NAME_TYPE_SUITE'].fillna('Missing', inplace=True)\n",
    "\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['OCCUPATION_TYPE', 'NAME_TYPE_SUITE']\n",
    "\n",
    "# Plotting\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_4_3'></a>[**2.4.3 Imputation of Missing Categorical Data Within OCCUPATION_TYPE Feature**](#toc0_)\n",
    "\n",
    "Given this distribution, here's an approach to distribute the missing values across the other categories while maintaining the overall structure of the data:\n",
    "\n",
    "1. We'll use a proportional distribution method, but with a slight modification to account for the large number of missing values.\n",
    "2. Instead of directly distributing all missing values, we'll distribute a portion of them (e.g., 80%) across the existing categories based on their current proportions. This helps maintain the general distribution while not overly inflating the existing categories.\n",
    "3. The remaining portion (e.g., 20%) will be assigned to a new category called \"Other\" or \"Unspecified\". This accounts for the possibility that some of these missing values might genuinely be unknown or not fit into existing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation for OCCUPATION_TYPE\n",
    "df = impute_categorical(df, 'OCCUPATION_TYPE', strategy='proportional', \n",
    "                        missing_value='Missing', new_category_name='Other', \n",
    "                        distribute_ratio=0.8)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['OCCUPATION_TYPE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_4_4'></a>[**2.4.4 Imputation of Missing Categorical Date Within NAME_TYPE_SUITE Feature**](#toc0_)\n",
    "\n",
    "1. Using the 'proportional' strategy to distribute the missing values across existing categories based on their current proportions.\n",
    "2. Setting distribute_ratio=0.95, which means 95% of the missing values will be distributed proportionally among existing categories, and only 5% will be assigned to the new 'Other' category.\n",
    "3. This approach will:\n",
    "\n",
    "    - Maintain the overall distribution of the data.\n",
    "    - Assign most of the missing values to 'Unaccompanied', reflecting the dominant trend in the data.\n",
    "    - Still allow for some diversity by assigning smaller portions to other categories.\n",
    "    - Create a small 'Other' category to account for any truly unknown or unique cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation\n",
    "df = impute_categorical(df, 'NAME_TYPE_SUITE', strategy='proportional', \n",
    "                        missing_value='Missing', new_category_name='Other_C', \n",
    "                        distribute_ratio=0.95)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['NAME_TYPE_SUITE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_5_'></a>[**2.5 Mean/Median Imputation for Numerical Features**](#toc0_)\n",
    "\n",
    "1. Defines a function impute_mean_median that:\n",
    "\n",
    "    - Takes a DataFrame and a dictionary specifying features and their imputation methods.\n",
    "    - Imputes each feature using the specified method (mean or median).\n",
    "    - Keeps track of the imputation process for each feature.\n",
    "    - Returns the imputed DataFrame and a summary of the imputation process.\n",
    "\n",
    "2. Specifies the features to impute and their respective methods in a dictionary.\n",
    "3. Performs the imputation using the `impute_mean_median` function.\n",
    "4. Displays a summary of the imputation process, showing:\n",
    "\n",
    "    - The feature name\n",
    "    - The imputation method used\n",
    "    - The value used for imputation\n",
    "    - The number of null values before and after imputation\n",
    "\n",
    "5. Creates a visualization showing the distribution of each imputed feature after imputation.\n",
    "6. Optionally prints summary statistics for each imputed feature.\n",
    "\n",
    "This approach allows for flexibility in choosing the imputation method for each feature while providing transparency about the imputation process. The visualization and summary statistics help in understanding the impact of imputation on the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mean_median(df, features_dict):\n",
    "    \"\"\"\n",
    "    Impute specified features using either mean or median.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param features_dict: dictionary with feature names as keys and 'mean' or 'median' as values\n",
    "    :return: DataFrame with imputed values and a summary of imputation\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for feature, method in features_dict.items():\n",
    "        original_null_count = df[feature].isnull().sum()\n",
    "        \n",
    "        if method == 'mean':\n",
    "            impute_value = df[feature].mean()\n",
    "        elif method == 'median':\n",
    "            impute_value = df[feature].median()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method for {feature}. Use 'mean' or 'median'.\")\n",
    "        \n",
    "        df[feature].fillna(impute_value, inplace=True)\n",
    "        \n",
    "        summary.append({\n",
    "            'Feature': feature,\n",
    "            'Method': method,\n",
    "            'Imputed Value': impute_value,\n",
    "            'Null Count Before': original_null_count,\n",
    "            'Null Count After': df[feature].isnull().sum()\n",
    "        })\n",
    "    \n",
    "    return df, pd.DataFrame(summary)\n",
    "\n",
    "# Specify features and imputation methods\n",
    "features_to_impute = {\n",
    "    'AMT_GOODS_PRICE': 'median',\n",
    "    'AMT_ANNUITY': 'median',\n",
    "    'CNT_FAM_MEMBERS': 'median',\n",
    "    'DAYS_LAST_PHONE_CHANGE': 'median'\n",
    "}\n",
    "\n",
    "# Perform imputation\n",
    "df, imputation_summary = impute_mean_median(df, features_to_impute)\n",
    "\n",
    "# Display imputation summary\n",
    "print(\"Imputation Summary:\")\n",
    "print(imputation_summary)\n",
    "\n",
    "# Visualize distributions before and after imputation\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, feature in enumerate(features_to_impute.keys()):\n",
    "    axs[i].hist(df[feature], bins=50, alpha=0.7, label='After Imputation')\n",
    "    axs[i].set_title(f'Distribution of {feature}')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in features_to_impute.keys():\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())\n",
    "Last edited just now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_6_'></a>[**2.06  Re-Check For Null Values In The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nFeatures with non-zero Null values and their data types:\")\n",
    "null_counts = df.isnull().sum()\n",
    "non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "# Get data types for features with non-zero null values\n",
    "data_types = df.dtypes[non_zero_nulls.index]\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    # Combine the non-zero nulls with their data types for display\n",
    "    combined_info = pd.DataFrame({'Null Counts': non_zero_nulls, 'Data Type': data_types})\n",
    "    print(combined_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv79ZXhNFnl6"
   },
   "source": [
    "#### <a id='toc1_6_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code checks for and displays the count of null values in each column of the dataset.\n",
    "\n",
    "#### <a id='toc1_6_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the extent and distribution of missing data is crucial because:\n",
    "\n",
    "1. It affects the quality and reliability of our analysis and model predictions.\n",
    "2. It guides our data preprocessing strategy, including decisions on imputation or feature dropping.\n",
    "3. Missing data patterns might provide insights into data collection processes or inherent characteristics of certain variables.\n",
    "\n",
    "#### <a id='toc1_6_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "1. Many features have no missing values, including key variables like **TARGET** (***our prediction goal***) and basic applicant information.\n",
    "2. Several features have a significant number of missing values:\n",
    "\n",
    "    - **OWN_CAR_AGE**: 202,929 missing values\n",
    "    - **OCCUPATION_TYPE**: 96,391 missing values\n",
    "   - **EXT_SOURCE_1**: 173,378 missing values\n",
    "    - **EXT_SOURCE_3**: 60,965 missing values\n",
    "\n",
    "3. Most features related to building characteristics (e.g., **APARTMENTS_AVG, BASEMENTAREA_AVG**, etc.) have a large number of missing values, ranging from about 150,000 to 215,000.\n",
    "4. Credit bureau request features (**AMT_REQ_CREDIT_BUREAU_***) all have 41,519 missing values each.\n",
    "5. Some potentially important features like **AMT_ANNUITY** (*12 missing*) and **AMT_GOODS_PRICE** (Ë†) have a small number of null values.\n",
    "\n",
    "#### <a id='toc1_6_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "1. The dataset has a mixed pattern of missing values, with some features being complete and others having significant gaps.\n",
    "2. The high number of missing values in building-related features suggests these might not be applicable or available for all loan applications.\n",
    "3. The consistent number of missing values in credit bureau request features indicates a systematic reason for their absence, possibly related to data availability or collection processes.\n",
    "4. Core features for loan assessment (e.g., income, credit amount, target variable) are largely complete, which is positive for our analysis.\n",
    "\n",
    "#### <a id='toc1_6_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "1. For features with a small number of missing values (e.g., **AMT_ANNUITY, AMT_GOODS_PRICE**): consider using imputation techniques like mean, median, or advanced methods like KNN imputation.\n",
    "2. For features with a large number of missing values:\n",
    "    1. If they're deemed crucial (like **EXT_SOURCE** variables), consider advanced imputation techniques or creating a \"missing\" category.\n",
    "    2. If they're less important or redundant (like many of the building-related features), consider dropping them or creating aggregate features that combine information from related columns.\n",
    "3. For credit bureau request features: consider creating a binary flag indicating whether this information was available, in addition to any imputation strategy.\n",
    "4. Analyze the impact of missing values on the target variable to ensure that dropping or imputing doesn't introduce bias into the model.\n",
    "5. Document all decisions made regarding handling of missing data, as this will be crucial for model interpretation and future data processing.\n",
    "\n",
    "#### <a id='toc1_6_5_'></a>[Decisions](#toc0_)\n",
    "\n",
    "1.  I used mean or median imputation on the following Features with null values:\n",
    "\n",
    "    > 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'CNT_FAM_MEMBERS', 'DAYS_LAST_PHONE_CHANGE'\n",
    "\n",
    "2. I used a proportional imputation technique combined with creation of a \"Other\" category with the following categorical features with missing data:\n",
    "\n",
    "    > 'OCCUPATION_TYPE', NAME_TYPE_SUITE'\n",
    "\n",
    "3.  I dropped any feature with more than 100K null values:\n",
    "\n",
    "    > 'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    > 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    > 'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    > 'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    > 'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    > 'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    > 'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    > 'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    > 'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    > 'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    > 'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    > 'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    > 'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    > 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    > 'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "\n",
    "4. I created a feature flag category for each credit bureau request features that indicates whether this information was available\n",
    "\n",
    "5. I used KNN imputation on the following features that have null values:\n",
    "\n",
    "    > 'EXT_SOURCE_3', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ss5CNjBFnl7"
   },
   "source": [
    "## <a id='toc1_7_'></a>[**3.0 Analyze the distribution of the target variable (loan default rate)**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-VHKmZ_Fnl8",
    "outputId": "6ca8e081-4141-41a9-da8d-a846ab18349c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print the percentage of defaults\n",
    "default_rate = df['TARGET'].mean() * 100\n",
    "print(f\"\\nPercentage of defaults: {default_rate:.2f}%\")\n",
    "\n",
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='TARGET', data=df)\n",
    "plt.title('Distribution of Loan Defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMc_qxKsFnl8"
   },
   "source": [
    "#### <a id='toc1_7_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code calculates the percentage of loan defaults and visualizes the distribution of the target variable.\n",
    "\n",
    "#### <a id='toc1_7_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the class distribution is crucial for binary classification problems, as imbalanced datasets can lead to biased models.\n",
    "\n",
    "#### <a id='toc1_7_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The percentage of defaults in the dataset\n",
    "- Visual representation of the class imbalance\n",
    "\n",
    "#### <a id='toc1_7_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "This analysis reveals whether we're dealing with a balanced or imbalanced dataset.\n",
    "\n",
    "#### <a id='toc1_7_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- If the dataset is heavily imbalanced, consider using techniques like SMOTE, undersampling, or adjusting class weights\n",
    "- If relatively balanced, proceed with caution and monitor for potential bias in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twtsj9AVFnl9"
   },
   "source": [
    "## <a id='toc1_8_'></a>[**4.0 Balance The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('TARGET', axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7pZXpd0Fnl9"
   },
   "source": [
    "#### <a id='toc1_8_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code applies the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset by creating synthetic examples of the minority class.\n",
    "\n",
    "#### <a id='toc1_8_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Balancing the dataset helps prevent the model from being biased towards the majority class, which is crucial for fair and accurate predictions, especially in loan default scenarios.\n",
    "\n",
    "#### <a id='toc1_8_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The change in the number of samples after applying SMOTE\n",
    "- The new ratio of default to non-default cases\n",
    "\n",
    "#### <a id='toc1_8_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "SMOTE has created a balanced dataset, which should help in training a more fair and accurate model.\n",
    "\n",
    "#### <a id='toc1_8_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Proceed with caution and validate the model's performance on both balanced and imbalanced test sets\n",
    "- Consider experimenting with other balancing techniques if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**5.0 Visualize The Balanced/Imbalanced Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Distribution of Loan Defaults After SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_8_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code visualizes the distribution of the target variable after applying SMOTE.\n",
    "\n",
    "#### <a id='toc1_8_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Visualizing the balanced dataset confirms the effectiveness of the SMOTE technique and provides a clear comparison with the original imbalanced distribution.\n",
    "\n",
    "#### <a id='toc1_8_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The new distribution of default and non-default cases\n",
    "- Comparison with the original imbalanced distribution\n",
    "\n",
    "#### <a id='toc1_8_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The visualization confirms that SMOTE has successfully balanced the dataset.\n",
    "\n",
    "#### <a id='toc1_8_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Use this balanced dataset for model training\n",
    "- Keep the original imbalanced distribution in mind when interpreting model performance on real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**6.0 Pre-process and encode the features**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_9_1_'></a>[**Part_6_1**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in X.select_dtypes(include=['object']):\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_9_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code preprocesses the data by encoding categorical variables, splitting the data into training and testing sets, and scaling the features.\n",
    "\n",
    "#### <a id='toc1_9_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Proper preprocessing ensures that the data is in a suitable format for the deep learning model and that the model's performance can be accurately evaluated.\n",
    "\n",
    "#### <a id='toc1_9_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The transformation of categorical variables into numerical format\n",
    "- The split of data into training and testing sets\n",
    "- The scaling of features to a common range\n",
    "\n",
    "#### <a id='toc1_9_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The data is now properly encoded, split, and scaled, ready for model training.\n",
    "\n",
    "#### <a id='toc1_9_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Ensure that the same preprocessing steps are applied to any new data used for predictions\n",
    "- Consider using cross-validation for a more robust evaluation of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[**7.0 Build and train a deep learning model**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_10_1_'></a>[**Part_6_1**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_10_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code defines a deep learning model architecture, compiles the model with appropriate loss function and optimizer, and trains the model on the preprocessed data.\n",
    "\n",
    "#### <a id='toc1_10_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Building and training the model is the core of the project, where the patterns in the data are learned to make predictions on loan defaults.\n",
    "\n",
    "#### <a id='toc1_10_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The model architecture (number of layers, neurons, activation functions)\n",
    "- The training process (number of epochs, batch size)\n",
    "- The training and validation accuracy/loss over epochs\n",
    "\n",
    "#### <a id='toc1_10_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The model has been trained on the balanced dataset and should be capable of predicting loan defaults.\n",
    "\n",
    "#### <a id='toc1_10_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Monitor the training process for signs of overfitting or underfitting\n",
    "- Experiment with different architectures or hyperparameters if the performance is not satisfactory\n",
    "- Consider using techniques like early stopping to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
