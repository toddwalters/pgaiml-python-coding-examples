{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/projects/houseLoanDataAnalysis/1688644938_ToddWalters_project_home_loan_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY8iESXrFnl4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [**Loan Default Prediction using Deep Learning**](#toc1_)    \n",
    "  - [**Project Context**](#toc1_1_)    \n",
    "  - [**Project Objectives**](#toc1_2_)    \n",
    "  - [**Project Dataset Description**](#toc1_3_)    \n",
    "  - [**Project Analysis Steps To Perform**](#toc1_4_)    \n",
    "  - [**1.0 Load The Dataset**](#toc1_5_)    \n",
    "    - [**1.1 Setup: Import Necessary Libraries**](#toc1_5_1_)    \n",
    "    - [**1.2 Loading The Dataset**](#toc1_5_2_)    \n",
    "  - [**2.0  Identify And Resolve Null Values In The Dataset**](#toc1_6_)    \n",
    "    - [**2.1  Drop Features With More Than 100K Null Values**](#toc1_6_1_)    \n",
    "      - [**Explanations**](#toc1_6_1_1_)    \n",
    "      - [**Importance**](#toc1_6_1_2_)    \n",
    "    - [**2.2 KNN Imputation of Numerical Features**](#toc1_6_2_)    \n",
    "      - [**2.2.1 Create Feature Flag Categories**](#toc1_6_2_1_)    \n",
    "      - [**2.2.2 KNN Feature Imputation**](#toc1_6_2_2_)    \n",
    "        - [**Explanations**](#toc1_6_2_2_1_)    \n",
    "        - [**Why Is It Important**](#toc1_6_2_2_2_)    \n",
    "    - [**2.3 Imputation of Missing Categorical Data**](#toc1_6_3_)    \n",
    "      - [**2.3.1 Identify Categorical Features With Missing Values**](#toc1_6_3_1_)    \n",
    "      - [**2.3.2 Investigate Categorical Features With Missing Values**](#toc1_6_3_2_)    \n",
    "        - [**2.3.2.1 Imputation of Missing Categorical Data Within `OCCUPATION_TYPE` Feature**](#toc1_6_3_2_1_)    \n",
    "        - [**2.3.2.2 Imputation of Missing Categorical Date Within `NAME_TYPE_SUITE` Feature**](#toc1_6_3_2_2_)    \n",
    "      - [**Explanations**](#toc1_6_3_3_)    \n",
    "      - [**Why It Is Important**](#toc1_6_3_4_)    \n",
    "      - [**Observations**](#toc1_6_3_5_)    \n",
    "      - [**Conclusions**](#toc1_6_3_6_)    \n",
    "      - [**Recommendations**](#toc1_6_3_7_)    \n",
    "      - [**Decisions**](#toc1_6_3_8_)    \n",
    "    - [**2.4 Mean/Median Imputation for Numerical Features**](#toc1_6_4_)    \n",
    "      - [**Explanation**](#toc1_6_4_1_)    \n",
    "      - [**Why It Is Important**](#toc1_6_4_2_)    \n",
    "    - [**2.5  Re-Check For Null Values In The Dataset**](#toc1_6_5_)    \n",
    "      - [**Explanation**](#toc1_6_5_1_)    \n",
    "      - [**Why It Is Important**](#toc1_6_5_2_)    \n",
    "  - [**3.0 Pre-process and encode the features**](#toc1_7_)    \n",
    "    - [**3.1 Categorical Data Preprocessing Strategy**](#toc1_7_1_)    \n",
    "    - [**3.1.1 Binary Encoding**](#toc1_7_2_)    \n",
    "      - [**Explanation**](#toc1_7_2_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_2_2_)    \n",
    "      - [**Observations**](#toc1_7_2_3_)    \n",
    "      - [**Conclusions**](#toc1_7_2_4_)    \n",
    "      - [**Recommendations**](#toc1_7_2_5_)    \n",
    "    - [**3.1.2 Ordinal Encoding**](#toc1_7_3_)    \n",
    "      - [**Explanation**](#toc1_7_3_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_3_2_)    \n",
    "      - [**Observations**](#toc1_7_3_3_)    \n",
    "      - [**Conclusions**](#toc1_7_3_4_)    \n",
    "      - [**Recommendations**](#toc1_7_3_5_)    \n",
    "    - [**3.1.3 One-Hot Encoding**](#toc1_7_4_)    \n",
    "      - [**Explanation**](#toc1_7_4_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_4_2_)    \n",
    "      - [**Observations**](#toc1_7_4_3_)    \n",
    "      - [**Conclusions**](#toc1_7_4_4_)    \n",
    "      - [**Recommendations**](#toc1_7_4_5_)    \n",
    "    - [**3.1.4 Handling High Cardinality**](#toc1_7_5_)    \n",
    "      - [**Explanation**](#toc1_7_5_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_5_2_)    \n",
    "      - [**Observations**](#toc1_7_5_3_)    \n",
    "      - [**Conclusions**](#toc1_7_5_4_)    \n",
    "      - [**Recommendations**](#toc1_7_5_5_)    \n",
    "    - [**3.1.5 Special Cases**](#toc1_7_6_)    \n",
    "      - [**3.1.5.1 Investigate Features With `XNA` Values**](#toc1_7_6_1_)    \n",
    "        - [**Explanation**](#toc1_7_6_1_1_)    \n",
    "        - [**Why It Is Important**](#toc1_7_6_1_2_)    \n",
    "        - [**Observations**](#toc1_7_6_1_3_)    \n",
    "        - [**Conclusions**](#toc1_7_6_1_4_)    \n",
    "        - [**Recommendations**](#toc1_7_6_1_5_)    \n",
    "      - [**3.1.5.2 Clean-up `CODE_GENDER` And Binary Encode The Feature**](#toc1_7_6_2_)    \n",
    "        - [**Explanation**](#toc1_7_6_2_1_)    \n",
    "        - [**Why It Is Important**](#toc1_7_6_2_2_)    \n",
    "        - [**Observations**](#toc1_7_6_2_3_)    \n",
    "        - [**Conclusions**](#toc1_7_6_2_4_)    \n",
    "        - [**Recommendations**](#toc1_7_6_2_5_)    \n",
    "    - [**3.1.6 Feature Engineering**](#toc1_7_7_)    \n",
    "      - [**Explanation**](#toc1_7_7_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_7_2_)    \n",
    "      - [**Observations**](#toc1_7_7_3_)    \n",
    "      - [**Conclusions**](#toc1_7_7_4_)    \n",
    "      - [**Recommendations**](#toc1_7_7_5_)    \n",
    "    - [**3.1.7 Handling Rare Categories**](#toc1_7_8_)    \n",
    "      - [**Explanation**](#toc1_7_8_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_8_2_)    \n",
    "      - [**Observations**](#toc1_7_8_3_)    \n",
    "      - [**Conclusions**](#toc1_7_8_4_)    \n",
    "      - [**Recommendations**](#toc1_7_8_5_)    \n",
    "    - [**3.1.8 Dealing with Unknown Values**](#toc1_7_9_)    \n",
    "      - [**3.1.8.1 Investigate Features With 'Unknown' As A Value**](#toc1_7_9_1_)    \n",
    "        - [**Explanation**](#toc1_7_9_1_1_)    \n",
    "        - [**Why It Is Important**](#toc1_7_9_1_2_)    \n",
    "        - [**Observations**](#toc1_7_9_1_3_)    \n",
    "        - [**Conclusions**](#toc1_7_9_1_4_)    \n",
    "        - [**Recommendations**](#toc1_7_9_1_5_)    \n",
    "      - [**3.1.8.2 Clean-up `NAME_FAMILY_STATUS` And One-Hot Encode The Feature**](#toc1_7_9_2_)    \n",
    "        - [**Explanation**](#toc1_7_9_2_1_)    \n",
    "        - [**Why It Is Important**](#toc1_7_9_2_2_)    \n",
    "        - [**Observations**](#toc1_7_9_2_3_)    \n",
    "        - [**Conclusions**](#toc1_7_9_2_4_)    \n",
    "        - [**Recommendations**](#toc1_7_9_2_5_)    \n",
    "      - [**3.1.9 Remove Object Type Features**](#toc1_7_9_3_)    \n",
    "        - [**Explanation**](#toc1_7_9_3_1_)    \n",
    "        - [**Why It Is Important**](#toc1_7_9_3_2_)    \n",
    "        - [**Observations**](#toc1_7_9_3_3_)    \n",
    "        - [**Conclusions**](#toc1_7_9_3_4_)    \n",
    "        - [**Recommendations**](#toc1_7_9_3_5_)    \n",
    "    - [**3.1.10 Correlation Heatmap**](#toc1_7_10_)    \n",
    "      - [**Explanation**](#toc1_7_10_1_)    \n",
    "      - [**Why It Is Important**](#toc1_7_10_2_)    \n",
    "      - [**Observations**](#toc1_7_10_3_)    \n",
    "      - [**Conclusions**](#toc1_7_10_4_)    \n",
    "      - [**Recommendations**](#toc1_7_10_5_)    \n",
    "    - [**3.1.10.1 Drop Highly Correlated Features**](#toc1_7_11_)    \n",
    "      - [Explanations](#toc1_7_11_1_)    \n",
    "      - [Why it's important:](#toc1_7_11_2_)    \n",
    "      - [Observations](#toc1_7_11_3_)    \n",
    "      - [Conclusions](#toc1_7_11_4_)    \n",
    "      - [Recommendations](#toc1_7_11_5_)    \n",
    "  - [**4.0 Analyze the distribution of the target variable (loan default rate)**](#toc1_8_)    \n",
    "      - [Explanations](#toc1_8_1_1_)    \n",
    "      - [Why it's important:](#toc1_8_1_2_)    \n",
    "      - [Observations](#toc1_8_1_3_)    \n",
    "      - [Conclusions](#toc1_8_1_4_)    \n",
    "      - [Recommendations](#toc1_8_1_5_)    \n",
    "  - [**5.0 Balance The Dataset**](#toc1_9_)    \n",
    "    - [**5.1 SMOTE**](#toc1_9_1_)    \n",
    "      - [Explanations](#toc1_9_1_1_)    \n",
    "      - [Why it's important:](#toc1_9_1_2_)    \n",
    "      - [Observations](#toc1_9_1_3_)    \n",
    "      - [Conclusions](#toc1_9_1_4_)    \n",
    "      - [Recommendations](#toc1_9_1_5_)    \n",
    "  - [**6.0 Visualize The Balanced/Imbalanced Data**](#toc1_10_)    \n",
    "      - [Explanations](#toc1_10_1_1_)    \n",
    "      - [Why it's important:](#toc1_10_1_2_)    \n",
    "      - [Observations](#toc1_10_1_3_)    \n",
    "      - [Conclusions](#toc1_10_1_4_)    \n",
    "      - [Recommendations](#toc1_10_1_5_)    \n",
    "  - [**7.0 Build And Train A Deep Learning Model**](#toc1_11_)    \n",
    "    - [**7.1 Test/Train Split and Data Normalization**](#toc1_11_1_)    \n",
    "    - [**7.2 Build And Train A Deep Learning Model**](#toc1_11_2_)    \n",
    "      - [Explanations](#toc1_11_2_1_)    \n",
    "      - [Why it's important:](#toc1_11_2_2_)    \n",
    "      - [Observations](#toc1_11_2_3_)    \n",
    "      - [Conclusions](#toc1_11_2_4_)    \n",
    "      - [Recommendations](#toc1_11_2_5_)    \n",
    "  - [**8.0 Evaluate the model using Sensitivity and ROC AUC metrics**](#toc1_12_)    \n",
    "      - [Explanations](#toc1_12_1_1_)    \n",
    "      - [Why it's important:](#toc1_12_1_2_)    \n",
    "      - [Observations](#toc1_12_1_3_)    \n",
    "      - [Conclusions](#toc1_12_1_4_)    \n",
    "      - [Recommendations](#toc1_12_1_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**Loan Default Prediction using Deep Learning**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "For a safe and secure lending experience, it's important to analyze the past data. In this project, you have to build a deep learning model to predict the chance of default for future loans using the historical data. As you will see, this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The main objective of this project is to create a deep learning model that can accurately predict whether an applicant will be able to repay a loan based on historical data. This involves:\n",
    "\n",
    "1. Analyzing and preprocessing the given dataset\n",
    "2. Handling imbalanced data\n",
    "3. Building and training a deep learning model\n",
    "4. Evaluating the model using appropriate metrics\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The dataset contains historical loan application data. It includes various features about loan applicants and a target variable indicating whether the loan was repaid or defaulted. The data is highly imbalanced, which presents an additional challenge for model training and evaluation.\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "1. Load the dataset\n",
    "2. Check for null values in the dataset\n",
    "3. Analyze the distribution of the target variable (loan default rate)\n",
    "4. Balance the dataset\n",
    "5. Visualize the balanced/imbalanced data\n",
    "6. Preprocess and encode the features\n",
    "7. Build and train a deep learning model\n",
    "8. Evaluate the model using Sensitivity and ROC AUC metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[**1.0 Load The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[**1.1 Setup: Import Necessary Libraries**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtYfv5plFnl5"
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn tensorflow category_encoders imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kxMwRpFnl6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = '/Users/toddwalters/Development/data/1688644938_dataset/1688644938_loan_data.csv'\n",
    "# Uncomment the following line to run in Google Colab\n",
    "# dataset_file = '/content/drive/MyDrive/Colab Notebooks/datasets/1688644938_dataset/1688644938_loan_data.csv'\n",
    "\n",
    "imputed_dataset_file = '/Users/toddwalters/Development/data/1688644938_dataset/1688644938_loan_data_imputed.csv'\n",
    "# Uncomment the following line to run in Google Colab\n",
    "# imputed_dataset_file = '/content/drive/MyDrive/Colab Notebooks/datasets/1688644938_dataset/1688644938_loan_data_imputed.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[**1.2 Loading The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0LPdLeSH338"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows and basic information about the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3wvj5HEH338"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzpqC3fOH338"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyyZjEjPH338"
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nThe df.describe output is:\\n\")\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(df.describe().transpose())\n",
    "\n",
    "print(f\"\\nThe df.describe output is:\\n\")\n",
    "print(df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[**2.0  Identify And Resolve Null Values In The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull values in the dataset:\")\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[**2.1  Drop Features With More Than 100K Null Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features with large amounts of missing data (any feature missing more than 100K of null values)\n",
    "df.drop(columns=[\n",
    "    'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_1_'></a>[**Explanations**](#toc0_)\n",
    "- This code drops features (columns) from the dataset where the number of missing values exceeds 100,000.\n",
    "\n",
    "#### <a id='toc1_6_1_2_'></a>[**Importance**](#toc0_)\n",
    "- Features with a high proportion of missing values can introduce noise, bias, or inaccuracies into the analysis. Removing such features helps to streamline the dataset and focus on features with more complete information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_2_'></a>[**2.2 KNN Imputation of Numerical Features**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_1_'></a>[**2.2.1 Create Feature Flag Categories**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_availability_flags(df, columns):\n",
    "    \"\"\"\n",
    "    Create binary flags indicating data availability for specified columns.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param columns: list of column names to create flags for\n",
    "    :return: DataFrame with added flag columns\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        flag_col_name = f\"{col}_FLAG\"\n",
    "        df[flag_col_name] = (~df[col].isnull()).astype(int)\n",
    "    return df\n",
    "\n",
    "# List of AMT_REQ_CREDIT_BUREAU features\n",
    "credit_bureau_features = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "]\n",
    "# Create availability flags\n",
    "df = create_availability_flags(df, credit_bureau_features)\n",
    "\n",
    "# Display info for flag columns\n",
    "print(\"\\nFlag columns:\")\n",
    "print(df[[col + '_FLAG' for col in credit_bureau_features]].info())\n",
    "print(f\"\\nFlag Feature Head:\")\n",
    "print(df[[col + '_FLAG' for col in credit_bureau_features]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_2_'></a>[**2.2.2 KNN Feature Imputation**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the features which were imputed by this function are not normalized in the returned dataframe. Let me explain why:\n",
    "\n",
    "1. The function does use normalization (specifically, standardization using `StandardScaler`) as part of the imputation process. This is done because KNN imputation works better with scaled data.\n",
    "\n",
    "2. However, after the imputation is performed, the function explicitly converts the data back to its original scale using `scaler.inverse_transform(imputed_features)`.\n",
    "\n",
    "3. The DataFrame is then updated with these inverse-transformed (i.e., original scale) values.\n",
    "\n",
    "So, the process goes like this:\n",
    "\n",
    "    Original data → Scaled data → Imputed scaled data → Inverse scaled (original scale) imputed data\n",
    "\n",
    "The final step ensures that the imputed values are on the same scale as the original data. This is important because it maintains the interpretability of the features and ensures consistency with any non-imputed values in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute_features(df, features, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Perform KNN imputation on specified columns.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param features: list of column names to impute\n",
    "    :param n_neighbors: number of neighbors to use for KNN imputation\n",
    "    :return: DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Prepare data for KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Scale the features\n",
    "    scaled_features = scaler.fit_transform(df[features])\n",
    "\n",
    "    # Perform KNN imputation\n",
    "    imputed_features = imputer.fit_transform(scaled_features)\n",
    "\n",
    "    # Convert back to original scale\n",
    "    imputed_features = scaler.inverse_transform(imputed_features)\n",
    "\n",
    "    # Update the DataFrame with imputed values\n",
    "    for i, col in enumerate(features):\n",
    "        df[col] = imputed_features[:, i]\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of all features that need KNN imputation\n",
    "all_features_to_impute = credit_bureau_features + [\n",
    "    'EXT_SOURCE_3',\n",
    "    'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "    'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "    'EXT_SOURCE_2'\n",
    "    # Add any other features that need imputation here\n",
    "]\n",
    "\n",
    "# Display info before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Check if the imputed file exists\n",
    "if os.path.exists(imputed_dataset_file):\n",
    "    print(\"Found pre-existing data imputation file.  Loading imputed data from file...\")\n",
    "    df = pd.read_csv(imputed_dataset_file)\n",
    "else:\n",
    "    print(\"Performing KNN imputation...\")\n",
    "    # Perform KNN imputation\n",
    "    df = knn_impute_features(df, all_features_to_impute)\n",
    "\n",
    "# Save the imputed dataframe if needed\n",
    "df.to_csv(imputed_dataset_file, index=False)\n",
    "\n",
    "print(\"\\nImputation complete. The specified features have been updated in the main dataframe.\\n\")\n",
    "\n",
    "# Display info after imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df[all_features_to_impute].info())\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in all_features_to_impute:\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())\n",
    "\n",
    "# Calculate the number of rows and columns needed for the subplots\n",
    "n_features = len(all_features_to_impute)\n",
    "n_cols = min(3, n_features)  # Max 3 columns\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axs = axs.ravel() if n_features > 1 else [axs]\n",
    "\n",
    "# Plot histograms for each feature\n",
    "for i, feature in enumerate(all_features_to_impute):\n",
    "    axs[i].hist(df[feature], bins=50)\n",
    "    axs[i].set_title(f'Distribution of {feature}\\nafter imputation')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <a id='toc1_6_2_2_1_'></a>[**Explanations**](#toc0_)\n",
    "\n",
    "This code provides a comprehensive method for handling missing data:\n",
    "\n",
    "1. Flags originally missing values\n",
    "2. Imputes missing values using KNN\n",
    "3. Provides summary statistics of imputed data\n",
    "4. Visualizes distribution of imputed features\n",
    "\n",
    "This approach ensures transparency in the imputation process and aids in understanding the impact of imputation on data distribution.\n",
    "\n",
    "\n",
    "1. **`create_availability_flags` Function**\n",
    "  \n",
    "    - **Input**: DataFrame and a list of column names\n",
    "    - **Process**:\n",
    "      - For each specified column, creates a new column with the suffix \"_FLAG\"\n",
    "      - Flag is 1 if the original value is not null, 0 if null\n",
    "    - **Purpose**: Tracks which values were originally missing before imputation\n",
    "\n",
    "2. **Creation of Flags for Credit Bureau Features**\n",
    "\n",
    "    - Defines a list of credit bureau features\n",
    "    - Calls `create_availability_flags` to create flag columns for these features\n",
    "    - Displays the resulting flag columns using `df.info()`\n",
    "\n",
    "3. **`knn_impute_features` Function**\n",
    "\n",
    "    - Scales input features using StandardScaler\n",
    "    - Performs KNN imputation using sklearn's KNNImputer\n",
    "    - Scales imputed values back to original range\n",
    "    - Updates original DataFrame with imputed values\n",
    "\n",
    "4. **KNN Imputation Process**\n",
    "\n",
    "    - Creates a list of all features to impute (credit bureau features and others)\n",
    "    - Displays DataFrame info before imputation\n",
    "    - Performs KNN imputation using `knn_impute_features` function\n",
    "    - Saves imputed DataFrame to CSV file\n",
    "    - Displays DataFrame info after imputation\n",
    "\n",
    "5. **Summary Statistics**\n",
    "\n",
    "   - For each imputed feature, prints summary statistics using `describe()`\n",
    "\n",
    "6. **Visualization**\n",
    "\n",
    "    - Creates a grid of subplots, one for each imputed feature\n",
    "    - Calculates number of rows and columns based on feature count\n",
    "    - For each feature:\n",
    "      - Plots a histogram showing distribution after imputation\n",
    "    - Removes any unused subplots\n",
    "    - Displays the resulting plot\n",
    "\n",
    "##### <a id='toc1_6_2_2_2_'></a>[**Why Is It Important**](#toc0_)\n",
    "- Imputing missing values is essential to prevent the loss of data points and to ensure that algorithms can work effectively. KNN imputation is a sophisticated method that leverages the relationships between features to estimate missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_3_'></a>[**2.3 Imputation of Missing Categorical Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical(df, column_name, strategy='proportional', missing_value=None, new_category_name='Unknown', distribute_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Impute missing values in a categorical column using various strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column_name: str, name of the column to impute\n",
    "    - strategy: str, one of 'proportional', 'new_category', 'mode'\n",
    "    - missing_value: value to be considered as missing (if None, will use pd.isnull())\n",
    "    - new_category_name: str, name of the new category if using 'new_category' strategy\n",
    "    - distribute_ratio: float, ratio of missing values to distribute when using 'proportional' strategy\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify missing values\n",
    "    if missing_value is None:\n",
    "        missing_mask = df[column_name].isnull()\n",
    "    else:\n",
    "        missing_mask = df[column_name] == missing_value\n",
    "\n",
    "    missing_count = missing_mask.sum()\n",
    "\n",
    "    if strategy == 'proportional':\n",
    "        # Calculate the distribution of non-missing values\n",
    "        value_counts = df[~missing_mask][column_name].value_counts()\n",
    "        total_non_missing = value_counts.sum()\n",
    "\n",
    "        # Calculate the number of values to distribute\n",
    "        distribute_count = int(missing_count * distribute_ratio)\n",
    "        other_count = missing_count - distribute_count\n",
    "\n",
    "        # Calculate the number of new values for each category\n",
    "        new_values = (value_counts / total_non_missing * distribute_count).round().astype(int)\n",
    "\n",
    "        # Adjust to ensure we have exactly the right number\n",
    "        while new_values.sum() + other_count < missing_count:\n",
    "            new_values[new_values.idxmax()] += 1\n",
    "        while new_values.sum() + other_count > missing_count:\n",
    "            new_values[new_values.idxmax()] -= 1\n",
    "\n",
    "        # Create a list of all new values\n",
    "        all_new_values = []\n",
    "        for category, count in new_values.items():\n",
    "            all_new_values.extend([category] * count)\n",
    "\n",
    "        # Add the new category for remaining values\n",
    "        all_new_values.extend([new_category_name] * other_count)\n",
    "\n",
    "        # Shuffle and assign new values\n",
    "        np.random.shuffle(all_new_values)\n",
    "        df.loc[missing_mask, column_name] = all_new_values\n",
    "\n",
    "    elif strategy == 'new_category':\n",
    "        # Simply replace all missing values with the new category name\n",
    "        df.loc[missing_mask, column_name] = new_category_name\n",
    "\n",
    "    elif strategy == 'mode':\n",
    "        # Replace missing values with the most frequent category\n",
    "        mode_value = df[~missing_mask][column_name].mode().iloc[0]\n",
    "        df.loc[missing_mask, column_name] = mode_value\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose 'proportional', 'new_category', or 'mode'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_3_1_'></a>[**2.3.1 Identify Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_object_features_with_nulls(df):\n",
    "    \"\"\"\n",
    "    Display object features with non-zero null values, their null counts, and percentages.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Get null counts for object features only\n",
    "    object_features = df.select_dtypes(include=['object'])\n",
    "    null_counts = object_features.isnull().sum()\n",
    "    \n",
    "    # Filter for non-zero nulls and sort\n",
    "    non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if non_zero_nulls.empty:\n",
    "        print(\"No object features found with null values.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_rows = len(df)\n",
    "    percentages = (non_zero_nulls / total_rows) * 100\n",
    "    \n",
    "    # Combine information\n",
    "    combined_info = pd.DataFrame({\n",
    "        'Null Count': non_zero_nulls,\n",
    "        'Null Percentage': percentages,\n",
    "        'Data Type': df[non_zero_nulls.index].dtypes\n",
    "    })\n",
    "    \n",
    "    print(\"Object features with non-zero null values:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.float_format', '{:.2f}%'.format):\n",
    "        print(combined_info)\n",
    "    \n",
    "    # Optional: Visualize the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    combined_info['Null Percentage'].plot(kind='bar')\n",
    "    plt.title(\"Percentage of Null Values in Object Features\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Percentage of Null Values\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "display_object_features_with_nulls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of Features of Type Object:\\n\")\n",
    "print(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "print(f\"\\nUnique values in OCCUPATION_TYPE: {df['OCCUPATION_TYPE'].unique()}\")\n",
    "print(f\"\\nUnique values in NAME_TYPE_SUITE: {df['NAME_TYPE_SUITE'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_3_2_'></a>[**2.3.2 Investigate Categorical Features With Missing Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_6_3_2_1_'></a>[**2.3.2.1 Imputation of Missing Categorical Data Within `OCCUPATION_TYPE` Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Fill missing values with 'Missing' to include them in the histogram\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "df = df.copy()\n",
    "\n",
    "df['OCCUPATION_TYPE'].fillna('Missing', inplace=True)\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['OCCUPATION_TYPE']\n",
    "\n",
    "# Plotting\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given this distribution, here's an approach to distribute the missing values across the other categories while maintaining the overall structure of the data:\n",
    "\n",
    "1. We'll use a proportional distribution method, but with a slight modification to account for the large number of missing values.\n",
    "2. Instead of directly distributing all missing values, we'll distribute a portion of them (e.g., 80%) across the existing categories based on their current proportions. This helps maintain the general distribution while not overly inflating the existing categories.\n",
    "3. The remaining portion (e.g., 20%) will be assigned to a new category called \"Other\" or \"Unspecified\". This accounts for the possibility that some of these missing values might genuinely be unknown or not fit into existing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation for OCCUPATION_TYPE\n",
    "df = impute_categorical(df, 'OCCUPATION_TYPE', strategy='proportional',\n",
    "                        missing_value='Missing', new_category_name='Other',\n",
    "                        distribute_ratio=0.8)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['OCCUPATION_TYPE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['OCCUPATION_TYPE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_6_3_2_2_'></a>[**2.3.2.2 Imputation of Missing Categorical Date Within `NAME_TYPE_SUITE` Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Fill missing values with 'Missing' to include them in the histogram\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "df = df.copy()\n",
    "\n",
    "df['NAME_TYPE_SUITE'].fillna('Missing', inplace=True)\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['NAME_TYPE_SUITE']\n",
    "\n",
    "# Plotting\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this distribution, here's an approach to distribute the missing values across the other categories while maintaining the overall structure of the data:\n",
    "\n",
    "1. Using the 'proportional' strategy to distribute the missing values across existing categories based on their current proportions.\n",
    "2. Setting distribute_ratio=0.95, which means 95% of the missing values will be distributed proportionally among existing categories, and only 5% will be assigned to the new 'Other' category.\n",
    "3. This approach will:\n",
    "\n",
    "    - Maintain the overall distribution of the data.\n",
    "    - Assign most of the missing values to 'Unaccompanied', reflecting the dominant trend in the data.\n",
    "    - Still allow for some diversity by assigning smaller portions to other categories.\n",
    "    - Create a small 'Other' category to account for any truly unknown or unique cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputation\n",
    "print(\"Before imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Apply the imputation\n",
    "df = impute_categorical(df, 'NAME_TYPE_SUITE', strategy='proportional',\n",
    "                        missing_value='Missing', new_category_name='Other_C',\n",
    "                        distribute_ratio=0.95)\n",
    "\n",
    "# After imputation\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df['NAME_TYPE_SUITE'].value_counts(dropna=False))\n",
    "\n",
    "# Visualize the new distribution\n",
    "columns_to_plot = ['NAME_TYPE_SUITE']\n",
    "\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 9))  # Adjust the size as needed\n",
    "    ax = sns.countplot(data=df, x=column, order=df[column].value_counts().index, hue=column, palette='viridis')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    # Annotate each bar with its total\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_3_3_'></a>[**Explanations**](#toc0_)\n",
    "\n",
    "This code checks for and displays the count of null values in each column of the dataset.\n",
    "\n",
    "#### <a id='toc1_6_3_4_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "Understanding the extent and distribution of missing data is crucial because:\n",
    "\n",
    "1. It affects the quality and reliability of our analysis and model predictions.\n",
    "2. It guides our data preprocessing strategy, including decisions on imputation or feature dropping.\n",
    "3. Missing data patterns might provide insights into data collection processes or inherent characteristics of certain variables.\n",
    "\n",
    "#### <a id='toc1_6_3_5_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "1. Many features have no missing values, including key variables like **TARGET** (***our prediction goal***) and basic applicant information.\n",
    "2. Several features have a significant number of missing values:\n",
    "\n",
    "    - **OWN_CAR_AGE**: 202,929 missing values\n",
    "    - **OCCUPATION_TYPE**: 96,391 missing values\n",
    "   - **EXT_SOURCE_1**: 173,378 missing values\n",
    "    - **EXT_SOURCE_3**: 60,965 missing values\n",
    "\n",
    "3. Most features related to building characteristics (e.g., **APARTMENTS_AVG, BASEMENTAREA_AVG**, etc.) have a large number of missing values, ranging from about 150,000 to 215,000.\n",
    "4. Credit bureau request features (**AMT_REQ_CREDIT_BUREAU_***) all have 41,519 missing values each.\n",
    "5. Some potentially important features like **AMT_ANNUITY** (*12 missing*) and **AMT_GOODS_PRICE** (ˆ) have a small number of null values.\n",
    "\n",
    "#### <a id='toc1_6_3_6_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "1. The dataset has a mixed pattern of missing values, with some features being complete and others having significant gaps.\n",
    "2. The high number of missing values in building-related features suggests these might not be applicable or available for all loan applications.\n",
    "3. The consistent number of missing values in credit bureau request features indicates a systematic reason for their absence, possibly related to data availability or collection processes.\n",
    "4. Core features for loan assessment (e.g., income, credit amount, target variable) are largely complete, which is positive for our analysis.\n",
    "\n",
    "#### <a id='toc1_6_3_7_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "1. For features with a small number of missing values (e.g., **AMT_ANNUITY, AMT_GOODS_PRICE**): consider using imputation techniques like mean, median, or advanced methods like KNN imputation.\n",
    "2. For features with a large number of missing values:\n",
    "    1. If they're deemed crucial (like **EXT_SOURCE** variables), consider advanced imputation techniques or creating a \"missing\" category.\n",
    "    2. If they're less important or redundant (like many of the building-related features), consider dropping them or creating aggregate features that combine information from related columns.\n",
    "3. For credit bureau request features: consider creating a binary flag indicating whether this information was available, in addition to any imputation strategy.\n",
    "4. Analyze the impact of missing values on the target variable to ensure that dropping or imputing doesn't introduce bias into the model.\n",
    "5. Document all decisions made regarding handling of missing data, as this will be crucial for model interpretation and future data processing.\n",
    "\n",
    "#### <a id='toc1_6_3_8_'></a>[**Decisions**](#toc0_)\n",
    "\n",
    "1.  I used mean or median imputation on the following Features with null values:\n",
    "\n",
    "    > 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'CNT_FAM_MEMBERS', 'DAYS_LAST_PHONE_CHANGE'\n",
    "\n",
    "2. I used a proportional imputation technique combined with creation of a \"Other\" category with the following categorical features with missing data:\n",
    "\n",
    "    > 'OCCUPATION_TYPE', NAME_TYPE_SUITE'\n",
    "\n",
    "3.  I dropped any feature with more than 100K null values:\n",
    "\n",
    "    > 'COMMONAREA_MEDI', 'COMMONAREA_AVG', 'COMMONAREA_MODE',\n",
    "    > 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    > 'FONDKAPREMONT_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MEDI',\n",
    "    > 'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    > 'YEARS_BUILD_MEDI', 'YEARS_BUILD_MODE', 'YEARS_BUILD_AVG',\n",
    "    > 'OWN_CAR_AGE', 'LANDAREA_MEDI', 'LANDAREA_MODE', 'LANDAREA_AVG',\n",
    "    > 'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE',\n",
    "    > 'EXT_SOURCE_1', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
    "    > 'ELEVATORS_MEDI', 'ELEVATORS_AVG', 'ELEVATORS_MODE',\n",
    "    > 'WALLSMATERIAL_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG', 'APARTMENTS_MODE',\n",
    "    > 'ENTRANCES_MEDI', 'ENTRANCES_AVG', 'ENTRANCES_MODE',\n",
    "    > 'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    > 'HOUSETYPE_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
    "    > 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
    "    > 'TOTALAREA_MODE', 'EMERGENCYSTATE_MODE'\n",
    "\n",
    "4. I created a feature flag category for each credit bureau request features that indicates whether this information was available\n",
    "\n",
    "5. I used KNN imputation on the following features that have null values:\n",
    "\n",
    "    > 'EXT_SOURCE_3', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_4_'></a>[**2.4 Mean/Median Imputation for Numerical Features**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mean_median(df, features_dict):\n",
    "    \"\"\"\n",
    "    Impute specified features using either mean or median.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param features_dict: dictionary with feature names as keys and 'mean' or 'median' as values\n",
    "    :return: DataFrame with imputed values and a summary of imputation\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    for feature, method in features_dict.items():\n",
    "        original_null_count = df[feature].isnull().sum()\n",
    "\n",
    "        if method == 'mean':\n",
    "            impute_value = df[feature].mean()\n",
    "        elif method == 'median':\n",
    "            impute_value = df[feature].median()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method for {feature}. Use 'mean' or 'median'.\")\n",
    "\n",
    "        df[feature].fillna(impute_value, inplace=True)\n",
    "\n",
    "        summary.append({\n",
    "            'Feature': feature,\n",
    "            'Method': method,\n",
    "            'Imputed Value': impute_value,\n",
    "            'Null Count Before': original_null_count,\n",
    "            'Null Count After': df[feature].isnull().sum()\n",
    "        })\n",
    "\n",
    "    return df, pd.DataFrame(summary)\n",
    "\n",
    "# Specify features and imputation methods\n",
    "features_to_impute = {\n",
    "    'AMT_GOODS_PRICE': 'median',\n",
    "    'AMT_ANNUITY': 'median',\n",
    "    'CNT_FAM_MEMBERS': 'median',\n",
    "    'DAYS_LAST_PHONE_CHANGE': 'median'\n",
    "}\n",
    "\n",
    "# Perform imputation\n",
    "df, imputation_summary = impute_mean_median(df, features_to_impute)\n",
    "\n",
    "# Display imputation summary\n",
    "print(\"Imputation Summary:\")\n",
    "print(imputation_summary)\n",
    "\n",
    "# Visualize distributions before and after imputation\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, feature in enumerate(features_to_impute.keys()):\n",
    "    axs[i].hist(df[feature], bins=50, alpha=0.7, label='After Imputation')\n",
    "    axs[i].set_title(f'Distribution of {feature}')\n",
    "    axs[i].set_xlabel('Value')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print summary statistics for each imputed feature\n",
    "for feature in features_to_impute.keys():\n",
    "    print(f\"\\nSummary statistics for {feature}:\")\n",
    "    print(df[feature].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_4_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code defines a function for imputing missing values in numerical columns using either the mean or median. It then applies this function to impute missing values in features like `AMT_GOODS_PRICE`, `AMT_ANNUITY`, `CNT_FAM_MEMBERS`, and `DAYS_LAST_PHONE_CHANGE`.\n",
    "\n",
    "1. Defines a function `impute_mean_median` that:\n",
    "\n",
    "    - Takes a DataFrame and a dictionary specifying features and their imputation methods.\n",
    "    - Imputes each feature using the specified method (mean or median).\n",
    "    - Keeps track of the imputation process for each feature.\n",
    "    - Returns the imputed DataFrame and a summary of the imputation process.\n",
    "\n",
    "2. Specifies the features to impute and their respective methods in a dictionary.\n",
    "3. Performs the imputation using the `impute_mean_median` function.\n",
    "4. Displays a summary of the imputation process, showing:\n",
    "\n",
    "    - The feature name\n",
    "    - The imputation method used\n",
    "    - The value used for imputation\n",
    "    - The number of null values before and after imputation\n",
    "\n",
    "5. Creates a visualization showing the distribution of each imputed feature after imputation.\n",
    "6. Optionally prints summary statistics for each imputed feature.\n",
    "\n",
    "This approach allows for flexibility in choosing the imputation method for each feature while providing transparency about the imputation process. The visualization and summary statistics help in understanding the impact of imputation on the data distribution.\n",
    "\n",
    "#### <a id='toc1_6_4_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Mean/median imputation is a simple yet effective way to fill in missing values in numerical features. It helps to maintain the overall distribution of the data and prevents the loss of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_5_'></a>[**2.5  Re-Check For Null Values In The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nFeatures with non-zero Null values and their data types:\")\n",
    "null_counts = df.isnull().sum()\n",
    "non_zero_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "# Get data types for features with non-zero null values\n",
    "data_types = df.dtypes[non_zero_nulls.index]\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    # Combine the non-zero nulls with their data types for display\n",
    "    combined_info = pd.DataFrame({'Null Counts': non_zero_nulls, 'Data Type': data_types})\n",
    "    print(combined_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_5_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code re-checks the dataset for any remaining null values after the imputation steps. It prints out the features with null values and their data types.\n",
    "\n",
    "#### <a id='toc1_6_5_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- This step is crucial to verify the effectiveness of the imputation process and to ensure that all missing values have been addressed before proceeding with further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[**3.0 Pre-process and encode the features**](#toc0_)\n",
    "\n",
    "The preprocessing strategy includes various encoding methods for categorical data, handling high cardinality, addressing special cases, and feature engineering. Each technique serves to convert categorical data into a numeric format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_1_'></a>[**3.1 Categorical Data Preprocessing Strategy**](#toc0_)\n",
    "\n",
    "**1. Binary Encoding:** For features with only two categories, use simple binary encoding.\n",
    "\n",
    "  - `FLAG_OWN_CAR`: {'N': 0, 'Y': 1}\n",
    "  - `FLAG_OWN_REALTY`: {'N': 0, 'Y': 1}\n",
    "\n",
    "**2. Ordinal Encoding:** For features with a clear ordinal relationship.\n",
    "\n",
    "  - **`NAME_EDUCATION_TYPE`**:\n",
    "    1. 'Lower secondary'\n",
    "    2. 'Secondary / secondary special'\n",
    "    3. 'Incomplete higher'\n",
    "    4. 'Higher education'\n",
    "    5. 'Academic degree'\n",
    "\n",
    "**3. One-Hot Encoding:** For nominal categories with low to medium cardinality.\n",
    "\n",
    "  - **`NAME_CONTRACT_TYPE`**\n",
    "  - **`NAME_TYPE_SUITE`**\n",
    "  - **`NAME_INCOME_TYPE`**\n",
    "  - **`NAME_FAMILY_STATUS`**\n",
    "  - **`NAME_HOUSING_TYPE`**\n",
    "  - **`WEEKDAY_APPR_PROCESS_START`**\n",
    "\n",
    "**4. Handling High Cardinality:** For features with many categories.\n",
    "\n",
    "  - **`OCCUPATION_TYPE`**: Consider grouping similar occupations or using target encoding.\n",
    "  - **`ORGANIZATION_TYPE`**: Group into broader categories or use target encoding.\n",
    "\n",
    "**5. Special Cases**\n",
    "\n",
    "  - **`CODE_GENDER`**:\n",
    "    - One-hot encode, but consider investigating and potentially removing 'XNA' entries.\n",
    "\n",
    "**6. Feature Engineering:** Consider creating new features:\n",
    "\n",
    "  - **`IS_WEEKEND`**: Binary feature derived from **`WEEKDAY_APPR_PROCESS_START`**.\n",
    "  - **`FAMILY_SIZE`**: Combine information from **`NAME_FAMILY_STATUS`** and **`NAME_TYPE_SUITE`**.\n",
    "\n",
    "**7. Handling Rare Categories:** For categories with very few occurrences.\n",
    "\n",
    "  - Group rare categories into an 'Other' category.\n",
    "  - Threshold: Consider grouping categories that appear in less than 1% of the data.\n",
    "\n",
    "**8. Dealing with Unknown Values**\n",
    "\n",
    "  - For 'Unknown' or 'XNA' values:\n",
    "    - If few, consider removing these entries.\n",
    "    - Otherwise, treat as a separate category in one-hot encoding.\n",
    "    - For ordinal features, assign a appropriate value (e.g., the median or a value outside the range)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_2_'></a>[**3.1.1 Binary Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].map({'N': 0, 'Y': 1})\n",
    "df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_2_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code converts binary categorical variables into numeric format by mapping 'N' to 0 and 'Y' to 1.\n",
    "\n",
    "#### <a id='toc1_7_2_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Binary encoding simplifies the dataset, making it compatible with machine learning algorithms that require numerical input\n",
    "\n",
    "#### <a id='toc1_7_2_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The output shows `FLAG_OWN_CAR` and `FLAG_OWN_REALTY` columns with 0 and 1 values, indicating successful binary encoding.\n",
    "\n",
    "#### <a id='toc1_7_2_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Binary encoding was applied correctly, facilitating the subsequent modeling process.\n",
    "\n",
    "#### <a id='toc1_7_2_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Ensure all binary categorical variables in the dataset are encoded similarly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_3_'></a>[**3.1.2 Ordinal Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_order = ['Lower secondary', 'Secondary / secondary special', 'Incomplete higher', 'Higher education', 'Academic degree']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
    "df['NAME_EDUCATION_TYPE'] = ordinal_encoder.fit_transform(df[['NAME_EDUCATION_TYPE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_3_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code converts ordinal categorical variables into numeric codes based on a defined order for educational levels.\n",
    "\n",
    "#### <a id='toc1_7_3_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Ordinal encoding maintains the inherent order in categorical data, which can impact the target variable.\n",
    "\n",
    "#### <a id='toc1_7_3_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The `NAME_EDUCATION_TYPE` column contains numeric values representing educational levels.\n",
    "\n",
    "#### <a id='toc1_7_3_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Ordinal encoding preserved the order of educational levels effectively.\n",
    "\n",
    "#### <a id='toc1_7_3_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Verify that the defined order for ordinal variables accurately reflects their true hierarchy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_4_'></a>[**3.1.3 One-Hot Encoding**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_4_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- One-hot encoding converts categorical variables into binary columns, preventing the model from interpreting false ordinal relationships.\n",
    "\n",
    "#### <a id='toc1_7_4_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- One-hot encoding avoids introducing ordinal bias and allows categorical data to be used in machine learning models.\n",
    "\n",
    "#### <a id='toc1_7_4_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The dataset has additional binary columns for each category within the specified categorical variables.\n",
    "\n",
    "#### <a id='toc1_7_4_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- One-hot encoding was successfully applied, enabling the use of categorical data in models.\n",
    "\n",
    "#### <a id='toc1_7_4_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Review the dataset to ensure no relevant categorical variables were omitted from the encoding process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_5_'></a>[**3.1.4 Handling High Cardinality**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TargetEncoder()\n",
    "df['OCCUPATION_TYPE_encoded'] = te.fit_transform(df['OCCUPATION_TYPE'], df['TARGET'])\n",
    "df['ORGANIZATION_TYPE_encoded'] = te.fit_transform(df['ORGANIZATION_TYPE'], df['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_5_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- Target encoding replaces each category with a mean target value, useful for high cardinality features.\n",
    "\n",
    "#### <a id='toc1_7_5_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Target encoding reduces dimensionality while retaining predictive power for high cardinality categorical variables.\n",
    "\n",
    "#### <a id='toc1_7_5_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The `OCCUPATION_TYPE` and `ORGANIZATION_TYPE` columns contain mean target values for each category.\n",
    "\n",
    "#### <a id='toc1_7_5_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Target encoding was effective in managing high cardinality categorical variables.\n",
    "\n",
    "#### <a id='toc1_7_5_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Use cross-validation to ensure target encoding does not lead to data leakage and overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_6_'></a>[**3.1.5 Special Cases**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_unknown_values(df, unknown_value):\n",
    "    \"\"\"\n",
    "    Identify categorical features that contain undefined values.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param unknown_value: The value to search for (default is 'Unknown')\n",
    "    :return: Dictionary with features and their undefined value counts\n",
    "    \"\"\"\n",
    "    # Select only object (string) and category dtype columns\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    unknown_features = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        # Count 'Unknown' values\n",
    "        unknown_count = (df[col] == unknown_value).sum()\n",
    "        \n",
    "        if unknown_count > 0:\n",
    "            # Calculate percentage\n",
    "            percentage = (unknown_count / len(df)) * 100\n",
    "            unknown_features[col] = {\n",
    "                'count': unknown_count,\n",
    "                'percentage': percentage\n",
    "            }\n",
    "    \n",
    "    return unknown_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_6_1_'></a>[**3.1.5.1 Investigate Features With `XNA` Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "unknown_features = identify_unknown_values(df, unknown_value='XNA')\n",
    "\n",
    "# Print results\n",
    "if unknown_features:\n",
    "    print(\"Features with 'XNA' values:\")\n",
    "    for feature, info in unknown_features.items():\n",
    "        print(f\"{feature}: {info['count']} 'XNA' values ({info['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"No features found with 'Unknown' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_7_6_1_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This function identifies and handles special cases where categorical values are unknown or incorrectly labeled.\n",
    "\n",
    "##### <a id='toc1_7_6_1_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Handling special cases ensures data quality and integrity, directly impacting model performance.\n",
    "\n",
    "##### <a id='toc1_7_6_1_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- Based on the fact that XNA represents 18% of the `ORGANIZATION_TYPE` Feature I will leave it as a Categorical Value associated with that feature.\n",
    "- Since there are only 4 entries that have `XNA` as a value in `CODE_GENDER` I will remove those lines from the dataframe.\n",
    "\n",
    "##### <a id='toc1_7_6_1_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Identification of special cases is crucial for addressing inconsistencies in the dataset.\n",
    "\n",
    "##### <a id='toc1_7_6_1_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Implement measures to clean or impute unknown values to improve data quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_6_2_'></a>[**3.1.5.2 Clean-up `CODE_GENDER` And Binary Encode The Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_binary_encode_gender(df, column='CODE_GENDER', remove_value='XNA', encoding_map={'M': 0, 'F': 1}, inplace=False):\n",
    "    \"\"\"\n",
    "    Remove specified value from gender column and apply binary encoding.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param column: name of the gender column\n",
    "    :param remove_value: value to be removed (e.g., 'XNA')\n",
    "    :param encoding_map: dictionary mapping original values to encoded values\n",
    "    :return: DataFrame with cleaned and encoded gender\n",
    "    \"\"\"\n",
    "    # Step 1: Print initial shape and distribution\n",
    "    print(f\"Initial shape of the dataset: {df.shape}\")\n",
    "    print(f\"\\nInitial {column} distribution:\")\n",
    "    print(df[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Step 2: Remove rows with specified value\n",
    "    df_cleaned = df[df[column] != remove_value].copy()\n",
    "    \n",
    "    # Step 3: Apply binary encoding\n",
    "    df_cleaned[column] = df_cleaned[column].map(encoding_map)\n",
    "    \n",
    "    # Step 4: Verify the changes\n",
    "    print(f\"\\nShape of the dataset after removing '{remove_value}': {df_cleaned.shape}\")\n",
    "    print(f\"\\n{column} distribution after encoding:\")\n",
    "    print(df_cleaned[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Check for any unexpected values\n",
    "    unexpected = df_cleaned[~df_cleaned[column].isin(encoding_map.values())]\n",
    "    if len(unexpected) > 0:\n",
    "        print(f\"\\nWarning: Unexpected values found in {column}:\")\n",
    "        print(unexpected[column].value_counts())\n",
    "    else:\n",
    "        print(f\"\\nAll {column} values successfully encoded.\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_cleaned[column].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {column} after Encoding')\n",
    "    plt.xlabel(f'{column} ({\", \".join([f\"{v}: {k}\" for k, v in encoding_map.items()])})')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    if inplace:\n",
    "        df[column] = df_cleaned[column]\n",
    "        return None\n",
    "    else:\n",
    "        return df_cleaned\n",
    "\n",
    "# Usage:\n",
    "# To get a new DataFrame:\n",
    "df_processed = clean_and_binary_encode_gender(df)\n",
    "\n",
    "# Update the dataset file with the cleaned and encoded data\n",
    "clean_and_binary_encode_gender(df, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_7_6_2_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code removes rows with 'XNA' values from 'CODE_GENDER' and applies binary encoding.\n",
    "\n",
    "##### <a id='toc1_7_6_2_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Cleaning and encoding categorical variables ensure the dataset is suitable for machine learning models.\n",
    "\n",
    "##### <a id='toc1_7_6_2_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The dataset shape changes after removing `XNA` values, and `CODE_GENDER` is successfully encoded.\n",
    "\n",
    "##### <a id='toc1_7_6_2_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Cleaning and encoding `CODE_GENDER` improved data quality and model compatibility.\n",
    "\n",
    "##### <a id='toc1_7_6_2_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Ensure similar cleaning and encoding procedures are applied to other categorical variables as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_7_'></a>[**3.1.6 Feature Engineering**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IS_WEEKEND'] = df['WEEKDAY_APPR_PROCESS_START'].isin(['SATURDAY', 'SUNDAY']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_7_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code creates a new binary feature indicating whether the application process started on a weekend.\n",
    "\n",
    "#### <a id='toc1_7_7_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Feature engineering can uncover hidden patterns, potentially improving model accuracy.\n",
    "\n",
    "#### <a id='toc1_7_7_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- A new 'IS_WEEKEND' column is added with binary values indicating weekends.\n",
    "\n",
    "#### <a id='toc1_7_7_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Feature engineering added a potentially useful feature to the dataset.\n",
    "\n",
    "#### <a id='toc1_7_7_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Continue exploring and creating new features to enhance model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_8_'></a>[**3.1.7 Handling Rare Categories**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rare_categories(series, threshold=0.01):\n",
    "    value_counts = series.value_counts(normalize=True)\n",
    "    mask = value_counts > threshold\n",
    "    return series.map(lambda x: x if x in value_counts[mask].index else 'Other')\n",
    "\n",
    "df['OCCUPATION_TYPE'] = group_rare_categories(df['OCCUPATION_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_8_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code groups rare categories into an 'Other' category to handle low-frequency categories.\n",
    "\n",
    "#### <a id='toc1_7_8_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Handling rare categories prevents overfitting and ensures model robustness.\n",
    "\n",
    "#### <a id='toc1_7_8_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- `OCCUPATION_TYPE` rare categories are grouped into 'Other'.\n",
    "\n",
    "#### <a id='toc1_7_8_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Grouping rare categories reduced noise and improved model stability.\n",
    "\n",
    "#### <a id='toc1_7_8_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Apply similar handling for other rare categories in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_9_'></a>[**3.1.8 Dealing with Unknown Values**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_9_1_'></a>[**3.1.8.1 Investigate Features With 'Unknown' As A Value**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_features = identify_unknown_values(df, unknown_value='Unknown')\n",
    "\n",
    "# Print results\n",
    "if unknown_features:\n",
    "    print(\"Features with 'Unknown' values:\")\n",
    "    for feature, info in unknown_features.items():\n",
    "        print(f\"{feature}: {info['count']} 'Unknown' values ({info['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"No features found with 'Unknown' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_7_9_1_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This function identifies columns containing 'Unknown' values and quantifies them.\n",
    "\n",
    "##### <a id='toc1_7_9_1_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Identifying unknown values is essential for cleaning and ensuring data integrity.\n",
    "\n",
    "##### <a id='toc1_7_9_1_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- `NAME_FAMILY_STATUS` contains 'Unknown' values, though very few.\n",
    "\n",
    "##### <a id='toc1_7_9_1_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- Identification of 'Unknown' values is crucial for cleaning the dataset.\n",
    "\n",
    "##### <a id='toc1_7_9_1_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Implement appropriate measures to clean or handle unknown values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_9_2_'></a>[**3.1.8.2 Clean-up `NAME_FAMILY_STATUS` And One-Hot Encode The Feature**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_encode_family_status(df, column='NAME_FAMILY_STATUS', inplace=False):\n",
    "    \"\"\"\n",
    "    Remove 'Unknown' values from NAME_FAMILY_STATUS and apply one-hot encoding.\n",
    "    \n",
    "    :param df: pandas DataFrame\n",
    "    :param column: name of the family status column\n",
    "    :param inplace: if True, modify the DataFrame in-place; if False, return a new DataFrame\n",
    "    :return: DataFrame with cleaned and encoded family status (if inplace=False)\n",
    "    \"\"\"\n",
    "    # Step 1: Print initial shape and distribution\n",
    "    print(\"Initial shape of the dataset:\", df.shape)\n",
    "    print(\"\\nInitial NAME_FAMILY_STATUS distribution:\")\n",
    "    print(df[column].value_counts(dropna=False))\n",
    "    \n",
    "    # Step 2: Remove rows where NAME_FAMILY_STATUS is 'Unknown'\n",
    "    df_cleaned = df[df[column] != 'Unknown'].copy()\n",
    "    \n",
    "    # Step 3: Apply one-hot encoding\n",
    "    df_encoded = pd.get_dummies(df_cleaned, columns=[column], prefix='FAMILY_STATUS')\n",
    "    \n",
    "    # Step 4: Verify the changes\n",
    "    print(\"\\nShape of the dataset after removing 'Unknown':\", df_encoded.shape)\n",
    "    print(\"\\nColumns created after one-hot encoding:\")\n",
    "    new_columns = [col for col in df_encoded.columns if col.startswith('FAMILY_STATUS_')]\n",
    "    print(new_columns)\n",
    "    \n",
    "    # Optional: Visualize the distribution of family status before and after\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    df[column].value_counts().plot(kind='bar', ax=ax1, title='Before Cleaning')\n",
    "    ax1.set_xlabel('Family Status')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    df_cleaned[column].value_counts().plot(kind='bar', ax=ax2, title='After Cleaning')\n",
    "    ax2.set_xlabel('Family Status')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if inplace:\n",
    "        # Update the original DataFrame\n",
    "        df.drop(index=df[df[column] == 'Unknown'].index, inplace=True)\n",
    "        new_columns = [col for col in df_encoded.columns if col.startswith('FAMILY_STATUS_')]\n",
    "        df[new_columns] = df_encoded[new_columns]\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "    else:\n",
    "        return df_encoded\n",
    "\n",
    "# Usage:\n",
    "# To get a new DataFrame:\n",
    "# df_processed = clean_and_encode_family_status(df)\n",
    "\n",
    "# To update the original DataFrame:\n",
    "clean_and_encode_family_status(df, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_7_9_2_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "1. **Function Definition:**\n",
    "   - `clean_and_encode_family_status` function is defined to clean and encode the 'NAME_FAMILY_STATUS' column.\n",
    "   - The function accepts three parameters: the DataFrame `df`, the column name `column`, and a boolean `inplace` to decide whether to modify the DataFrame in place or return a new DataFrame.\n",
    "\n",
    "2. **Initial Shape and Distribution:**\n",
    "   - Prints the initial shape of the dataset and the distribution of 'NAME_FAMILY_STATUS' values, including 'Unknown'.\n",
    "\n",
    "3. **Removing 'Unknown' Values:**\n",
    "   - Removes rows where 'NAME_FAMILY_STATUS' is 'Unknown'.\n",
    "\n",
    "4. **One-Hot Encoding:**\n",
    "   - Applies one-hot encoding to the cleaned DataFrame for the 'NAME_FAMILY_STATUS' column.\n",
    "\n",
    "5. **Verification:**\n",
    "   - Prints the shape of the DataFrame after removing 'Unknown' values and lists the new columns created by one-hot encoding.\n",
    "   - Optionally, visualizes the distribution of 'NAME_FAMILY_STATUS' before and after cleaning.\n",
    "\n",
    "6. **In-Place Update or Return:**\n",
    "   - If `inplace` is `True`, updates the original DataFrame. Otherwise, returns the cleaned and encoded DataFrame.\n",
    "\n",
    "##### <a id='toc1_7_9_2_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Cleaning and encoding categorical variables ensure the dataset is suitable for machine learning models. Removing 'Unknown' values prevents potential noise and bias in the data, while one-hot encoding allows categorical data to be used effectively in models.\n",
    "\n",
    "##### <a id='toc1_7_9_2_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- Initial shape of the dataset: (307511, 82)\n",
    "- Initial 'NAME_FAMILY_STATUS' distribution showed six categories, including 'Unknown' with 2 entries.\n",
    "- Shape after removing 'Unknown': (307509, 86)\n",
    "- New columns created after one-hot encoding:\n",
    "  - `FAMILY_STATUS_Civil marriage`\n",
    "  - `FAMILY_STATUS_Married`\n",
    "  - `FAMILY_STATUS_Separated`\n",
    "  - `FAMILY_STATUS_Single / not married`\n",
    "  - `FAMILY_STATUS_Widow`\n",
    "\n",
    "##### <a id='toc1_7_9_2_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- The cleaning and one-hot encoding of `NAME_FAMILY_STATUS` successfully removed 'Unknown' values and transformed the categorical variable into multiple binary columns, making it ready for machine learning algorithms.\n",
    "\n",
    "##### <a id='toc1_7_9_2_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Ensure similar cleaning and encoding procedures are applied to other categorical variables with 'Unknown' values.\n",
    "- Regularly check for any new 'Unknown' entries that might appear in updated datasets and apply similar cleaning steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_9_3_'></a>[**3.1.9 Remove Object Type Features**](#toc0_)\n",
    "\n",
    "Now that all of the Categorical Features have been Encoded we can remove those Features from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all remaining categorical features\n",
    "df.drop(columns=[\n",
    "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
    "    'NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
    "    'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_7_9_3_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This code first checks the data types of the columns to identify those with object types (categorical variables). Then, it drops these columns from the DataFrame.\n",
    "\n",
    "##### <a id='toc1_7_9_3_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Removing object type features after encoding ensures that the dataset contains only numeric data, which is necessary for most machine learning algorithms.\n",
    "\n",
    "##### <a id='toc1_7_9_3_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- The DataFrame no longer contains object type columns, indicating they have been successfully removed after encoding.\n",
    "\n",
    "##### <a id='toc1_7_9_3_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- The dataset is now entirely numeric, making it ready for machine learning algorithms.\n",
    "\n",
    "##### <a id='toc1_7_9_3_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "- Ensure all categorical features are properly encoded before removal to avoid losing valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_10_'></a>[**3.1.10 Correlation Heatmap**](#toc0_)\n",
    "\n",
    "**Interpretation and Next Steps**\n",
    "\n",
    "1. Look for highly correlated feature pairs (correlation close to 1 or -1).\n",
    "2. Consider removing one feature from highly correlated pairs to reduce multicollinearity.\n",
    "3. Identify features strongly correlated with your target variable (if included in the heatmap).\n",
    "4. Use these insights to guide your feature selection process.\n",
    "5. Be cautious about interpreting correlations for features that were originally categorical and then encoded.\n",
    "\n",
    "> Remember, correlation doesn't imply causation, but it's a valuable tool for understanding your data and informing your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df, figsize=(100, 75), correlation_threshold=0.75):\n",
    "    # Select only numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Find highly correlated feature pairs\n",
    "    high_correlation_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):  # Only lower triangle\n",
    "            if (abs(correlation_matrix.iloc[i, j]) >= correlation_threshold and \n",
    "                correlation_matrix.index[i] != correlation_matrix.columns[j]):\n",
    "                high_correlation_pairs.append((\n",
    "                    correlation_matrix.index[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "\n",
    "    # Sort pairs by absolute correlation value in descending order\n",
    "    high_correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    # Print highly correlated feature pairs\n",
    "    print(f\"\\nFeature pairs with correlation >= {correlation_threshold}:\")\n",
    "    for pair in high_correlation_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.4f}\")\n",
    "\n",
    "    return high_correlation_pairs\n",
    "\n",
    "# Usage:\n",
    "# Assuming 'df' is your DataFrame after preprocessing\n",
    "high_corr_pairs = plot_correlation_heatmap(df, correlation_threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_10_1_'></a>[**Explanation**](#toc0_)\n",
    "\n",
    "- This function creates a heatmap to visualize the correlation between numeric features. It identifies and lists highly correlated feature pairs based on a specified threshold.\n",
    "\n",
    "#### <a id='toc1_7_10_2_'></a>[**Why It Is Important**](#toc0_)\n",
    "\n",
    "- Understanding correlations between features helps in identifying multicollinearity, which can adversely affect model performance. Removing one of the highly correlated pairs can improve model stability and performance.\n",
    "\n",
    "#### <a id='toc1_7_10_3_'></a>[**Observations**](#toc0_)\n",
    "\n",
    "- Several feature pairs show high correlation (>= 0.75), indicating potential multicollinearity issues.\n",
    "\n",
    "#### <a id='toc1_7_10_4_'></a>[**Conclusions**](#toc0_)\n",
    "\n",
    "- High correlations between certain features necessitate careful selection or removal to enhance model performance.\n",
    "\n",
    "#### <a id='toc1_7_10_5_'></a>[**Recommendations**](#toc0_)\n",
    "\n",
    "1. AMT_REQ_CREDIT_BUREAU_* vs each other\n",
    "\n",
    "    These are almost perfectly positively correlated\n",
    "    Recommendation:  Keep AMT_REQ_CREDIT_BUREAU_YEAR_FLAG and eliminate AMT_REQ_CREDIT_BUREAU_HOUR_FLAG, AMT_REQ_CREDIT_BUREAU_DAY_FLAG, AMT_REQ_CREDIT_BUREAU_WEEK_FLAG, AMT_REQ_CREDIT_BUREAU_MON_FLAG, AMT_REQ_CREDIT_BUREAU_QRT_FLAG\n",
    "    Rationale: Information preservation: Since all these flags are perfectly correlated, keeping just one preserves all the information.\n",
    "\n",
    "2. FLAG_EMP_PHONE - DAYS_EMPLOYED: -0.9998\n",
    "\n",
    "    These are almost perfectly negatively correlated.\n",
    "    Recommendation: Keep DAYS_EMPLOYED and eliminate FLAG_EMP_PHONE.\n",
    "    Rationale: DAYS_EMPLOYED likely provides more granular information.\n",
    "\n",
    "\n",
    "3. OBS_60_CNT_SOCIAL_CIRCLE - OBS_30_CNT_SOCIAL_CIRCLE: 0.9985\n",
    "\n",
    "    Very high positive correlation.\n",
    "    Recommendation: Keep OBS_30_CNT_SOCIAL_CIRCLE and eliminate OBS_60_CNT_SOCIAL_CIRCLE.\n",
    "    Rationale: The 30-day metric might be more recent and relevant. However, if the 60-day metric is more standard in your industry, consider keeping that instead.\n",
    "\n",
    "\n",
    "4. AMT_GOODS_PRICE - AMT_CREDIT: 0.9867\n",
    "\n",
    "    Very high positive correlation.\n",
    "    Recommendation: Keep AMT_CREDIT and eliminate AMT_GOODS_PRICE.\n",
    "    Rationale: AMT_CREDIT likely encompasses more information about the loan.\n",
    "\n",
    "\n",
    "5. REGION_RATING_CLIENT_W_CITY - REGION_RATING_CLIENT: 0.9508\n",
    "\n",
    "    High positive correlation.\n",
    "    Recommendation: Keep REGION_RATING_CLIENT_W_CITY and eliminate REGION_RATING_CLIENT.\n",
    "    Rationale: The rating with city likely provides more specific information.\n",
    "\n",
    "\n",
    "6. CNT_FAM_MEMBERS - CNT_CHILDREN: 0.8792\n",
    "\n",
    "    Strong positive correlation, but not extremely high.\n",
    "    Recommendation: Keep both if possible, as they provide slightly different information.\n",
    "    If needed to reduce features, keep CNT_FAM_MEMBERS as it's more inclusive.\n",
    "\n",
    "\n",
    "7. LIVE_REGION_NOT_WORK_REGION - REG_REGION_NOT_WORK_REGION: 0.8606\n",
    "\n",
    "    Strong positive correlation.\n",
    "    Recommendation: Keep LIVE_REGION_NOT_WORK_REGION and eliminate REG_REGION_NOT_WORK_REGION.\n",
    "    Rationale: Current living situation might be more relevant than registered location.\n",
    "\n",
    "\n",
    "8. DEF_60_CNT_SOCIAL_CIRCLE - DEF_30_CNT_SOCIAL_CIRCLE: 0.8605\n",
    "\n",
    "    Strong positive correlation.\n",
    "    Recommendation: Similar to #2, keep DEF_30_CNT_SOCIAL_CIRCLE for consistency.\n",
    "    However, if 60-day metrics are standard in your industry, keep DEF_60_CNT_SOCIAL_CIRCLE instead.\n",
    "\n",
    "\n",
    "9. LIVE_CITY_NOT_WORK_CITY - REG_CITY_NOT_WORK_CITY: 0.8256\n",
    "\n",
    "    Strong positive correlation.\n",
    "    Recommendation: Keep LIVE_CITY_NOT_WORK_CITY and eliminate REG_CITY_NOT_WORK_CITY.\n",
    "    Rationale: Current living situation is likely more relevant than registered location.\n",
    "\n",
    "\n",
    "10. AMT_GOODS_PRICE - AMT_ANNUITY: 0.7748\n",
    "\n",
    "    Moderate to strong positive correlation.\n",
    "    Recommendation: Keep both if possible, as they represent different aspects of the loan.\n",
    "    If needed to reduce features, keep AMT_ANNUITY as it's directly related to loan repayment.\n",
    "\n",
    "\n",
    "11. AMT_ANNUITY - AMT_CREDIT: 0.7701\n",
    "\n",
    "    Moderate to strong positive correlation.\n",
    "    Recommendation: Keep both if possible, as they represent different aspects of the loan.\n",
    "    If needed to reduce features, keep AMT_CREDIT as it's more comprehensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_11_'></a>[**3.1.10.1 Drop Highly Correlated Features**](#toc0_)\n",
    "\n",
    "Currently for those high feature correlations where the recommendations were to keep both, I will be keeping both, but all other definitive feature eliminations will be dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all remaining categorical features\n",
    "df.drop(columns=[\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR_FLAG', 'AMT_REQ_CREDIT_BUREAU_DAY_FLAG', \n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK_FLAG', 'AMT_REQ_CREDIT_BUREAU_MON_FLAG', \n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT_FLAG', 'FLAG_EMP_PHONE', \n",
    "    'OBS_60_CNT_SOCIAL_CIRCLE', 'AMT_GOODS_PRICE', 'REGION_RATING_CLIENT', \n",
    "    'REG_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_WORK_CITY'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_11_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "- This code drops the highly correlated features identified in the previous step to reduce multicollinearity.\n",
    "\n",
    "#### <a id='toc1_7_11_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "- Removing highly correlated features enhances model performance by reducing redundancy and preventing multicollinearity.\n",
    "\n",
    "#### <a id='toc1_7_11_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The DataFrame now has fewer columns, indicating successful removal of highly correlated features.\n",
    "\n",
    "#### <a id='toc1_7_11_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "- Dropping highly correlated features streamlined the dataset and likely improved model robustness.\n",
    "\n",
    "#### <a id='toc1_7_11_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Regularly review and update feature selection based on correlation analysis to maintain optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[**4.0 Analyze the distribution of the target variable (loan default rate)**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the percentage of defaults\n",
    "default_rate = df['TARGET'].mean() * 100\n",
    "print(f\"\\nPercentage of defaults: {default_rate:.2f}%\")\n",
    "\n",
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='TARGET', data=df)\n",
    "plt.title('Distribution of Loan Defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_8_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code calculates the percentage of loan defaults and visualizes the distribution of the target variable.\n",
    "\n",
    "#### <a id='toc1_8_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the class distribution is crucial for binary classification problems, as imbalanced datasets can lead to biased models.\n",
    "\n",
    "#### <a id='toc1_8_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The percentage of defaults in the dataset\n",
    "- Visual representation of the class imbalance\n",
    "\n",
    "#### <a id='toc1_8_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "This analysis reveals whether we're dealing with a balanced or imbalanced dataset.\n",
    "\n",
    "#### <a id='toc1_8_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "-The dataset is heavily imbalanced, and using SMOTE is recommend to resolve the imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**5.0 Balance The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_9_1_'></a>[**5.1 SMOTE**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('TARGET', axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_9_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code applies the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset by creating synthetic examples of the minority class.\n",
    "\n",
    "#### <a id='toc1_9_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Balancing the dataset helps prevent the model from being biased towards the majority class, which is crucial for fair and accurate predictions, especially in loan default scenarios.\n",
    "\n",
    "#### <a id='toc1_9_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The change in the number of samples after applying SMOTE\n",
    "- The new ratio of default to non-default cases\n",
    "\n",
    "#### <a id='toc1_9_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "SMOTE has created a balanced dataset, which should help in training a more fair and accurate model.\n",
    "\n",
    "#### <a id='toc1_9_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Proceed with caution and validate the model's performance on both balanced and imbalanced test sets\n",
    "- Consider experimenting with other balancing techniques if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[**6.0 Visualize The Balanced/Imbalanced Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Distribution of Loan Defaults After SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_10_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code visualizes the distribution of the target variable after applying SMOTE.\n",
    "\n",
    "#### <a id='toc1_10_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Visualizing the balanced dataset confirms the effectiveness of the SMOTE technique and provides a clear comparison with the original imbalanced distribution.\n",
    "\n",
    "#### <a id='toc1_10_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The new distribution of default and non-default cases\n",
    "- Comparison with the original imbalanced distribution\n",
    "\n",
    "#### <a id='toc1_10_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The visualization confirms that SMOTE has successfully balanced the dataset.\n",
    "\n",
    "#### <a id='toc1_10_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Use this balanced dataset for model training\n",
    "- Keep the original imbalanced distribution in mind when interpreting model performance on real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[**7.0 Build And Train A Deep Learning Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_1_'></a>[**7.1 Test/Train Split and Data Normalization**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_11_2_'></a>[**7.2 Build And Train A Deep Learning Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_11_2_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "1. **Data Splitting:**\n",
    "   - Splits the resampled data into training (80%) and testing (20%) sets.\n",
    "   - Ensures we have separate data for training and evaluating the model.\n",
    "2. **Feature Scaling:**\n",
    "   - Uses StandardScaler to normalize the features.\n",
    "   - Fits the scaler on training data and applies it to both training and test data.\n",
    "3. **Model Architecture:**\n",
    "   - Creates a Sequential model with multiple Dense layers and Dropout for regularization.\n",
    "   - Uses ReLU activation for hidden layers and sigmoid for the output layer.\n",
    "4. **Model Compilation:**\n",
    "   - Uses Adam optimizer with a learning rate of 0.001.\n",
    "   - Binary cross-entropy loss function for binary classification.\n",
    "   - Tracks accuracy as a metric during training.\n",
    "5. **Model Training:**\n",
    "   - Fits the model on the training data for 50 epochs.\n",
    "   - Uses a batch size of 32 and a 20% validation split.\n",
    "\n",
    "#### <a id='toc1_11_2_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "1. **Data Splitting:** Ensures unbiased evaluation of the model's performance on unseen data.\n",
    "2. **Feature Scaling:** Normalizes features to the same scale, improving model convergence and performance.\n",
    "3. **Model Architecture:** Defines the structure capable of learning complex patterns in the data.\n",
    "4. **Dropout Layers:** Help prevent overfitting by randomly deactivating neurons during training.\n",
    "5. **Model Compilation:** Sets up the learning process, defining how the model should be optimized.\n",
    "6. **Model Training:** The actual learning process where the model adjusts its weights based on the training data.\n",
    "\n",
    "#### <a id='toc1_11_2_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "1. **Rapid Initial Improvement:**\n",
    "   - Accuracy jumps from 85.13% to 89.36% in the first epoch.\n",
    "   - Validation accuracy improves from 89.84% to 90.13%.\n",
    "2. **Consistent Improvement:**\n",
    "   - Both training and validation accuracy show steady improvement over epochs.\n",
    "3. **Final Performance:**\n",
    "   - *Training accuracy:* 91.01%\n",
    "   - *Validation accuracy:* 91.23%\n",
    "   - *Training loss:* 0.2251\n",
    "   - *Validation loss:* 0.2221\n",
    "4. **Convergence:**\n",
    "   - The model seems to converge around epoch 30-35, with minimal improvements thereafter.\n",
    "\n",
    "#### <a id='toc1_11_2_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "1. **Effective Learning:** The model demonstrates a good learning curve, consistently improving over epochs.\n",
    "2. **Good Generalization:** Validation metrics closely follow training metrics, suggesting the model generalizes well to unseen data.\n",
    "3. **No Significant Overfitting:** The close alignment of training and validation metrics indicates the model isn't overfitting substantially.\n",
    "4. **Satisfactory Performance:** Achieving over 91% accuracy on both training and validation sets is promising for a binary classification task.\n",
    "5. **Diminishing Returns:** The rate of improvement slows significantly after about 30 epochs.\n",
    "\n",
    "#### <a id='toc1_11_2_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "1. **Early Stopping:** Implement early stopping to prevent unnecessary computation. The model could potentially stop training around epoch 35-40.\n",
    "2. **Learning Rate Schedule:** Consider implementing a learning rate schedule that reduces the learning rate over time to fine-tune the model in later epochs.\n",
    "3. **Hyperparameter Tuning:** Experiment with different architectures, neuron counts, or dropout rates to potentially improve performance.\n",
    "4. **Cross-Validation:** Implement k-fold cross-validation to ensure the model's performance is consistent across different data subsets.\n",
    "5. **Feature Importance Analysis:** Analyze which features contribute most to the predictions to potentially simplify the model or focus on key features.\n",
    "6. **Ensemble Methods:** Consider creating an ensemble of models (e.g., bagging or boosting) to potentially improve overall performance.\n",
    "7. **Threshold Tuning:** Experiment with different classification thresholds to optimize for specific metrics (e.g., precision vs. recall) based on business requirements.\n",
    "8. **Advanced Techniques:** If further improvement is needed, consider advanced techniques like residual connections or batch normalization.\n",
    "9. **Model Interpretation:** Use techniques like SHAP values to understand and explain the model's predictions, which can be crucial for stakeholder buy-in and regulatory compliance in the financial sector.\n",
    "10. **Regular Retraining:** Set up a system to periodically retrain the model on new data to maintain its performance over time.\n",
    "\n",
    "By implementing these recommendations, you can potentially improve the model's performance, ensure its reliability, and maximize its value in predicting loan defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[**8.0 Evaluate the model using Sensitivity and ROC AUC metrics**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_12_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "1. **Model Prediction:**\n",
    "   - `y_pred_proba = model.predict(X_test_scaled)`: Generates probability predictions for the test set.\n",
    "   - `y_pred = (y_pred_proba > 0.5).astype(int)`: Converts probabilities to binary predictions using a 0.5 threshold.\n",
    "\n",
    "2. **Sensitivity Calculation:**\n",
    "   - Uses `confusion_matrix` to get true negatives (tn), false positives (fp), false negatives (fn), and true positives (tp).\n",
    "   - Calculates sensitivity as tp / (tp + fn).\n",
    "\n",
    "3. **ROC AUC Calculation:**\n",
    "   - Uses `roc_auc_score` to calculate the Area Under the Receiver Operating Characteristic curve.\n",
    "\n",
    "4. **Plotting:**\n",
    "   - Creates two subplots for loss and accuracy over epochs.\n",
    "   - Plots training and validation metrics for both loss and accuracy.\n",
    "\n",
    "#### <a id='toc1_12_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "1. **Imbalanced Data Handling:** Sensitivity and ROC AUC are particularly important for imbalanced datasets, which is common in loan default prediction.\n",
    "2. **Decision Making:** These metrics guide decisions on model deployment, further improvements, or feature engineering.\n",
    "3. **Sensitivity (Recall):**\n",
    "   - Measures the model's ability to correctly identify positive cases (loan defaults).\n",
    "   - Crucial in loan default prediction to minimize missed defaults.\n",
    "4. **ROC AUC:**\n",
    "   - Assesses the model's ability to distinguish between classes across various thresholds.\n",
    "   - Provides a single score that summarizes the model's performance.\n",
    "5. **Loss and Accuracy Plots:**\n",
    "   - Help visualize the model's learning progress and identify potential overfitting or underfitting.\n",
    "\n",
    "#### <a id='toc1_12_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "1. **High Sensitivity (0.8360):**\n",
    "   - The model correctly identifies 83.60% of actual loan defaults.\n",
    "   - The model is catching a good portion of positive cases, but there's room for improvement.\n",
    "\n",
    "2. **Excellent ROC AUC (0.9610):**\n",
    "   - Indicates strong discriminative power across different classification thresholds.\n",
    "\n",
    "3. **Learning Curves:**\n",
    "   - Rapid initial improvement in both loss and accuracy.\n",
    "   - Convergence of training and validation metrics, suggesting good generalization.\n",
    "   - Slight oscillations in validation accuracy in later epochs.\n",
    "\n",
    "4. **Final Metrics:**\n",
    "   - **Training Accuracy:** ~91.01%\n",
    "   - **Validation Accuracy:** ~91.23%\n",
    "   - **Training Loss:** ~0.2251\n",
    "   - **Validation Loss:** ~0.2221\n",
    "\n",
    "#### <a id='toc1_12_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "1. **Strong Overall Performance:** The model demonstrates strong predictive power with high accuracy and ROC AUC.\n",
    "2. **Good Generalization:** The close tracking of training and validation metrics suggests the model will likely perform well on unseen data.\n",
    "3. **Effective Learning:** The model shows consistent improvement throughout training, with diminishing returns in later epochs.\n",
    "4. **Balanced Precision-Recall Trade-off:** The high ROC AUC indicates a good balance between precision and recall across various thresholds.\n",
    "5. **Room for Improvement in Sensitivity:** While good, the sensitivity could potentially be improved to catch more positive cases.\n",
    "\n",
    "#### <a id='toc1_12_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "1. **Model Deployment:** Given the strong performance, this model is likely suitable for deployment in a loan default prediction system.\n",
    "2. **Threshold Tuning:** Experiment with different classification thresholds to potentially improve sensitivity without significantly impacting overall accuracy.\n",
    "3. **Feature Engineering:** Consider creating new features or transforming existing ones to potentially improve sensitivity.\n",
    "4. **Ensemble Methods:** Try ensemble techniques like Random Forests or Gradient Boosting to potentially improve overall performance and sensitivity.\n",
    "5. **Early Stopping:** Implement early stopping in future training to save computational resources, as improvements plateau in later epochs.\n",
    "6. **Class Weighting:** If improving sensitivity is a priority, consider applying class weights to give more importance to the minority class.\n",
    "7. **Regularization:** If there's any sign of overfitting in future iterations, consider adding regularization techniques like dropout or L2 regularization.\n",
    "8. **Cross-Validation:** Implement k-fold cross-validation to ensure consistent performance across different data subsets.\n",
    "9. **Advanced Techniques:** Explore techniques like focal loss or advanced sampling methods to further address any remaining class imbalance issues.\n",
    "10. **Error Analysis:** Conduct a detailed analysis of false negatives to understand what types of defaults the model is missing.\n",
    "11. **Regular Retraining:** Plan for periodic model retraining to adapt to potential data drift in the loan environment.\n",
    "12. **Interpretability:** Implement model interpretability techniques (e.g., SHAP values) to understand which features are driving the predictions, which can provide business insights and help in further feature engineering.\n",
    "\n",
    "By following these recommendations, you can potentially improve the model's performance, particularly in terms of sensitivity, while maintaining its strong overall predictive power. The high ROC AUC suggests that this model provides a solid foundation for accurate loan default prediction."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
