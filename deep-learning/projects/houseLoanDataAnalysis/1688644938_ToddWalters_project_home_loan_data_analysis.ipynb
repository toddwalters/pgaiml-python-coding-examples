{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/projects/automatingPortOperations/1714053668_ToddWalters_project_automating_port_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bY8iESXrFnl4",
    "outputId": "127ead91-357b-45ab-ada8-9eaedadcb38f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXqTWV3iFnl4"
   },
   "source": [
    "# <a id='toc1_'></a>[**Loan Default Prediction using Deep Learning**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ7XucjeFnl5"
   },
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "For a safe and secure lending experience, it's important to analyze the past data. In this project, you have to build a deep learning model to predict the chance of default for future loans using the historical data. As you will see, this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The main objective of this project is to create a deep learning model that can accurately predict whether an applicant will be able to repay a loan based on historical data. This involves:\n",
    "\n",
    "1. Analyzing and preprocessing the given dataset\n",
    "2. Handling imbalanced data\n",
    "3. Building and training a deep learning model\n",
    "4. Evaluating the model using appropriate metrics\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "The dataset contains historical loan application data. It includes various features about loan applicants and a target variable indicating whether the loan was repaid or defaulted. The data is highly imbalanced, which presents an additional challenge for model training and evaluation.\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "1. Load the dataset\n",
    "2. Check for null values in the dataset\n",
    "3. Analyze the distribution of the target variable (loan default rate)\n",
    "4. Balance the dataset\n",
    "5. Visualize the balanced/imbalanced data\n",
    "6. Preprocess and encode the features\n",
    "7. Build and train a deep learning model\n",
    "8. Evaluate the model using Sensitivity and ROC AUC metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHz_5EEfFnl5"
   },
   "source": [
    "## <a id='toc1_5_'></a>[**1.0 Load The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqVKXKtFnl5"
   },
   "source": [
    "### <a id='toc1_5_1_'></a>[**1.1 Setup: Import Necessary Libraries**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtYfv5plFnl5"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kxMwRpFnl6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNDE8nqrFnl6"
   },
   "source": [
    "### <a id='toc1_5_2_'></a>[**1.2 Loading The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yq_ODKMLFnl6",
    "outputId": "7233cc95-bfde-4f17-a2aa-9f53e103cb75"
   },
   "outputs": [],
   "source": [
    "# Load the dataset (replace 'loan_data.csv' with your actual filename)\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[**2.0  Check for null values in the dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull values in the dataset:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv79ZXhNFnl6"
   },
   "source": [
    "#### <a id='toc1_6_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code checks for and displays the count of null values in each column of the dataset.\n",
    "\n",
    "#### <a id='toc1_6_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Identifying missing data is crucial as it can significantly impact the model's performance and may require specific handling techniques.\n",
    "\n",
    "#### <a id='toc1_6_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- Columns with null values (if any)\n",
    "- The extent of missing data in affected columns\n",
    "\n",
    "#### <a id='toc1_6_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "Understanding the presence and extent of missing data guides our data preprocessing strategies.\n",
    "\n",
    "#### <a id='toc1_6_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- For columns with a small number of nulls, consider imputation techniques\n",
    "- For columns with a large number of nulls, consider dropping the column or using advanced imputation methods\n",
    "- If no nulls are present, proceed with the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ss5CNjBFnl7"
   },
   "source": [
    "## <a id='toc1_7_'></a>[**3.0 Analyze the distribution of the target variable (loan default rate)**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-VHKmZ_Fnl8",
    "outputId": "6ca8e081-4141-41a9-da8d-a846ab18349c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print the percentage of defaults\n",
    "default_rate = df['TARGET'].mean() * 100\n",
    "print(f\"\\nPercentage of defaults: {default_rate:.2f}%\")\n",
    "\n",
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='TARGET', data=df)\n",
    "plt.title('Distribution of Loan Defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMc_qxKsFnl8"
   },
   "source": [
    "#### <a id='toc1_7_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code calculates the percentage of loan defaults and visualizes the distribution of the target variable.\n",
    "\n",
    "#### <a id='toc1_7_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding the class distribution is crucial for binary classification problems, as imbalanced datasets can lead to biased models.\n",
    "\n",
    "#### <a id='toc1_7_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The percentage of defaults in the dataset\n",
    "- Visual representation of the class imbalance\n",
    "\n",
    "#### <a id='toc1_7_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "This analysis reveals whether we're dealing with a balanced or imbalanced dataset.\n",
    "\n",
    "#### <a id='toc1_7_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- If the dataset is heavily imbalanced, consider using techniques like SMOTE, undersampling, or adjusting class weights\n",
    "- If relatively balanced, proceed with caution and monitor for potential bias in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twtsj9AVFnl9"
   },
   "source": [
    "## <a id='toc1_8_'></a>[**4.0 Balance The Dataset**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('TARGET', axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7pZXpd0Fnl9"
   },
   "source": [
    "#### <a id='toc1_8_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code applies the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset by creating synthetic examples of the minority class.\n",
    "\n",
    "#### <a id='toc1_8_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Balancing the dataset helps prevent the model from being biased towards the majority class, which is crucial for fair and accurate predictions, especially in loan default scenarios.\n",
    "\n",
    "#### <a id='toc1_8_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The change in the number of samples after applying SMOTE\n",
    "- The new ratio of default to non-default cases\n",
    "\n",
    "#### <a id='toc1_8_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "SMOTE has created a balanced dataset, which should help in training a more fair and accurate model.\n",
    "\n",
    "#### <a id='toc1_8_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Proceed with caution and validate the model's performance on both balanced and imbalanced test sets\n",
    "- Consider experimenting with other balancing techniques if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**5.0 Visualize The Balanced/Imbalanced Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Distribution of Loan Defaults After SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_8_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code visualizes the distribution of the target variable after applying SMOTE.\n",
    "\n",
    "#### <a id='toc1_8_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Visualizing the balanced dataset confirms the effectiveness of the SMOTE technique and provides a clear comparison with the original imbalanced distribution.\n",
    "\n",
    "#### <a id='toc1_8_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The new distribution of default and non-default cases\n",
    "- Comparison with the original imbalanced distribution\n",
    "\n",
    "#### <a id='toc1_8_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The visualization confirms that SMOTE has successfully balanced the dataset.\n",
    "\n",
    "#### <a id='toc1_8_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Use this balanced dataset for model training\n",
    "- Keep the original imbalanced distribution in mind when interpreting model performance on real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**6.0 Pre-process and encode the features**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_9_1_'></a>[**Part_6_1**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in X.select_dtypes(include=['object']):\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_9_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code preprocesses the data by encoding categorical variables, splitting the data into training and testing sets, and scaling the features.\n",
    "\n",
    "#### <a id='toc1_9_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Proper preprocessing ensures that the data is in a suitable format for the deep learning model and that the model's performance can be accurately evaluated.\n",
    "\n",
    "#### <a id='toc1_9_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The transformation of categorical variables into numerical format\n",
    "- The split of data into training and testing sets\n",
    "- The scaling of features to a common range\n",
    "\n",
    "#### <a id='toc1_9_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The data is now properly encoded, split, and scaled, ready for model training.\n",
    "\n",
    "#### <a id='toc1_9_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Ensure that the same preprocessing steps are applied to any new data used for predictions\n",
    "- Consider using cross-validation for a more robust evaluation of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[**7.0 Build and train a deep learning model**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_10_1_'></a>[**Part_6_1**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_10_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "This code defines a deep learning model architecture, compiles the model with appropriate loss function and optimizer, and trains the model on the preprocessed data.\n",
    "\n",
    "#### <a id='toc1_10_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Building and training the model is the core of the project, where the patterns in the data are learned to make predictions on loan defaults.\n",
    "\n",
    "#### <a id='toc1_10_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "- The model architecture (number of layers, neurons, activation functions)\n",
    "- The training process (number of epochs, batch size)\n",
    "- The training and validation accuracy/loss over epochs\n",
    "\n",
    "#### <a id='toc1_10_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The model has been trained on the balanced dataset and should be capable of predicting loan defaults.\n",
    "\n",
    "#### <a id='toc1_10_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Monitor the training process for signs of overfitting or underfitting\n",
    "- Experiment with different architectures or hyperparameters if the performance is not satisfactory\n",
    "- Consider using techniques like early stopping to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
