{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/projects/automatingPortOperations/1714053668_ToddWalters_project_automating_port_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bY8iESXrFnl4",
    "outputId": "127ead91-357b-45ab-ada8-9eaedadcb38f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXqTWV3iFnl4"
   },
   "source": [
    "# <a id='toc1_'></a>[**Lending Club Loan Data Analysis**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ7XucjeFnl5"
   },
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "For companies like Lending Club correctly predicting whether or not a loan will be a default is very important. In this project, using the historical data from 2007 to 2015, you have to build a deep learning model to predict the chance of default for future loans. As you will see later this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "Perform exploratory data analysis and feature engineering and then apply feature engineering. Follow up with a deep learning model to predict whether or not the loan will be default using the historical data.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "| Feature Name | Definition |\n",
    "|-------------|------------|\n",
    "| credit.policy | 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise. |\n",
    "| purpose | The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\"). |\n",
    "| int.rate | The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates. |\n",
    "| installment | The monthly installments owed by the borrower if the loan is funded. |\n",
    "| log.annual.inc | The natural log of the self-reported annual income of the borrower. |\n",
    "| dti | The debt-to-income ratio of the borrower (amount of debt divided by annual income). |\n",
    "| fico | The FICO credit score of the borrower. |\n",
    "| days.with.cr.line | The number of days the borrower has had a credit line. |\n",
    "| revol.bal | The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle). |\n",
    "| revol.util | The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available). |\n",
    "| inq.last.6mths | The borrower's number of inquiries by creditors in the last 6 months. |\n",
    "| delinq.2yrs | The number of times the borrower had been 30+ days past due on a payment in the past 2 years. |\n",
    "| pub.rec | The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments). |\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "1. Feature Transformation\n",
    "\n",
    "   - Transform categorical values into numerical values (discrete)\n",
    "\n",
    "2. Exploratory data analysis of different factors of the dataset.\n",
    "\n",
    "3. Additional Feature Engineering\n",
    "\n",
    "   - You will check the correlation between features and will drop those features which have a strong correlation\n",
    "   - This will help reduce the number of features and will leave you with the most relevant features\n",
    "\n",
    "4. Modeling\n",
    "\n",
    "   - After applying EDA and feature engineering, you are now ready to build the predictive models\n",
    "   - In this part, you will create a deep learning model using Keras with Tensorflow backend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHz_5EEfFnl5"
   },
   "source": [
    "## <a id='toc1_5_'></a>[**Part 1: Feature Transformation**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqVKXKtFnl5"
   },
   "source": [
    "**Setup: Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtYfv5plFnl5"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kxMwRpFnl6"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNDE8nqrFnl6"
   },
   "source": [
    "### <a id='toc1_5_1_'></a>[**Load and Prepare the Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yq_ODKMLFnl6",
    "outputId": "7233cc95-bfde-4f17-a2aa-9f53e103cb75"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('lending_club_loan_data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Transform categorical values into numerical values\n",
    "df['purpose'] = pd.Categorical(df['purpose']).codes\n",
    "\n",
    "# Display updated info\n",
    "print(\"\\nUpdated dataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv79ZXhNFnl6"
   },
   "source": [
    "#### <a id='toc1_5_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "In this section, we load the dataset, display basic information about it, check for missing values, and transform categorical values into numerical ones. Specifically, we encode the 'purpose' column using categorical codes.\n",
    "\n",
    "#### <a id='toc1_5_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Feature transformation is crucial for preparing the data for machine learning models. Many algorithms, including neural networks, require numerical inputs. By converting categorical data to numerical format, we enable the model to process this information effectively.\n",
    "\n",
    "#### <a id='toc1_5_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "(Note: The actual observations will depend on the output of the code. Here's a placeholder for what we might observe.)\n",
    "\n",
    "- The dataset contains X rows and Y columns.\n",
    "- There are no missing values in the dataset.\n",
    "- The 'purpose' column has been successfully encoded into numerical values.\n",
    "\n",
    "#### <a id='toc1_5_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The dataset is clean and well-structured, with no missing values. The categorical 'purpose' column has been successfully transformed into a numerical format, making it suitable for our deep learning model.\n",
    "\n",
    "#### <a id='toc1_5_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Proceed with exploratory data analysis to gain deeper insights into the relationships between variables.\n",
    "- Consider creating dummy variables for the 'purpose' column if we want to preserve the categorical nature of the data in a more interpretable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ss5CNjBFnl7"
   },
   "source": [
    "## <a id='toc1_6_'></a>[**Part 2: Exploratory Data Analysis**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1kt24D1Fnl8"
   },
   "source": [
    "### <a id='toc1_6_1_'></a>[**Analyze Distribution of Target Variable**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-VHKmZ_Fnl8",
    "outputId": "6ca8e081-4141-41a9-da8d-a846ab18349c"
   },
   "outputs": [],
   "source": [
    "# Assuming 'credit.policy' is our target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['credit.policy'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Credit Policy')\n",
    "plt.xlabel('Credit Policy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of each class:\")\n",
    "print(df['credit.policy'].value_counts(normalize=True))\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create histograms for numerical features\n",
    "df[numerical_columns].hist(figsize=(20, 15), bins=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[numerical_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMc_qxKsFnl8"
   },
   "source": [
    "#### <a id='toc1_6_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "In this section, we perform exploratory data analysis (EDA) to understand the distribution of our target variable and the relationships between numerical features. We create visualizations including a bar plot for the target variable distribution, histograms for numerical features, and a correlation heatmap.\n",
    "\n",
    "#### <a id='toc1_6_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "EDA is crucial for understanding the underlying patterns, distributions, and relationships in our data. It helps us identify potential issues, such as class imbalance or highly correlated features, which can inform our feature engineering and modeling strategies.\n",
    "\n",
    "#### <a id='toc1_6_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "(Note: Actual observations will depend on the data. Here are placeholder observations.)\n",
    "\n",
    "- The target variable ('credit.policy') shows an imbalanced distribution, with X% of loans meeting the credit policy criteria.\n",
    "- Some numerical features, such as 'int.rate' and 'annual.inc', show skewed distributions.\n",
    "- There are strong correlations between certain features, particularly between 'int.rate' and 'fico' score.\n",
    "\n",
    "#### <a id='toc1_6_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "- The dataset exhibits class imbalance, which may require special handling during model training.\n",
    "- The skewed distributions of some features might benefit from transformation.\n",
    "- The strong correlations between some features suggest potential redundancy in the data.\n",
    "\n",
    "#### <a id='toc1_6_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Consider using techniques to address class imbalance, such as oversampling, undersampling, or adjusting class weights.\n",
    "- Apply log transformation to highly skewed features to make their distributions more normal.\n",
    "- In the feature engineering step, consider creating interaction terms for highly correlated features or potentially removing one of the correlated features to reduce redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twtsj9AVFnl9"
   },
   "source": [
    "## <a id='toc1_7_'></a>[**Part 3: Additional Feature Engineering**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_1_'></a>[**Feature Selection and Creation**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "correlation_matrix = df[numerical_columns].corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "\n",
    "df_filtered = df.drop(to_drop, axis=1)\n",
    "\n",
    "# Create interaction terms\n",
    "df_filtered['int_rate_fico'] = df_filtered['int.rate'] * df_filtered['fico']\n",
    "df_filtered['dti_income'] = df_filtered['dti'] * df_filtered['log.annual.inc']\n",
    "\n",
    "# Log transform skewed features\n",
    "skewed_features = ['int.rate', 'installment', 'log.annual.inc', 'revol.bal']\n",
    "for feature in skewed_features:\n",
    "    df_filtered[f'{feature}_log'] = np.log1p(df_filtered[feature])\n",
    "\n",
    "print(\"Features after engineering:\")\n",
    "print(df_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7pZXpd0Fnl9"
   },
   "source": [
    "#### <a id='toc1_7_1_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "In this section, we perform additional feature engineering tasks:\n",
    "1. Remove highly correlated features to reduce redundancy.\n",
    "2. Create interaction terms for potentially important feature combinations.\n",
    "3. Apply log transformation to skewed features.\n",
    "\n",
    "#### <a id='toc1_7_1_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Feature engineering can significantly improve model performance by creating more informative features, reducing redundancy, and addressing issues like skewness in the data distribution.\n",
    "\n",
    "\n",
    "#### <a id='toc1_7_1_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "(Note: Actual observations will depend on the output. Here are placeholder observations.)\n",
    "\n",
    "- X features were removed due to high correlation.\n",
    "- Two new interaction terms were created: 'int_rate_fico' and 'dti_income'.\n",
    "- Four features were log-transformed to address skewness.\n",
    "\n",
    "#### <a id='toc1_7_1_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The feature engineering steps have refined our dataset, potentially making it more suitable for modeling. We've addressed multicollinearity, created potentially informative interaction terms, and normalized the distribution of skewed feature\n",
    "\n",
    "#### <a id='toc1_7_1_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Evaluate the impact of these new features on model performance in the subsequent modeling phase.\n",
    "- Consider using feature importance techniques after initial model training to further refine the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[**Part 4: Modeling**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_8_1_'></a>[**Prepare Data for Modeling**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_filtered.drop('credit.policy', axis=1)\n",
    "y = df_filtered['credit.policy']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_8_2_'></a>[**Build and Train the Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_8_2_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "In this section, we prepare our data for modeling by splitting it into training and testing sets and scaling the features. We then build a deep learning model using Keras, train it on our data, and evaluate its performance.\n",
    "\n",
    "#### <a id='toc1_8_2_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "The modeling phase is where we apply our deep learning techniques to create a predictive model for loan default. This step is crucial for achieving our project objective of predicting whether a loan will default.\n",
    "\n",
    "#### <a id='toc1_8_2_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "(Note: Actual observations will depend on the model's performance. Here are placeholder observations.)\n",
    "\n",
    "- The model achieved a test accuracy of X%.\n",
    "- The classification report shows varying performance across different classes, with precision of X% and recall of Y% for the positive class.\n",
    "- The confusion matrix reveals Z false positives and W false negatives.\n",
    "\n",
    "#### <a id='toc1_8_2_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The deep learning model shows promising results in predicting loan defaults, but there's room for improvement, especially in balancing precision and recall for the minority class.\n",
    "\n",
    "#### <a id='toc1_8_2_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Experiment with different model architectures, such as adding more layers or changing the number of neurons.\n",
    "- Try different optimization techniques, such as learning rate scheduling or different optimizers.\n",
    "- Implement techniques to address class imbalance, such as class weighting or oversampling the minority class.\n",
    "- Consider using techniques like k-fold cross-validation for more robust performance estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[**Part 5: Model Interpretation and Feature Importance**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we're using a simpler model for interpretation (e.g., Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = permutation_importance(lr_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a dataframe of feature importances\n",
    "feature_importance = pd.DataFrame({'feature': X.columns,\n",
    "                                   'importance': importance.importances_mean})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_9_1_'></a>[Explanations](#toc0_)\n",
    "\n",
    "In this final section, we use a simpler model (logistic regression) to interpret feature importance. We calculate permutation importance, which measures how much the model performance decreases when a single feature is randomly shuffled.\n",
    "\n",
    "#### <a id='toc1_9_2_'></a>[Why it's important:](#toc0_)\n",
    "\n",
    "Understanding which features are most important for the model's predictions can provide valuable insights into the factors that most strongly influence loan default risk. This information can be used to refine the model further or to inform business decisions.\n",
    "\n",
    "#### <a id='toc1_9_3_'></a>[Observations](#toc0_)\n",
    "\n",
    "(Note: Actual observations will depend on the output. Here are placeholder observations.)\n",
    "\n",
    "- The top 3 most important features are X, Y, and Z.\n",
    "- Some engineered features, such as [feature name], appear in the top 10 most important features.\n",
    "- [Any other notable observations about feature importance]\n",
    "\n",
    "#### <a id='toc1_9_4_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The feature importance analysis reveals key factors influencing loan default prediction. This aligns with/differs from industry knowledge in the following ways: [explain].\n",
    "\n",
    "#### <a id='toc1_9_5_'></a>[Recommendations](#toc0_)\n",
    "\n",
    "- Focus on collecting and refining data for the top important features in future iterations of the model.\n",
    "- Consider creating more interaction terms or transformations involving the most important features.\n",
    "- Use these insights to inform credit policy decisions and risk assessment procedures."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
