{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**Automating Port Operations**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "Marina Pier Inc. is leveraging technology to automate their operations on the San Francisco port.\n",
    "The companyâ€™s management has set out to build a bias-free/ corruption-free automatic system that reports & avoids faulty situations caused by human error. \n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "Marina Pier wants to use Deep Learning techniques to build an automatic reporting system that recognizes the boat. The company is also looking to use a transfer learning approach of any lightweight pre-trained model in order to deploy in mobile devices.\n",
    "As a deep learning engineer, your task is to:\n",
    "\n",
    "1.\tBuild a CNN network to classify the boat.\n",
    "\n",
    "2.\tBuild a lightweight model with the aim of deploying the solution on a mobile device using transfer learning. You can use any lightweight pre-trained model as the initial (first) layer. MobileNetV2 is a popular lightweight pre-trained model built using Keras API. \n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Dataset**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "**boat_type_classification_dataset.zip**\n",
    "\n",
    "The dataset contains images of 9 types of boats. It contains a total of 1162 images. The training images are provided in the directory of the specific class itself. \n",
    "Classes:\n",
    "\n",
    "- ferry_boat\n",
    "- gondola\n",
    "- sailboat\n",
    "- cruise_ship\n",
    "- kayak\n",
    "- inflatable_boat\n",
    "- paper_boat\n",
    "- buoy\n",
    "- freight_boat\n",
    "\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Analysis Steps to Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "1.\tBuild a CNN network to classify the boat.\n",
    "\n",
    "    1.1.\tSplit the dataset into train and test in the ratio 80:20, with shuffle and random state=43. \n",
    "\n",
    "    1.2.\tUse tf.keras.preprocessing.image_dataset_from_directory to load the train and test datasets. This function also supports data normalization.*(Hint: image_scale=1./255)*\n",
    "\n",
    "    1.3.\tLoad train, validation and test dataset in batches of 32 using the function initialized in the above step. \n",
    "\n",
    "    1.4.\tBuild a CNN network using Keras with the following layers\n",
    "\n",
    "       - Cov2D with 32 filters, kernel size 3,3, and activation relu, followed by MaxPool2D\n",
    "\n",
    "       - Cov2D with 32 filters, kernel size 3,3, and activation relu, followed by MaxPool2D\n",
    "\n",
    "       - GLobalAveragePooling2D layer\n",
    "\n",
    "       - Dense layer with 128 neurons and activation relu\n",
    "\n",
    "       - Dense layer with 128 neurons and activation relu\n",
    "    \n",
    "       - Dense layer with 9 neurons and activation softmax.\n",
    "\n",
    "    1.5.\tCompile the model with Adam optimizer, categorical_crossentropy loss, and with metrics accuracy, precision, and recall.\n",
    "\n",
    "    1.6.\tTrain the model for 20 epochs and plot training loss and accuracy against epochs.\n",
    "\n",
    "    1.7.\tEvaluate the model on test images and print the test loss and accuracy.\n",
    "\n",
    "    1.8.\tPlot heatmap of the confusion matrix and print classification report.\n",
    "\n",
    "2.\tBuild a lightweight model with the aim of deploying the solution on a mobile device using transfer learning. You can use any lightweight pre-trained model as the initial (first) layer. MobileNetV2 is a popular lightweight pre-trained model built using Keras API. \n",
    "\n",
    "    2.1.\tSplit the dataset into train and test datasets in the ration 70:30, with shuffle and random state=1.\n",
    "\n",
    "    2.2.\tUse tf.keras.preprocessing.image_dataset_from_directory to load the train and test datasets. This function also supports data normalization.*(Hint: Image_scale=1./255)*\n",
    "\n",
    "    2.3.\tLoad train, validation and test datasets in batches of 32 using the function initialized in the above step.\n",
    "\n",
    "    2.4.\tBuild a CNN network using Keras with the following layers. \n",
    "\n",
    "      - Load MobileNetV2 - Light Model as the first layer *(Hint: Keras API Doc)*\n",
    "\n",
    "      - GLobalAveragePooling2D layer\n",
    "\n",
    "      - Dropout(0.2)\n",
    "\n",
    "      - Dense layer with 256 neurons and activation relu\n",
    "\n",
    "      - BatchNormalization layer\n",
    "\n",
    "      - Dropout(0.1)\n",
    "\n",
    "      - Dense layer with 128 neurons and activation relu\n",
    "\n",
    "      - BatchNormalization layer\n",
    "\n",
    "      - Dropout(0.1)\n",
    "\n",
    "      - Dense layer with 9 neurons and activation softmax\n",
    "\n",
    "    2.5.\tCompile the model with Adam optimizer, categorical_crossentropy loss, and metrics accuracy, Precision, and Recall.\n",
    "\n",
    "    2.6.\tTrain the model for 50 epochs and Early stopping while monitoring validation loss.\n",
    "\n",
    "    2.7.\tEvaluate the model on test images and print the test loss and accuracy.\n",
    "\n",
    "    2.8.\tPlot Train loss Vs Validation loss and Train accuracy Vs Validation accuracy.\n",
    "    \n",
    "3.\tCompare the results of both models built in steps 1 and 2 and state your observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[**Setup: Import Necessary Libraries**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <a id='toc1_4_2_'></a>[**1. Build A CNN Network To Classify A Boat**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_1'></a>[**1.1 Split the dataset into train and test in the ratio 80:20, with shuffle and random state=43**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your dataset\n",
    "data_dir = pathlib.Path(\"boat_type_classification_dataset\")\n",
    "\n",
    "# Set common parameters\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load and split the full dataset\n",
    "full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=43,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=None,  # Load without batching initially\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Split the full dataset into train and test\n",
    "train_ds, test_ds = full_ds\n",
    "\n",
    "print(\"Number of training samples:\", tf.data.experimental.cardinality(train_ds))\n",
    "print(\"Number of test samples:\", tf.data.experimental.cardinality(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- This section is responsible for loading the dataset and splitting it into training and testing sets.\n",
    "- We use `tf.keras.preprocessing.image_dataset_from_directory` to load images directly from the filesystem. This function is convenient as it handles the file reading and label assignment automatically.\n",
    "- We set `validation_split=0.2` and `subset=\"both\"` to get both training and testing sets in an 80:20 ratio.\n",
    "- `seed=43` ensures reproducibility of the random split.\n",
    "- `shuffle=True` randomizes the order of the samples, which is important for training neural networks.\n",
    "- We set `batch_size=None` initially to load the entire dataset without batching, giving us more flexibility in the subsequent processing steps.\n",
    "- The `image_size` parameter resizes all images to a consistent size, which is necessary for batch processing in neural networks.\n",
    "\n",
    "**Why it's important:**\n",
    "\n",
    "- Properly splitting the data ensures we have separate sets for training and evaluation, which is crucial for assessing the model's performance on unseen data.\n",
    "- The 80:20 split is a common ratio that balances having enough training data while still retaining a significant portion for testing.\n",
    "- Shuffling the data helps prevent any bias that might occur from the order of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_2'></a>[**1.2 Use tf.keras.preprocessing.image_dataset_from_directory to load the train and test datasets. This function also supports data normalization.** *(Hint: image_scale=1./255)*](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Apply normalization to the datasets\n",
    "train_ds = train_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Function to safely check normalization\n",
    "def check_normalization(dataset):\n",
    "    for images, _ in dataset.take(1):\n",
    "        min_val = tf.reduce_min(images).numpy()\n",
    "        max_val = tf.reduce_max(images).numpy()\n",
    "        return min_val, max_val\n",
    "    return None, None\n",
    "\n",
    "print(\"Checking train dataset normalization:\")\n",
    "train_min, train_max = check_normalization(train_ds)\n",
    "if train_min is not None and train_max is not None:\n",
    "    print(f\"Image data range: {train_min} to {train_max}\")\n",
    "else:\n",
    "    print(\"Unable to check train dataset normalization\")\n",
    "\n",
    "print(\"\\nChecking test dataset normalization:\")\n",
    "test_min, test_max = check_normalization(test_ds)\n",
    "if test_min is not None and test_max is not None:\n",
    "    print(f\"Image data range: {test_min} to {test_max}\")\n",
    "else:\n",
    "    print(\"Unable to check test dataset normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- This section focuses on normalizing the image data and verifying that the normalization was applied correctly.\n",
    "- We define a `normalize_img` function that converts the image data from integers in the range [0, 255] to floating-point numbers in the range [0, 1].\n",
    "- We apply this normalization to both the training and testing datasets using the map function.\n",
    "- The `check_normalization` function safely checks the range of values in the normalized datasets.\n",
    "\n",
    "**Why it's important:**\n",
    "\n",
    "- Normalization is crucial for neural network training. It helps the model converge faster and can lead to better performance.\n",
    "- Scaling the input to a standard range (like [0, 1]) ensures that all features contribute equally to the model's learning process.\n",
    "- Checking the normalization helps verify that our preprocessing steps are working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_3'></a>[**1.3 Load train, validation and test dataset in batches of 32 using the function initialized in the above step.**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare dataset\n",
    "def prepare_dataset(dataset, is_training=False):\n",
    "    # Shuffle the dataset if it's the training set\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Use data augmentation only on the training set\n",
    "    if is_training:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip('horizontal'),\n",
    "            tf.keras.layers.RandomRotation(0.2),\n",
    "        ])\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Use buffered prefetching on all datasets\n",
    "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = prepare_dataset(train_ds, is_training=True)\n",
    "test_ds = prepare_dataset(test_ds)\n",
    "\n",
    "# Further split test_ds into validation and test\n",
    "val_ds = test_ds.take(tf.data.experimental.cardinality(test_ds) // 2)\n",
    "test_ds = test_ds.skip(tf.data.experimental.cardinality(test_ds) // 2)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Number of training batches:\", tf.data.experimental.cardinality(train_ds))\n",
    "print(\"Number of validation batches:\", tf.data.experimental.cardinality(val_ds))\n",
    "print(\"Number of test batches:\", tf.data.experimental.cardinality(test_ds))\n",
    "\n",
    "# Function to safely get a batch from a dataset\n",
    "def get_batch(dataset):\n",
    "    for batch in dataset.take(1):\n",
    "        return batch\n",
    "    print(\"Dataset is empty\")\n",
    "    return None\n",
    "\n",
    "# Print an example batch to verify the shape\n",
    "example_batch = get_batch(train_ds)\n",
    "if example_batch is not None:\n",
    "    images, labels = example_batch\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "\n",
    "# Get class names\n",
    "class_names = full_ds[0].class_names\n",
    "print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- This section prepares the datasets for training by applying several important transformations.\n",
    "- The prepare_dataset function:\n",
    "    - Shuffles the training data to randomize the order of samples in each epoch.\n",
    "    - Batches the data into groups of 32 images (as specified by batch_size).\n",
    "    - Applies data augmentation (random flips and rotations) to the training set only.\n",
    "    - Uses prefetching to optimize data loading performance.\n",
    "- We split the test set further into validation and test sets.\n",
    "- We print information about the datasets and verify the shape of a sample batch.\n",
    "\n",
    "**Why it's important**:\n",
    "\n",
    "- Batching is necessary for efficient processing in neural networks. It allows the model to update its weights based on multiple samples at once.\n",
    "- Data augmentation helps prevent overfitting by artificially increasing the diversity of the training set.\n",
    "- Shuffling the training data ensures that the model sees the data in a different order each epoch, which can improve generalization.\n",
    "- Prefetching helps optimize performance by preparing the next batch of data while the current batch is being processed.\n",
    "- Splitting the test set into validation and test sets allows us to tune hyperparameters on the validation set while still having a completely unseen test set for final evaluation.\n",
    "- Verifying the shapes of the batches ensures that our data is in the correct format for our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
