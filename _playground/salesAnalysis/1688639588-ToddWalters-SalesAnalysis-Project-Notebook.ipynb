{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by loading the data to check for any issues and perform the data wrangling process.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'AusApparalSales4thQrt2020.csv'\n",
    "sales_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(sales_data.head())\n",
    "\n",
    "# Display the last few rows of the dataframe\n",
    "print(\"\\n\", sales_data.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_data = sales_data.isna().sum()\n",
    "found_data = sales_data.notna().sum()\n",
    "\n",
    "# Check for any suspicious or unusual data that might indicate incorrect data\n",
    "# For the 'Date' column, we'll check if all entries are valid dates\n",
    "# For the 'Time' column, we'll check the unique values to ensure they're expected values\n",
    "# For 'State', 'Group', and 'Unit', 'Sales' we'll check for unique values as well\n",
    "date_check = pd.to_datetime(sales_data['Date'], errors='coerce').isna().sum()\n",
    "time_unique_values = sales_data['Time'].unique()\n",
    "state_unique_values = sales_data['State'].unique()\n",
    "group_unique_values = sales_data['Group'].unique()\n",
    "\n",
    "# Strip leading/trailing spaces from 'Time', 'State', and 'Group' columns\n",
    "sales_data['Time'] = sales_data['Time'].str.strip()\n",
    "sales_data['State'] = sales_data['State'].str.strip()\n",
    "sales_data['Group'] = sales_data['Group'].str.strip()\n",
    "\n",
    "# Now, we will perform the descriptive statistical analysis on the 'Sales' and 'Unit' columns\n",
    "descriptive_stats = sales_data[['Unit', 'Sales']].describe()\n",
    "\n",
    "# Calculate the mode separately as it's not included in the describe() by default\n",
    "mode_units = sales_data['Unit'].mode()[0]\n",
    "mode_sales = sales_data['Sales'].mode()[0]\n",
    "\n",
    "# Add the mode to the descriptive statistics\n",
    "descriptive_stats.loc['mode'] = [mode_units, mode_sales]\n",
    "\n",
    "# Display the descriptive statistics\n",
    "descriptive_stats\n",
    "\n",
    "print(\"Using `isna` to check for missing values in the dataset:\")\n",
    "print(missing_data)\n",
    "print(\"\\nUsing `notna` to check for found values in the dataset:\")\n",
    "print(found_data)\n",
    "print(\"\\nChecking for invalid dates in the 'Date' column:\")\n",
    "print(date_check)\n",
    "print(\"\\nUnique values in the 'Time' column:\")\n",
    "print(time_unique_values)\n",
    "print(\"\\nUnique values in the 'State' column:\")\n",
    "print(state_unique_values)\n",
    "print(\"\\nUnique values in the 'Group' column:\")\n",
    "print(group_unique_values)\n",
    "print(\"\\nDescriptive Statistics for Unit and Sales Data:\")\n",
    "print()\n",
    "print(descriptive_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanliness Review\n",
    "\n",
    "The dataset consists of 7560 entries and six columns without any missing values. Here's a quick overview of the data structure:\n",
    "\n",
    "- **Date**: The date of the transaction (currently in string format).\n",
    "- **Time**: The time of day for the transaction (Morning, Afternoon, etc.).\n",
    "- **State**: The Australian state where the transaction occurred.\n",
    "- **Group**: The customer group (Kids, Men, Women, Seniors).\n",
    "- **Unit**: The number of units sold.\n",
    "- **Sales**: The sales amount in (presumably) Australian dollars.\n",
    "\n",
    "\n",
    "Overall Results:\n",
    "- The data doesn't have any missing values, \n",
    "- All dates in the 'Date' column are valid. \n",
    "- The 'Time', 'State', and 'Group' columns contain expected unique values. \n",
    "- The 'Unit' and 'Sales' columns have a wide range of values, which is normal for sales data.\n",
    "\n",
    "Now, let's consider the treatment for any potential incorrect data that doesn't necessarily show up as missing:\n",
    "\n",
    "- For 'Unit' and 'Sales', any extremely low or high values could be potential outliers. However, unless there's a clear reason to consider them incorrect (such as a negative sales value), they may represent actual sales data.\n",
    "- Any incorrect categorization in 'State' or 'Group' would require domain knowledge or a reference to identify.\n",
    "- Since we don't have any missing data, there's no need for dropping or filling null values.  However if there had been missing data then I would using the following guidelines to determine how I would handle the missing data:\n",
    "  - If the dataset had a small number of missing values, particularly if they are missing at random, I would recommend filling in the missing data using the median (for numerical data) or mode (for categorical data) to fill these gaps.\n",
    "  - If there was a pattern in the missing data related to specific times or states (not random), I would suggest investigating further to understand why the data is missing before deciding whether to impute or remove the data.\n",
    "  - If entire rows or significant portions of the data were missing for certain groups or states, it might be preferable to impute using a model-based approach, which could preserve the relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization/Normalization\n",
    "\n",
    "Next, I will proceed with data normalization. This is the preferred approach because it will adjust the range of data to a standard scale, which is useful for analyzing and comparing sales performance across different states and groups.\n",
    "\n",
    "Let's normalize the 'Unit' and 'Sales' columns using min-max normalization to scale the values between 0 and 1.\n",
    "\n",
    "After normalization, we'll consider the use of the groupby() function to aggregate data for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the 'Unit' and 'Sales' columns\n",
    "sales_data[['Unit', 'Sales']] = scaler.fit_transform(sales_data[['Unit', 'Sales']])\n",
    "\n",
    "# Display the normalized data\n",
    "print(\"Normalized Data Sample:\")\n",
    "print(sales_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization and aggregation have been completed successfully. The 'Unit' and 'Sales' columns are now normalized between 0 and 1, making it easier to compare the data on the same scale. \n",
    "\n",
    "- **Mean (Average)**: Both sales and units have an average normalized value of approximately 0.254.\n",
    "- **Standard Deviation**: Indicates moderate variability in both sales and units with a standard deviation of around 0.205.\n",
    "- **Minimum and Maximum Values**: Both metrics range from 0 (minimum) to 1 (maximum), confirming the effectiveness of the normalization.\n",
    "- **Mode**: The most frequent value for both sales and units is approximately 0.111, suggesting a common sale and unit transaction size.\n",
    "\n",
    "\n",
    "The `groupby()` function can be very useful for chunking data into groups, such as sales by state or by group. It will allow us to perform analyses on these chunks separately, which will be crucial in answering the project's key questions.\n",
    "For the AAL dataset and the objectives outlined in the project statement, for example through the use of the `groupby()` function the following analysis can be performed on the data provided:\n",
    "\n",
    "1. **Revenue Analysis by State**:\n",
    "   - To identify the states generating the highest and lowest revenues, use `groupby('State')` to calculate the total sales for each state. This will provide a clear picture of geographical performance.\n",
    "\n",
    "2. **Product Group Performance**:\n",
    "   - Apply `groupby('Group')` to determine the revenue generation by different groupsâ€”kids, women, men, and seniors. Understanding which groups are driving the most sales can influence product stocking and marketing efforts.\n",
    "\n",
    "3. **Temporal Analysis**:\n",
    "   - If the dataset includes timestamps, `groupby()` can help analyze sales trends over different time frames. For instance, use `groupby([pd.Grouper(key='Date', freq='M')])` for monthly sales trends, which is useful for formulating monthly sales programs.\n",
    "\n",
    "4. **State and Group Analysis**:\n",
    "   - To formulate sales programs for states with lower revenues, combine state and group categories using `groupby(['State', 'Group'])`. This dual-level grouping allows you to tailor strategies for specific customer segments within each state.\n",
    "\n",
    "5. **Preparing Data for Visualization**:\n",
    "   - To build the dashboard for the Head of S&M, use `groupby()` in combination with aggregation functions to prepare the data for visualization. The summarized data can be directly used to create various charts and graphs for the dashboard.\n",
    "\n",
    "By strategically using the `groupby()` function to segment the data along these lines, you are able to perform a comprehensive analysis that aligns with the CEO's directives for understanding and improving sales performance across Australia.\n",
    "\n",
    "The `groupby()` function has also been used to sum the sales by 'State' and by 'Group'.\n",
    "\n",
    "Here's a brief overview of the findings:\n",
    "\n",
    "_By Group:_\n",
    "\n",
    "The group generating the **highest sales** is **'Men'**\n",
    "The group generating the **lowest sales** is **'Seniors'**\n",
    "\n",
    "_By State:_\n",
    "\n",
    "The state generating the **highest sales** is **Victoria (VIC)**.\n",
    "The state generating the **lowest sales** is **Western Australia (WA)**.\n",
    "\n",
    "_Time-based Sales Reports:_\n",
    "\n",
    "**Weekly:** The **highest sales** were made in the **52nd week**, while the **lowest** were in the **40th week**.\n",
    "**Monthly:** The month of **December** had the **highest sales**, and **November** had the **lower sales**.\n",
    "**Quarterly:** There is only one quarter represented here, **the 4th**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform descriptive statistical analysis on the data\n",
    "desc_stats = sales_data[['Unit', 'Sales']].describe()\n",
    "\n",
    "# Calculate the mode separately as it's not included in the describe() by default\n",
    "mode_units = sales_data['Unit'].mode()[0]\n",
    "mode_sales = sales_data['Sales'].mode()[0]\n",
    "\n",
    "# Add the mode to the descriptive statistics\n",
    "descriptive_stats.loc['mode'] = [mode_units, mode_sales]\n",
    "\n",
    "# Determine which group is generating the highest and lowest sales\n",
    "sales_by_group = sales_data.groupby('Group')['Sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Determine which state is generating the highest and lowest sales\n",
    "sales_by_state = sales_data.groupby('State')['Sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Now, convert the 'Date' column to datetime type to generate time-based reports\n",
    "sales_data['Date'] = pd.to_datetime(sales_data['Date'])\n",
    "\n",
    "# Extract day of the week (Monday=0, Sunday=6)\n",
    "sales_data['Weekday'] = sales_data['Date'].dt.dayofweek\n",
    "\n",
    "# Map the day of the week from numbers to names\n",
    "days = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "sales_data['Weekday'] = sales_data['Weekday'].map(days)\n",
    "\n",
    "# Aggregate sales by day of the week\n",
    "weekday_sales = sales_data.groupby('Weekday')['Sales'].sum().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# Determine which day of the week generates the highest and lowest sales\n",
    "#sales_by_weekday = weekday_sales.sort_values(ascending=False)\n",
    "sales_by_weekday = weekday_sales\n",
    "\n",
    "# Extracting day, week, month, and year for aggregation\n",
    "sales_data['Day'] = sales_data['Date'].dt.day\n",
    "sales_data['Week'] = sales_data['Date'].dt.isocalendar().week\n",
    "sales_data['Month'] = sales_data['Date'].dt.month\n",
    "sales_data['Quarter'] = sales_data['Date'].dt.quarter\n",
    "\n",
    "\n",
    "# Determine weekly, monthly, and quarterly sales\n",
    "daily_sales = sales_data.groupby('Day')['Sales'].sum()\n",
    "weekly_sales = sales_data.groupby('Week')['Sales'].sum()\n",
    "monthly_sales = sales_data.groupby('Month')['Sales'].sum()\n",
    "quarterly_sales = sales_data.groupby('Quarter')['Sales'].sum()\n",
    "\n",
    "# desc_stats, sales_by_group, sales_by_state, weekly_sales, monthly_sales, quarterly_sales\n",
    "\n",
    "print(\"Descriptive Statistics for Unit and Sales Data:\")\n",
    "print(desc_stats)\n",
    "print(\"\\nSales by Group:\")\n",
    "print(sales_by_group)\n",
    "print(\"\\nSales by State:\")\n",
    "print(sales_by_state)\n",
    "print(\"\\nWeekday Sales:\")\n",
    "print(sales_by_weekday)\n",
    "print(\"\\nWeekly Sales:\")\n",
    "print(weekly_sales)\n",
    "print(\"\\nMonthly Sales:\")\n",
    "print(monthly_sales)\n",
    "print(\"\\nQuarterly Sales:\")\n",
    "print(quarterly_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Now, letâ€™s proceed with the visualization of this data.\n",
    "\n",
    "Given that Seaborn is recommended for statistical analysis, I will use it for the visualization. It integrates well with Pandas, provides attractive and informative statistical graphs, and is generally easy to use for creating complex visualizations.\n",
    "\n",
    "Bar Plots and Box Plots:\n",
    "\n",
    "For the bar plots and box plots required Seaborn is a more efficient choice. It simplifies creating complex visualizations like displaying the distribution of sales by day with automatic handling of categories and statistical summaries.  Using `sns.barplot` and `sns.boxplot` allowed us to easily map categorical data from pandas and display it with automatically configured axes and a polished style.\n",
    "Custom Plot Requirements:\n",
    "\n",
    "I mainly used Matplotlib to create a multi-panel plot onto which i plotted Seaborn plots.Matplotlib is beneficial for this use due to its flexibility in handling subplot arrangements and fine-tuning. Creating a multi-panel figure with various types of plots (bar, line, box) arranged in a specific layout is more straightforward with Matplotlibâ€™s subplot functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(6, 2, figsize=(40, 80))\n",
    "\n",
    "# State-wise sales analysis for different groups\n",
    "sns.barplot(x='Sales', y='State', hue='Group', data=sales_data, ax=ax[0, 0], errorbar=None)\n",
    "ax[0, 0].set_title('Bar Plot of State-wise Sales Analysis for Different Groups')\n",
    "\n",
    "# Group-wise sales analysis across different states\n",
    "sns.barplot(x='Group', y='Sales', hue='State', data=sales_data, ax=ax[0, 1], errorbar=None)\n",
    "ax[0, 1].set_title('Bar Plot of Group-wise Sales Analysis Across Different States')\n",
    "\n",
    "# Daily Sales\n",
    "sns.barplot(x=daily_sales.index, y=daily_sales.values, ax=ax[1, 0], errorbar=None)\n",
    "ax[1,0].set_title('Bar Plot of Daily Sales')\n",
    "\n",
    "# Use this palette for the bars\n",
    "sns.barplot(x=weekly_sales.index, y=weekly_sales.values, ax=ax[1, 1], errorbar=None)\n",
    "ax[1,1].set_title('Bar Plot of Weekly Sales')\n",
    "\n",
    "# Day-of-the-week analysis for sales\n",
    "sns.barplot(x=weekday_sales.index, y=weekday_sales.values, ax=ax[2,0], errorbar=None)\n",
    "ax[2,0].set_title('Bar Plot of Day-of-the-Week Sales Analysis')\n",
    "\n",
    "# Monthly Sales\n",
    "sns.barplot(x=monthly_sales.index, y=monthly_sales.values, ax=ax[2, 1], errorbar=None)\n",
    "ax[2,1].set_title('Bar Plot of Monthly Sales')\n",
    "\n",
    "# Time-of-the-day analysis for sales\n",
    "sns.lineplot(x='Time', y='Sales', data=sales_data, estimator=sum, ax=ax[3,0], errorbar=None, sort=False)\n",
    "ax[3,0].set_title('Line Plot of Time-of-the-Day Sales Analysis')\n",
    "\n",
    "# State-wise Sales Distribution\n",
    "state_sales_boxplot = sales_data\n",
    "state_sales_boxplot['Hue'] = sales_data['State']\n",
    "\n",
    "sns.boxplot(data=sales_data, x='State', y='Sales', hue='Hue', ax=ax[3,1])\n",
    "ax[3,1].set_title('Box Plot of State-wise Sales Distribution')\n",
    "ax[3,1].set_ylabel('Normalized Sales')\n",
    "ax[3,1].set_xlabel('State')\n",
    "\n",
    "# Distribution of Sales Amounts\n",
    "group_sales_boxplot = sales_data\n",
    "group_sales_boxplot['Hue'] = sales_data['Group']\n",
    "\n",
    "sns.boxplot(data=sales_data, x='Group', y='Sales', hue='Hue', ax=ax[4,0])\n",
    "ax[4,0].set_title('Box Plot of Distribution of Sales Amounts by Group')\n",
    "ax[4,0].set_ylabel('Normalized Sales')\n",
    "ax[4,0].set_xlabel('Group')\n",
    "\n",
    "# # Time-of-Day Sales Distribution\n",
    "# sns.boxplot(data=sales_data, x='Time', y='Sales', ax=ax[4,1])\n",
    "# ax[4,1].set_title('Time-of-Day Sales Distribution')\n",
    "\n",
    "# Create a box plot for the distribution of sales across different days of the week\n",
    "weekday_sales_boxplot = sales_data\n",
    "weekday_sales_boxplot['Hue'] = sales_data['Weekday']\n",
    "\n",
    "sns.boxplot(x='Weekday', y='Sales', data=sales_data, hue='Hue', ax=ax[4,1], order=[\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "ax[4,1].set_title('Box Plot of Day-of-the-Week Sales')\n",
    "ax[4,1].set_ylabel('Normalized Sales')\n",
    "ax[4,1].set_xlabel('Day of the Week')\n",
    "\n",
    "# Descriptive statistics using box plots for the 'Sales' column\n",
    "sns.boxplot(y='Sales', data=sales_data, ax=ax[5, 0])\n",
    "ax[5,0].set_title('Box Plot of Sales')\n",
    "\n",
    "# Descriptive statistics using box plots for the 'Units Sold' column\n",
    "sns.boxplot(y='Unit', data=sales_data, ax=ax[5, 1])\n",
    "ax[5,1].set_title('Box Plot of Units Sold')\n",
    "\n",
    "# Adjust layout to prevent overlap of titles and labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to a file\n",
    "viz_file_path = 'sales_analysis_viz.png'\n",
    "plt.savefig(viz_file_path)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization Inferences\n",
    "\n",
    "### State-wise Sales Analysis for Different Groups\n",
    "\n",
    "This bar chart shows the sales for different groups (Kids, Men, Women, and Seniors) across the states. It is evident that Victoria has the highest sales across all groups, and Western Australia has the lowest.\n",
    "\n",
    "### Group-wise Sales Analysis Across Different States\n",
    "\n",
    "This bar chart compares the total sales made by each group across the states. Men's group appears to lead in sales in most states.\n",
    "\n",
    "### Daily Sales\n",
    "\n",
    "1. The bar chart shows fluctuations in daily sales, with some days experiencing significantly higher sales. This could correlate with weekends, holidays, or special promotional events.\n",
    "2. Monitoring these spikes can help in planning stock and staffing requirements.\n",
    "\n",
    "### Weekly Sales:\n",
    "\n",
    "1. Sales trends on a weekly basis reveal periods of higher sales, particularly towards the end of the sampling period, which likely corresponds to holiday shopping in December.\n",
    "2. The company might consider focusing marketing efforts during these peak weeks to maximize revenue.\n",
    "\n",
    "### Days of the Week Sales:\n",
    "\n",
    "1. Sales trends vary by day of the week, with some days clearly performing better than others. This could be used to tailor daily promotions or marketing campaigns to boost sales on slower days.\n",
    "2. Understanding these patterns can also aid in inventory management and staffing.\n",
    "\n",
    "### Monthly Sales:\n",
    "\n",
    "The bar chart aggregates the weekly sales data and shows similar trends.  December has the highest sales and November the lowest and October sitting between.\n",
    "\n",
    "### Time-of-the-Day Sales Analysis\n",
    "\n",
    "The line plot indicates that sales are highest in the morning and decrease as the day progresses, with the evening having the lowest sales. This information can help the Sales & Marketing team to target their efforts during different times of the day.\n",
    "\n",
    "### State-wise Sales Distribution Box Plot\n",
    "\n",
    "There's notable variability in sales among different states. States like VIC show higher median sales and a wider distribution, indicating both high sales days and significant fluctuation. States with tighter distributions, like WA, suggest more consistent but lower sales.\n",
    "\n",
    "### Distribution of Sales Amounts by Group Box Plot\n",
    "\n",
    "1. The sales across different groups (Kids, Men, Women, Seniors) are relatively similar, with men showing slightly higher median sales. This uniformity suggests that all groups are significant contributors to the revenue, though men's products might have slightly better performance.\n",
    "2. The presence of outliers in all groups suggests occasional spikes in sales, potentially driven by seasonal trends or promotions.\n",
    "\n",
    "### Days of The Week Sales Box Plot:\n",
    "\n",
    "1. *Variability and Spread:*\n",
    "\n",
    "    - Days like Tuesday and Wednesday show a narrower interquartile range (IQR), indicating more consistent sales figures across these days.\n",
    "    - Monday and Sunday display wider IQRs, suggesting greater variability in sales. This might indicate that these days are subject to fluctuations possibly driven by external factors such as weather, holidays, or special events.\n",
    "\n",
    "2. *Outliers:*\n",
    "\n",
    "   - Several days show outliers, particularly Sunday. This could suggest occasional very high sales, possibly due to specific promotions or events that occur sporadically.\n",
    "\n",
    "3. *Median Sales:*\n",
    "\n",
    "   - The median sales across the days are relatively consistent, with no day showing a dramatically different median compared to others. This suggests an overall steady demand across the week.\n",
    "\n",
    "### Sales and Units Sold Box Plots\n",
    "\n",
    "1. Both plots show a similar distribution pattern, with a few outliers indicating unusually high sales or unit transactions on certain days.\n",
    "2. The median (middle line in the box) is closer to the lower quartile, suggesting a skew towards lower sales and units for the majority of transactions.\n",
    "\n",
    "## Strategic Recommendations:\n",
    "\n",
    "- **Resource Allocation:** Adjust staffing and inventory based on expected sales volumes as indicated by day-of-the-week and weekly trends.\n",
    "- **Consistency in Midweek:** The consistency in sales from Tuesday to Thursday could be leveraged for steady revenue generation, planning inventory, and staffing efficiently.\n",
    "- **Potential for Improvement:** The variability in sales on Monday and Sunday suggests these days might benefit from targeted marketing campaigns or special promotions to stabilize and potentially increase sales.\n",
    "- **Strategic Promotions:** The presence of outliers, especially on days like Sunday, indicates that certain strategies may be very effective in driving up sales. Identifying what works on these days could be replicated or adapted for use on other days."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
