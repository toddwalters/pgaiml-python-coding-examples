{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# <a id='toc1_'></a>[**Sales Forecasting Project**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "Fresh Analytics, a data analytics company, aims to comprehend and predict the demand for various items across restaurants. The primary goal of the project is to determine the sales of items across different restaurants over the years. In an ever-changing competitive market, accurate forecasting is crucial for making correct decisions and plans related to sales, production, and other business aspects.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "In ever-changing competitive market conditions, there is a need to make correct decisions and plans for future events related to business like sales, production, and many more. The effectiveness of a decision taken by business managers is influenced by the accuracy of the models used. Demand is the most important aspect of a business's ability to achieve its objectives. Many decisions in business depend on demand, like production, sales, and staff requirements. Forecasting is necessary for business at both international and domestic levels. \n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "1. **restaurants.csv**: Contains information about the restaurants or stores.\n",
    "   - id: Unique identification of the restaurant or store\n",
    "   - name: Name of the restaurant\n",
    "\n",
    "2. **items.csv**: Provides details about the items sold.\n",
    "   - id: Unique identification of the item\n",
    "   - store_id: Unique identification of the store\n",
    "   - name: Name of the item\n",
    "   - kcal: A measure of energy nutrients (calories) in the item\n",
    "   - cost: The unit price of the item\n",
    "\n",
    "3. **sales.csv**: Contains sales data for items at different stores on various dates.\n",
    "   - date: Date of purchase\n",
    "   - item: Name of the item bought\n",
    "   - Price: Unit price of the item\n",
    "   - item_count: Total count of the items bought on that day\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "4.1  Preliminary analysis:\n",
    "\n",
    "         4.1.1. Import the datasets into the Python environment\n",
    "\n",
    "         4.1.2. Examine the dataset's shape and structure, and look out for any outlier\n",
    "\n",
    "         4.1.3. Merge the datasets into a single dataset that includes the date, item id, price, item count, item names, kcal values, store id, and store name\n",
    "\n",
    "4.2  Exploratory data analysis:\n",
    "\n",
    "         4.2.1. Examine the overall date wise sales to understand the pattern\n",
    "      \n",
    "         4.2.2. Find out how sales fluctuate across different days of the week\n",
    "      \n",
    "         4.2.3. Look for any noticeable trends in the sales data for different months of the year\n",
    "      \n",
    "         4.2.4. Examine the sales distribution across different quarters averaged over the years. Identify any noticeable patterns.\n",
    "      \n",
    "         4.2.5. Compare the performances of the different restaurants. Find out which restaurant had the most sales and look at the sales for each restaurant across different years, months, and days.\n",
    "      \n",
    "         4.2.6. Identify the most popular items overall and the stores where they are being sold. Also, find out the most popular item at each store.\n",
    "      \n",
    "         4.2.7. Determine if the store with the highest sales volume is also making the most money per day\n",
    "      \n",
    "         4.2.8. Identify the most expensive item at each restaurant and find out its calorie count\n",
    "\n",
    "4.3 Forecasting using machine learning algorithms\n",
    "\n",
    "         4.3.1. Forecasting using machine learning algorithms\n",
    "\n",
    "            4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on\n",
    "\n",
    "            4.3.1.2. Use the data from the last six months as the testing data\n",
    "\n",
    "            4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances\n",
    "\n",
    "            4.3.1.4. Use the best-performing models to make a forecast for the next year\n",
    "\n",
    "4.4 Forecasting using deep learning algorithms\n",
    "\n",
    "         4.4.1. Use sales amount for predictions instead of item count\n",
    "      \n",
    "         4.4.2. Build a long short-term memory (LSTM) model for predictions\n",
    "      \n",
    "            4.4.2.1. Define the train and test series\n",
    "      \n",
    "            4.4.2.2. Generate synthetic data for the last 12 months\n",
    "      \n",
    "            4.4.2.3. Build and train an LSTM model.\n",
    "      \n",
    "            4.4.2.4. Use the model to make predictions for the test data.\n",
    "      \n",
    "         4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance\n",
    "      \n",
    "         4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[**4.1. Preliminary analysis**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**4.1.1. Import Datasets**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import calendar\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.fft import fft\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pandas.plotting import andrews_curves\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1971)\n",
    "\n",
    "load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "\n",
    "DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "restaurants_ds_file = f'{DATASET_PATH}/resturants.csv'\n",
    "items_ds_file = f'{DATASET_PATH}/items.csv'\n",
    "sales_ds_file = f'{DATASET_PATH}/sales.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "restaurants_df = pd.read_csv(restaurants_ds_file)\n",
    "items_df = pd.read_csv(items_ds_file)\n",
    "sales_df = pd.read_csv(sales_ds_file)\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"Restaurants dataset:\")\n",
    "print(restaurants_df.head())\n",
    "print(\"\\nItems dataset:\")\n",
    "print(items_df.head())\n",
    "print(\"\\nSales dataset:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code block imports necessary libraries (`pandas`, `numpy`, `matplotlib`, and `seaborn`) and reads the three CSV files into pandas DataFrames. It then displays the first few rows of each dataset to give an initial view of the data.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Importing and examining the datasets is crucial as it allows us to understand the structure and content of our data. This step helps identify any immediate issues with data formatting or missing values and provides a foundation for all subsequent analyses.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_2_'></a>[**4.1.2. Examine the dataset's shape and structure, and look out for any outlier**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of each dataset\n",
    "print(\"Restaurants dataset shape:\", restaurants_df.shape)\n",
    "print(\"Items dataset shape:\", items_df.shape)\n",
    "print(\"Sales dataset shape:\", sales_df.shape)\n",
    "\n",
    "# Display info for each dataset\n",
    "print(\"\\nRestaurants dataset info:\")\n",
    "restaurants_df.info()\n",
    "print(\"\\nItems dataset info:\")\n",
    "items_df.info()\n",
    "print(\"\\nSales dataset info:\")\n",
    "sales_df.info()\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"\\nRestaurants Dataset Statistics:\")\n",
    "print(restaurants_df.describe())\n",
    "print(\"\\nItems Dataset Statistics:\")\n",
    "print(items_df.describe())\n",
    "print(\"\\nSales Dataset Statistics:\")\n",
    "print(sales_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in Restaurants dataset:\")\n",
    "print(restaurants_df.isnull().sum())\n",
    "print(\"\\nMissing values in Items dataset:\")\n",
    "print(items_df.isnull().sum())\n",
    "print(\"\\nMissing values in Sales dataset:\")\n",
    "print(sales_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicates in Restaurants dataset:\", restaurants_df.duplicated().sum())\n",
    "print(\"Duplicates in Items dataset:\", items_df.duplicated().sum())\n",
    "print(\"Duplicates in Sales dataset:\", sales_df.duplicated().sum())\n",
    "\n",
    "# Check for outliers using box plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131)\n",
    "sns.boxplot(data=items_df, y='kcal')\n",
    "plt.title('Kcal Distribution')\n",
    "plt.subplot(132)\n",
    "sns.boxplot(data=items_df, y='cost')\n",
    "plt.title('Cost Distribution')\n",
    "plt.subplot(133)\n",
    "sns.boxplot(data=sales_df, y='item_count')\n",
    "plt.title('Item Count Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.info()\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "sales_df.info()\n",
    "print(f\"Items_df information: \")\n",
    "items_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# datasets = [restaurants_df, items_df, sales_df]\n",
    "datasets = [items_df, sales_df]\n",
    "\n",
    "restaurants_df.attrs['name'] = 'Restaurants Dataset'\n",
    "items_df.attrs['name'] = 'Items Dataset'\n",
    "sales_df.attrs['name'] = 'Sales Dataset'\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "def plot_correlation_matrix(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    corr = df.select_dtypes(include=[np.number]).corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title(f'{dataset_name} Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_correlation_matrix(df)\n",
    "\n",
    "# 2. Pairplot\n",
    "def create_pairplot(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    sns.pairplot(df.select_dtypes(include=[np.number]))\n",
    "    plt.suptitle(f'{dataset_name} Pairplot of Numerical Variables', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    create_pairplot(df)\n",
    "\n",
    "# # 3. Z-score Analysis\n",
    "# def identify_outliers_zscore(df, threshold=3):\n",
    "#     outliers = pd.DataFrame()\n",
    "#     for column in df.select_dtypes(include=[np.number]).columns:\n",
    "#         z_scores = np.abs(stats.zscore(df[column]))\n",
    "#         outliers[column] = df[column][(z_scores > threshold)]\n",
    "#     return outliers\n",
    "# \n",
    "# outliers_zscore = identify_outliers_zscore(items)\n",
    "# print(\"Outliers (Z-score method):\")\n",
    "# print(outliers_zscore)\n",
    "# \n",
    "# # 4. IQR Method for Outliers\n",
    "# def identify_outliers_iqr(df):\n",
    "#     outliers = pd.DataFrame()\n",
    "#     for column in df.select_dtypes(include=[np.number]).columns:\n",
    "#         Q1 = df[column].quantile(0.25)\n",
    "#         Q3 = df[column].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         outliers[column] = df[column][(df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR))]\n",
    "#     return outliers\n",
    "# \n",
    "# outliers_iqr = identify_outliers_iqr(items)\n",
    "# print(\"Outliers (IQR method):\")\n",
    "# print(outliers_iqr)\n",
    "\n",
    "# 5. Time Series Decomposition\n",
    "def plot_time_series_decomposition(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    df_temp = df.copy()\n",
    "    df_temp.set_index('date', inplace=True)\n",
    "    result = seasonal_decompose(df_temp['item_count'], model='additive', period=30)  # Adjust period as needed\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Time Series Decomposition of {dataset_name}', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series_decomposition(sales_df)\n",
    "\n",
    "# 6. Distribution Plots\n",
    "def plot_distributions(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    n_cols = 2\n",
    "    n_rows = (len(num_cols) + 1) // 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    for i, col in enumerate(num_cols):\n",
    "        ax = axes[i//n_cols, i%n_cols]\n",
    "        sns.histplot(df[col], kde=True, ax=ax)\n",
    "        ax.set_title(f'Distribution of {col} in {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_distributions(df)\n",
    "\n",
    "# 7. Cumulative Distribution Functions (CDF)\n",
    "# def plot_cdfs(df):\n",
    "#     # Retrieve the dataset name from the DataFrame's attributes\n",
    "#     dataset_name = df.attrs.get('name', 'Dataset')\n",
    "#     num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "#     n_cols = 2\n",
    "#     n_rows = (len(num_cols) + 1) // 2\n",
    "#     fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "#     for i, col in enumerate(num_cols):\n",
    "#         ax = axes[i//n_cols, i%n_cols]\n",
    "#         sorted_data = np.sort(df[col])\n",
    "#         yvals = np.arange(len(sorted_data))/float(len(sorted_data)-1)\n",
    "#         ax.plot(sorted_data, yvals)\n",
    "#         ax.set_title(f'CDF of {col} in {dataset_name}')\n",
    "#         ax.set_xlabel(col)\n",
    "#         ax.set_ylabel('Cumulative Probability')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# \n",
    "# for df in datasets:\n",
    "#     plot_cdfs(df)\n",
    "# \n",
    "# # 8. Box Plots by Category\n",
    "# def plot_boxplots_by_category(df, category_col, value_col):\n",
    "#     # Retrieve the dataset name from the DataFrame's attributes\n",
    "#     dataset_name = df.attrs.get('name', 'Dataset')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     sns.boxplot(x=category_col, y=value_col, data=df)\n",
    "#     plt.title(f'{value_col} by {category_col} in {dataset_name}')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_boxplots_by_category(sales_df, 'item_id', 'price')  # Example\n",
    "# \n",
    "# # 9. Missing Value Patterns\n",
    "# def plot_missing_values(df):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "#     plt.title('Missing Value Patterns')\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_missing_values(sales)\n",
    "# \n",
    "# 10. Value Counts for Categorical Variables\n",
    "# def plot_value_counts(df, column):\n",
    "#     # Retrieve the dataset name from the DataFrame's attributes\n",
    "#     dataset_name = df.attrs.get('name', 'Dataset')\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     df[column].value_counts().plot(kind='bar')\n",
    "#     plt.title(f'Value Counts for {column} in {dataset_name}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_value_counts(sales_df, 'item_id')  # Example\n",
    "\n",
    "# 11. Scatter Plot Matrix\n",
    "def plot_scatter_matrix(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    pd.plotting.scatter_matrix(df.select_dtypes(include=[np.number]), figsize=(15, 15), diagonal='kde')\n",
    "    plt.suptitle(f'Scatter Plot Matrix of {dataset_name}', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_scatter_matrix(df)\n",
    "\n",
    "\n",
    "# 12. 3D Scatter Plot\n",
    "def plot_3d_scatter(df, x_col, y_col, z_col):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df[x_col], df[y_col], df[z_col])\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_zlabel(z_col)\n",
    "    plt.title(f'3D Scatter Plot of {dataset_name}')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_scatter(items_df, 'kcal', 'cost', 'id')  # Example\n",
    "\n",
    "# 13. Parallel Coordinates Plot\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pandas.plotting import parallel_coordinates\n",
    "# import pandas as pd\n",
    "# \n",
    "# def plot_parallel_coordinates(df):\n",
    "#     # Retrieve the dataset name from the DataFrame's attributes\n",
    "#     dataset_name = df.attrs.get('name', 'Dataset')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     parallel_coordinates(df.select_dtypes(include=[np.number]), 'id')\n",
    "#     plt.title(f'Parallel Coordinates Plot of {dataset_name}')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# for df in datasets:\n",
    "#     plot_parallel_coordinates(df)\n",
    "# plot_parallel_coordinates(items_df)\n",
    "\n",
    "# 14. Andrews Curves\n",
    "# from pandas.plotting import andrews_curves\n",
    "# def plot_andrews_curves(df):\n",
    "#     # Retrieve the dataset name from the DataFrame's attributes\n",
    "#     dataset_name = df.attrs.get('name', 'Dataset')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     andrews_curves(df.select_dtypes(include=[np.number]), 'id')\n",
    "#     plt.title(f'Andrews Curves of {dataset_name}')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     \n",
    "# plot_andrews_curves(items_df)\n",
    "\n",
    "# 15. Cluster Analysis\n",
    "from sklearn.cluster import KMeans\n",
    "def perform_cluster_analysis(df, n_clusters=3):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    numeric_data = df.select_dtypes(include=[np.number])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(numeric_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(numeric_data.iloc[:, 0], numeric_data.iloc[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.title(f'K-means Clustering of Numeric Variables in {dataset_name}')\n",
    "    plt.xlabel(numeric_data.columns[0])\n",
    "    plt.ylabel(numeric_data.columns[1])\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    perform_cluster_analysis(df)\n",
    "# perform_cluster_analysis(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code examines the shape, structure, and quality of each dataset. It checks the number of rows and columns, data types of each column, presence of missing values, and existence of duplicate entries.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Understanding the dataset's structure and quality is crucial for data preprocessing and analysis. It helps identify potential issues like missing data or duplicates that need to be addressed before proceeding with the analysis.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_3_'></a>[**4.1.2. Merge the datasets into a single dataset that includes the date, item id, price, item count, item names, kcal values, store id, and store name**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with items\n",
    "merged_df = pd.merge(sales_df, items_df, left_on='item_id', right_on='id', how='left')\n",
    "\n",
    "# Merge with restaurants\n",
    "merged_df = pd.merge(merged_df, restaurants_df, left_on='store_id', right_on='id', how='left')\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "merged_df = merged_df.rename(columns={'name_x': 'item_name', 'name_y': 'restaurant_name'})\n",
    "\n",
    "# Select relevant columns\n",
    "final_df = merged_df[['date', 'item_id', 'price', 'item_count', 'item_name', 'kcal', 'store_id', 'restaurant_name']]\n",
    "\n",
    "# Convert date to datetime\n",
    "final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "print(final_df.head())\n",
    "\n",
    "# Display info of the merged dataset\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code merges the three datasets based on common identifiers (item_id and store_id). It then selects relevant columns, renames them for clarity, and converts the date column to datetime format.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Merging the datasets is crucial for conducting comprehensive analyses that involve information from all three sources. It allows us to examine relationships between sales, item characteristics, and restaurant information in a single DataFrame.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[**4.2. Exploratory data analysis**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_1_'></a>[**4.2.1. Examine the overall date wise sales to understand the pattern**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and calculate total sales\n",
    "daily_sales = final_df.groupby('date').agg({'price': 'sum', 'item_count': 'sum'})\n",
    "daily_sales['total_sales'] = daily_sales['price'] * daily_sales['item_count']\n",
    "\n",
    "# Plot daily sales\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales.index, daily_sales['total_sales'])\n",
    "plt.title('Daily Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot 7-day moving average\n",
    "daily_sales['7_day_ma'] = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales.index, daily_sales['total_sales'], alpha=0.5, label='Daily Sales')\n",
    "plt.plot(daily_sales.index, daily_sales['7_day_ma'], color='red', label='7-day Moving Average')\n",
    "plt.title('Daily Sales and 7-day Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First, let's check the structure of your DataFrame\n",
    "print(daily_sales.index)\n",
    "print(daily_sales.columns)\n",
    "\n",
    "# If the index is not already a DatetimeIndex, we'll try to convert it\n",
    "if not isinstance(daily_sales.index, pd.DatetimeIndex):\n",
    "    # Check if there's a column that could be a date\n",
    "    date_columns = daily_sales.select_dtypes(include=[np.datetime64]).columns\n",
    "    if len(date_columns) > 0:\n",
    "        date_column = date_columns[0]\n",
    "        daily_sales.set_index(date_column, inplace=True)\n",
    "    else:\n",
    "        # If no date column is found, we'll create a date range\n",
    "        daily_sales.index = pd.date_range(start='2019-01-01', periods=len(daily_sales), freq='D')\n",
    "\n",
    "# Ensure the index is sorted\n",
    "daily_sales.sort_index(inplace=True)\n",
    "\n",
    "# 1. Decomposition Plot\n",
    "def plot_decomposition():\n",
    "    result = seasonal_decompose(daily_sales['total_sales'], model='additive', period=365)\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16))\n",
    "    result.observed.plot(ax=ax1)\n",
    "    ax1.set_title('Observed')\n",
    "    result.trend.plot(ax=ax2)\n",
    "    ax2.set_title('Trend')\n",
    "    result.seasonal.plot(ax=ax3)\n",
    "    ax3.set_title('Seasonal')\n",
    "    result.resid.plot(ax=ax4)\n",
    "    ax4.set_title('Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Heatmap of Daily Sales\n",
    "def plot_daily_heatmap():\n",
    "    daily_data = daily_sales['total_sales'].resample('D').sum()\n",
    "    heatmap_data = pd.DataFrame({\n",
    "        'year': daily_data.index.year,\n",
    "        'day_of_year': daily_data.index.dayofyear,\n",
    "        'sales': daily_data.values\n",
    "    })\n",
    "    heatmap_pivot = heatmap_data.pivot(index='year', columns='day_of_year', values='sales')\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(heatmap_pivot, cmap='YlOrRd')\n",
    "    plt.title('Daily Sales Heatmap')\n",
    "    plt.xlabel('Day of Year')\n",
    "    plt.ylabel('Year')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Year-over-Year Comparison\n",
    "def plot_year_over_year():\n",
    "    daily_sales['year'] = daily_sales.index.year\n",
    "    daily_sales['day_of_year'] = daily_sales.index.dayofyear\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for year in daily_sales['year'].unique():\n",
    "        year_data = daily_sales[daily_sales['year'] == year]\n",
    "        plt.plot(year_data['day_of_year'], year_data['total_sales'], label=str(year))\n",
    "    plt.title('Year-over-Year Sales Comparison')\n",
    "    plt.xlabel('Day of Year')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 4. Box plots by Month and Day of Week\n",
    "def plot_boxplots():\n",
    "    daily_sales['month'] = daily_sales.index.month\n",
    "    daily_sales['day_of_week'] = daily_sales.index.day_name()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    sns.boxplot(x='month', y='total_sales', data=daily_sales, ax=ax1)\n",
    "    ax1.set_title('Sales Distribution by Month')\n",
    "    sns.boxplot(x='day_of_week', y='total_sales', data=daily_sales, \n",
    "                order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ax=ax2)\n",
    "    ax2.set_title('Sales Distribution by Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Autocorrelation and Partial Autocorrelation Plots\n",
    "def plot_acf_pacf():\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    plot_acf(daily_sales['total_sales'], ax=ax1, lags=50)\n",
    "    plot_pacf(daily_sales['total_sales'], ax=ax2, lags=50)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Cumulative Sales Plot\n",
    "def plot_cumulative_sales():\n",
    "    cumulative_sales = daily_sales['total_sales'].cumsum()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_sales.index, cumulative_sales)\n",
    "    plt.title('Cumulative Sales Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Sales')\n",
    "    plt.show()\n",
    "\n",
    "# 7. Seasonal Subseries Plot\n",
    "def plot_seasonal_subseries():\n",
    "    daily_sales['quarter'] = daily_sales.index.quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    for i, quarter in enumerate([1, 2, 3, 4]):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        quarter_data = daily_sales[daily_sales['quarter'] == quarter]\n",
    "        sns.lineplot(x=quarter_data.index.dayofyear, y='total_sales', hue=quarter_data.index.year, data=quarter_data, ax=ax)\n",
    "        ax.set_title(f'Q{quarter} Sales')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 8. Rolling Statistics\n",
    "def plot_rolling_stats():\n",
    "    rolling_mean = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "    rolling_std = daily_sales['total_sales'].rolling(window=7).std()\n",
    "    rolling_median = daily_sales['total_sales'].rolling(window=7).median()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_sales.index, daily_sales['total_sales'], label='Daily Sales')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label='7-day Moving Average')\n",
    "    plt.plot(rolling_std.index, rolling_std, label='7-day Moving Std')\n",
    "    plt.plot(rolling_median.index, rolling_median, label='7-day Moving Median')\n",
    "    plt.title('Rolling Statistics of Daily Sales')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 9. Fourier Transform\n",
    "def plot_fourier_transform():\n",
    "    sales_fft = fft(daily_sales['total_sales'].values)\n",
    "    frequencies = np.fft.fftfreq(len(sales_fft))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(frequencies, np.abs(sales_fft))\n",
    "    plt.title('Fourier Transform of Sales Data')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xlim(0, 0.5)  # Only show positive frequencies\n",
    "    plt.show()\n",
    "\n",
    "# 10. Anomaly Detection\n",
    "def detect_anomalies():\n",
    "    rolling_mean = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "    rolling_std = daily_sales['total_sales'].rolling(window=7).std()\n",
    "    anomalies = daily_sales[(daily_sales['total_sales'] > rolling_mean + 2*rolling_std) | \n",
    "                            (daily_sales['total_sales'] < rolling_mean - 2*rolling_std)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_sales.index, daily_sales['total_sales'], label='Daily Sales')\n",
    "    plt.scatter(anomalies.index, anomalies['total_sales'], color='red', label='Anomalies')\n",
    "    plt.title('Daily Sales with Anomalies')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run all the functions\n",
    "plot_decomposition()\n",
    "plot_daily_heatmap()\n",
    "plot_year_over_year()\n",
    "plot_boxplots()\n",
    "plot_acf_pacf()\n",
    "plot_cumulative_sales()\n",
    "plot_seasonal_subseries()\n",
    "plot_rolling_stats()\n",
    "plot_fourier_transform()\n",
    "detect_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code calculates daily total sales by multiplying price and item count. It then plots the daily sales over time and a 7-day moving average to smooth out daily fluctuations and reveal underlying trends.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Examining overall sales patterns helps identify long-term trends, seasonality, and potential anomalies in the data. This information is crucial for understanding the business's performance over time and can inform forecasting models.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_2_'></a>[**4.2.2. Find out how sales fluctuate across different days of the week**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The daily_sales dataset info:\")\n",
    "print(daily_sales.info())\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week column\n",
    "daily_sales['day_of_week'] = daily_sales.index.dayofweek\n",
    "daily_sales['day_of_week_name'] = daily_sales.index.day_name()\n",
    "\n",
    "# Calculate average sales by day of week\n",
    "avg_sales_by_day = daily_sales.groupby('day_of_week_name')['total_sales'].mean().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "total_sales_by_day = daily_sales.groupby('day_of_week_name')['item_count'].sum().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# Plot average sales by day of week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=avg_sales_by_day.index, y=avg_sales_by_day.values, palette='viridis')\n",
    "plt.title('Average Sales by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(avg_sales_by_day.values):\n",
    "    plt.text(index, value, f'${value:,.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=total_sales_by_day.index, y=total_sales_by_day.values, palette='viridis')\n",
    "plt.title('Total Item Sales by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Total Items Sold')\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(total_sales_by_day.values):\n",
    "    plt.text(index, value, f'{value:,.0f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Define the order of the days\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Boxplot of sales by day of week\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='day_of_week_name', y='total_sales', data=daily_sales, order=days_order, palette='viridis')\n",
    "plt.title('Sales Distribution by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Annotate the median values\n",
    "medians = daily_sales.groupby('day_of_week_name')['total_sales'].median().reindex(days_order)\n",
    "for index, median in enumerate(medians):\n",
    "    plt.text(index, median, f'${median:,.2f}', ha='center', va='bottom', color='black', weight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code analyzes sales fluctuations across different days of the week. It calculates average sales for each day and creates a bar plot to visualize these averages. Additionally, it generates a box plot to show the distribution of sales for each day of the week.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Understanding weekly sales patterns is crucial for inventory management, staffing decisions, and marketing strategies. It can help businesses optimize their operations based on expected demand for different days of the week.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_3_'></a>[**4.2.3. Look for any noticeable trends in the sales data for different months of the year**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month column\n",
    "daily_sales['month'] = daily_sales.index.month\n",
    "daily_sales['month_name'] = daily_sales.index.month_name()\n",
    "\n",
    "# 2. Identify trends in sales data for different months\n",
    "# merged_df['month'] = merged_df['date'].dt.month_name()\n",
    "# monthly_sales = merged_df.groupby('month')['item_count'].sum().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "\n",
    "# Calculate average sales by month\n",
    "avg_sales_by_month = daily_sales.groupby('month_name')['total_sales'].mean().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "total_sales_by_month = daily_sales.groupby('month_name')['item_count'].sum().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "# Plot average sales by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=avg_sales_by_month.index, y=avg_sales_by_month.values, palette='viridis')\n",
    "plt.title('Average Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(avg_sales_by_month.values):\n",
    "    plt.text(index, value, f'${value:,.2f}', ha='center', va='bottom', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot total sales by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=total_sales_by_month.index, y=total_sales_by_month.values, palette='viridis')\n",
    "plt.title('Total Item Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(total_sales_by_month.values):\n",
    "    plt.text(index, value, f'{value:,.0f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of sales by month and day of week\n",
    "# Pivot the data to create a heatmap-friendly format\n",
    "# Heatmap of sales by month and day of week\n",
    "monthly_dow_sales = daily_sales.groupby(['month_name', 'day_of_week_name'])['total_sales'].mean().unstack()\n",
    "\n",
    "# Define the correct order for the days of the week and months of the year\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Reindex the DataFrame to ensure the correct order\n",
    "monthly_dow_sales = monthly_dow_sales.reindex(index=months_order, columns=days_order)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(monthly_dow_sales, cmap='YlOrRd', annot=True, fmt='.0f')\n",
    "plt.title('Average Sales by Month and Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code analyzes sales trends across different months of the year. It calculates and plots average sales for each month. Additionally, it creates a heatmap to visualize the relationship between months, days of the week, and sales.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Analyzing monthly sales trends helps identify seasonal patterns in the business. This information is crucial for long-term planning, inventory management, and marketing strategies. It can also inform the development of more accurate forecasting models by accounting for seasonality.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_4_'></a>[**4.2.4. Examine the sales distribution across different quarters averaged over the years. Identify any noticeable patterns**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Add quarter column\n",
    "daily_sales['quarter'] = daily_sales.index.quarter\n",
    "\n",
    "# Calculate average sales by quarter\n",
    "avg_sales_by_quarter = daily_sales.groupby('quarter')['total_sales'].mean()\n",
    "\n",
    "# Plot average sales by quarter\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=avg_sales_by_quarter.index, y=avg_sales_by_quarter.values, palette='viridis')\n",
    "plt.title('Average Sales by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for i, v in enumerate(avg_sales_by_quarter.values):\n",
    "    ax.text(i, v, f'${v:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of sales by quarter\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "plt.title('Sales Distribution by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the boxplot\n",
    "medians = daily_sales.groupby('quarter')['total_sales'].median()\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='bottom', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Violin plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "plt.title('Sales Distribution by Quarter (Violin Plot)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the violin plot\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Box plot with swarm\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "sns.swarmplot(x='quarter', y='total_sales', data=daily_sales, color=\".25\", size=3)\n",
    "plt.title('Sales Distribution by Quarter (Box Plot with Swarm)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the boxplot\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='bottom', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Year-over-Year Quarterly Sales Growth heatmap\n",
    "quarterly_yoy = daily_sales.groupby([daily_sales.index.year, 'quarter'])['total_sales'].sum().unstack()\n",
    "quarterly_yoy_pct = quarterly_yoy.pct_change() * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.heatmap(quarterly_yoy_pct, cmap='RdYlGn', annot=True, fmt='.1f')\n",
    "plt.title('Year-over-Year Quarterly Sales Growth (%)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Year')\n",
    "\n",
    "# Rotate the tick labels\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code examines sales distribution across different quarters of the year. It calculates and plots average sales for each quarter, creates a box plot to show the distribution of sales by quarter, and generates a line plot to visualize quarterly sales trends over the years.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Analyzing quarterly sales patterns helps identify broader seasonal trends and potential fiscal year effects on the business. This information is valuable for strategic planning, budgeting, and setting sales targets. It can also reveal how the business performance has evolved over the years on a quarterly basis.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_5_'></a>[**4.2.5. Compare the performances of the different restaurants. Find out which restaurant had the most sales and look at the sales for each restaurant across different years, months, and days**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "# Calculate daily sales for each restaurant\n",
    "restaurant_daily_sales = final_df.groupby(['date', 'restaurant_name']).agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index().rename(columns={'price': 'daily_sales'})\n",
    "\n",
    "# Calculate total sales by restaurant\n",
    "restaurant_sales = final_df.groupby('restaurant_name').agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).rename(columns={'price': 'total_sales'}).sort_values('total_sales', ascending=False)\n",
    "\n",
    "# 2. Separate plots\n",
    "bobs_data = restaurant_sales[restaurant_sales.index == \"Bob's Diner\"]\n",
    "other_data = restaurant_sales[restaurant_sales.index != \"Bob's Diner\"]\n",
    "\n",
    "# 4. Secondary y-axis for Bob's Diner\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot for other restaurants\n",
    "sns.barplot(x=other_data.index, y='total_sales', data=other_data, ax=ax1, palette='husl')\n",
    "ax1.set_xlabel('Restaurant')\n",
    "ax1.set_ylabel('Total Sales (Other Restaurants)')\n",
    "ax1.tick_params(axis='x')\n",
    "\n",
    "# Add values on top of bars for other restaurants\n",
    "for p in ax1.patches:\n",
    "    ax1.annotate('${:,.2f}'.format(p.get_height()), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', va='center', \n",
    "                 xytext=(0, 9), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "# Secondary y-axis for Bob's Diner\n",
    "ax2 = ax1.twinx()\n",
    "sns.barplot(x=bobs_data.index, y='total_sales', data=bobs_data, ax=ax2, color='red', alpha=0.5)\n",
    "ax2.set_ylabel(\"Total Sales (Bob's Diner)\")\n",
    "\n",
    "# Add values on top of bars for Bob's Diner\n",
    "for p in ax2.patches:\n",
    "    ax2.annotate('${:,.2f}'.format(p.get_height()), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', va='center', \n",
    "                 xytext=(0, 9), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Total Sales by Restaurant (Dual Axis)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Assuming final_df is already created and contains the necessary data\n",
    "# Calculate total sales if not already done\n",
    "final_df['total_sales'] = final_df['price'] * final_df['item_count']\n",
    "\n",
    "# Function to create individual bar plots\n",
    "def plot_individual_bars(data, x, y, title, x_label, y_label):\n",
    "    # Define a color palette\n",
    "    palette = sns.color_palette(\"husl\", len(data['restaurant_name'].unique()))\n",
    "    \n",
    "    restaurants = data['restaurant_name'].unique()\n",
    "    \n",
    "    for restaurant in restaurants:\n",
    "        restaurant_data = data[data['restaurant_name'] == restaurant]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(data=restaurant_data, x=x, y=y, palette=palette)\n",
    "        plt.title(f\"{title} - {restaurant}\")\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        if restaurant != \"Bob's Diner\" or title != \"Average Daily Sales\":\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(f'${p.get_height():,.2f}', \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                            ha='center', va='center', \n",
    "                            xytext=(0, 9), \n",
    "                            textcoords='offset points')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 1. Yearly sales\n",
    "yearly_sales = final_df.groupby([final_df['date'].dt.year, 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "yearly_sales = yearly_sales.rename(columns={'date': 'year'})\n",
    "plot_individual_bars(yearly_sales, 'year', 'total_sales', 'Yearly Sales', 'Year', 'Total Sales')\n",
    "\n",
    "# 2. Monthly sales (averaged across years)\n",
    "monthly_sales = final_df.groupby([final_df['date'].dt.month, 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "monthly_sales = monthly_sales.rename(columns={'date': 'month'})\n",
    "monthly_sales['month_name'] = monthly_sales['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "plot_individual_bars(monthly_sales, 'month_name', 'total_sales', 'Average Monthly Sales', 'Month', 'Average Sales')\n",
    "\n",
    "# 3. Daily sales (averaged across years and months)\n",
    "daily_sales = final_df.groupby([final_df['date'].dt.day, 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "daily_sales = daily_sales.rename(columns={'date': 'day'})\n",
    "plot_individual_bars(daily_sales, 'day', 'total_sales', 'Average Daily Sales', 'Day of Month', 'Average Sales')\n",
    "\n",
    "# 4. Day-name sales (averaged across all dates)\n",
    "final_df['day_name'] = final_df['date'].dt.day_name()\n",
    "day_name_sales = final_df.groupby(['day_name', 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "day_name_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_name_sales['day_name'] = pd.Categorical(day_name_sales['day_name'], categories=day_name_order, ordered=True)\n",
    "day_name_sales = day_name_sales.sort_values('day_name')\n",
    "plot_individual_bars(day_name_sales, 'day_name', 'total_sales', 'Average Sales by Day of Week', 'Day of Week', 'Average Sales')\n",
    "\n",
    "# Assuming final_df is already created and contains the necessary data\n",
    "# Calculate total sales if not already done\n",
    "final_df['total_sales'] = final_df['price'] * final_df['item_count']\n",
    "\n",
    "# Function to create individual boxplots for each restaurant\n",
    "def plot_individual_boxplots(data, x, y, title_prefix, x_label, y_label, is_monthly=False):\n",
    "    restaurants = data['restaurant_name'].unique()\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(restaurants)))\n",
    "    \n",
    "    for restaurant, color in zip(restaurants, colors):\n",
    "        restaurant_data = data[data['restaurant_name'] == restaurant]\n",
    "        \n",
    "        if is_monthly:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            ax = sns.boxplot(data=restaurant_data, x='month', y=y, order=range(1, 13), palette='viridis')\n",
    "            plt.title(f\"{title_prefix} - {restaurant}\")\n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            \n",
    "            # Customize x-axis labels\n",
    "            month_names = [calendar.month_abbr[i] for i in range(1, 13)]\n",
    "            plt.xticks(range(12), month_names)\n",
    "            \n",
    "            # Add median values on the boxplot\n",
    "            medians = restaurant_data.groupby('month')[y].median()\n",
    "            for i, median in enumerate(medians):\n",
    "                ax.text(i, median, f'${median:,.2f}', \n",
    "                        horizontalalignment='center', size='small', color='white', \n",
    "                        weight='semibold', bbox=dict(facecolor='black', edgecolor='none', alpha=0.5))\n",
    "        else:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            ax = sns.boxplot(data=restaurant_data, x=x, y=y, color=color)\n",
    "            plt.title(f\"{title_prefix} - {restaurant}\")\n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            \n",
    "            # Add median values on the boxplot\n",
    "            medians = restaurant_data.groupby(x)[y].median().values\n",
    "            pos = range(len(medians))\n",
    "            for tick, label in zip(pos, ax.get_xticklabels()):\n",
    "                ax.text(pos[tick], medians[tick], f'${medians[tick]:,.2f}', \n",
    "                        horizontalalignment='center', size='small', color='black', \n",
    "                        weight='semibold', rotation=0, alpha=0.7, \n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 1. Daily sales\n",
    "daily_sales = final_df.groupby(['date', 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "plot_individual_boxplots(daily_sales, 'restaurant_name', 'total_sales', 'Daily Sales Distribution', 'Restaurant', 'Daily Total Sales')\n",
    "\n",
    "# 2. Monthly sales\n",
    "final_df['month'] = final_df['date'].dt.month\n",
    "monthly_sales = final_df.groupby(['month', 'restaurant_name', final_df['date'].dt.to_period('M')])['total_sales'].sum().reset_index()\n",
    "plot_individual_boxplots(monthly_sales, 'month', 'total_sales', 'Monthly Sales Distribution', 'Month', 'Monthly Total Sales', is_monthly=True)\n",
    "\n",
    "# 3. Quarterly sales\n",
    "final_df['year_quarter'] = final_df['date'].dt.to_period('Q')\n",
    "quarterly_sales = final_df.groupby(['year_quarter', 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "quarterly_sales['year_quarter'] = quarterly_sales['year_quarter'].astype(str)\n",
    "plot_individual_boxplots(quarterly_sales, 'year_quarter', 'total_sales', 'Quarterly Sales Distribution', 'Year-Quarter', 'Quarterly Total Sales')\n",
    "\n",
    "# 4. Yearly sales\n",
    "yearly_sales = final_df.groupby([final_df['date'].dt.year, 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "yearly_sales = yearly_sales.rename(columns={'date': 'year'})\n",
    "plot_individual_boxplots(yearly_sales, 'year', 'total_sales', 'Yearly Sales Distribution', 'Year', 'Yearly Total Sales')\n",
    "\n",
    "# 5. Day of week sales\n",
    "final_df['day_of_week'] = final_df['date'].dt.day_name()\n",
    "day_of_week_sales = final_df.groupby(['day_of_week', 'restaurant_name', 'date'])['total_sales'].sum().reset_index()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_of_week_sales['day_of_week'] = pd.Categorical(day_of_week_sales['day_of_week'], categories=day_order, ordered=True)\n",
    "day_of_week_sales = day_of_week_sales.sort_values('day_of_week')\n",
    "plot_individual_boxplots(day_of_week_sales, 'day_of_week', 'total_sales', 'Day of Week Sales Distribution', 'Day of Week', 'Total Sales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_6_'></a>[**4.2.6. Identify the most popular items overall and the stores where they are being sold. Also, find out the most popular item at each store**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total sales and quantity sold for each item\n",
    "item_sales = final_df.groupby(['item_id', 'item_name']).agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum(),\n",
    "    'item_count': 'sum'\n",
    "}).reset_index().rename(columns={'price': 'total_sales', 'item_count': 'quantity_sold'})\n",
    "\n",
    "# Sort items by quantity sold and get top 10\n",
    "top_10_items = item_sales.sort_values('quantity_sold', ascending=False).head(10)\n",
    "\n",
    "# Plot top 10 items by quantity sold\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='item_name', y='quantity_sold', data=top_10_items)\n",
    "plt.title('Top 10 Items by Quantity Sold')\n",
    "plt.xlabel('Item')\n",
    "plt.ylabel('Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify most popular item at each store\n",
    "store_popular_items = final_df.groupby(['store_id', 'restaurant_name', 'item_id', 'item_name'])['item_count'].sum().reset_index()\n",
    "store_popular_items = store_popular_items.loc[store_popular_items.groupby('store_id')['item_count'].idxmax()]\n",
    "\n",
    "print(\"Most popular item at each store:\")\n",
    "print(store_popular_items[['restaurant_name', 'item_name', 'item_count']])\n",
    "\n",
    "# Plot distribution of sales for top 5 items\n",
    "top_5_items = item_sales.sort_values('total_sales', ascending=False).head(5)\n",
    "top_5_item_sales = final_df[final_df['item_id'].isin(top_5_items['item_id'])]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='item_name', y='price', data=top_5_item_sales)\n",
    "plt.title('Sales Distribution for Top 5 Items')\n",
    "plt.xlabel('Item')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_7_'></a>[**4.2.7. Determine if the store with the highest sales volume is also making the most money per day**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily sales volume and revenue for each store\n",
    "daily_store_performance = final_df.groupby(['date', 'store_id', 'restaurant_name']).agg({\n",
    "    'item_count': 'sum',\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index().rename(columns={'item_count': 'daily_volume', 'price': 'daily_revenue'})\n",
    "\n",
    "# Calculate average daily volume and revenue\n",
    "avg_store_performance = daily_store_performance.groupby('restaurant_name').agg({\n",
    "    'daily_volume': 'mean',\n",
    "    'daily_revenue': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot relationship between average daily volume and revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='daily_volume', y='daily_revenue', data=avg_store_performance, s=100)\n",
    "\n",
    "for i, row in avg_store_performance.iterrows():\n",
    "    plt.annotate(row['restaurant_name'], (row['daily_volume'], row['daily_revenue']))\n",
    "\n",
    "plt.title('Average Daily Sales Volume vs Revenue by Restaurant')\n",
    "plt.xlabel('Average Daily Sales Volume')\n",
    "plt.ylabel('Average Daily Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between daily volume and revenue\n",
    "correlation = daily_store_performance['daily_volume'].corr(daily_store_performance['daily_revenue'])\n",
    "print(f\"Correlation between daily sales volume and revenue: {correlation:.2f}\")\n",
    "\n",
    "# Identify store with highest sales volume and its revenue ranking\n",
    "highest_volume_store = avg_store_performance.loc[avg_store_performance['daily_volume'].idxmax()]\n",
    "revenue_rank = avg_store_performance['daily_revenue'].rank(ascending=False)\n",
    "highest_volume_store_rank = revenue_rank[highest_volume_store.name]\n",
    "\n",
    "print(f\"Store with highest sales volume: {highest_volume_store['restaurant_name']}\")\n",
    "print(f\"Its revenue rank: {highest_volume_store_rank} out of {len(avg_store_performance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_2_8_'></a>[**4.2.8. Identify the most expensive item at each restaurant and find out its calorie count**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most expensive item at each restaurant\n",
    "most_expensive_items = final_df.groupby(['store_id', 'restaurant_name', 'item_id', 'item_name', 'price', 'kcal']).size().reset_index(name='count')\n",
    "most_expensive_items = most_expensive_items.loc[most_expensive_items.groupby('store_id')['price'].idxmax()]\n",
    "\n",
    "# Sort by price in descending order\n",
    "most_expensive_items = most_expensive_items.sort_values('price', ascending=False)\n",
    "\n",
    "# Display the most expensive items and their calorie counts\n",
    "print(\"Most expensive item at each restaurant and its calorie count:\")\n",
    "print(most_expensive_items[['restaurant_name', 'item_name', 'price', 'kcal']])\n",
    "\n",
    "# Visualize price vs calorie count for the most expensive items\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='kcal', y='price', data=most_expensive_items, s=100)\n",
    "\n",
    "for i, row in most_expensive_items.iterrows():\n",
    "    plt.annotate(row['restaurant_name'], (row['kcal'], row['price']))\n",
    "\n",
    "plt.title('Price vs Calorie Count for Most Expensive Items')\n",
    "plt.xlabel('Calories')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between price and calorie count for all items\n",
    "price_calorie_correlation = final_df[['price', 'kcal']].drop_duplicates().corr().iloc[0, 1]\n",
    "print(f\"Correlation between price and calorie count for all items: {price_calorie_correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[**4.3. Forecasting using machine learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_3_1_'></a>[**4.3.1. Forecasting using machine learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_3_1_1_'></a>[**4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_3_1_2_'></a>[**4.3.1.2. Use the data from the last six months as the testing data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_3_1_3_'></a>[**4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_3_1_4_'></a>[**4.3.1.4. Use the best-performing models to make a forecast for the next year**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_4_'></a>[**4.4. Forecasting using deep learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_4_1_'></a>[**4.4.1. Use sales amount for predictions instead of item count**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_4_2_'></a>[**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_4_2_1_'></a>[**4.4.2.1. Define the train and test series**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_4_2_2_'></a>[**4.4.2.2. Generate synthetic data for the last 12 months**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_4_2_3_'></a>[**4.4.2.3. Build and train an LSTM model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_4_4_2_4_'></a>[**4.4.2.4. Use the model to make predictions for the test data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_4_3_'></a>[**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_4_4_'></a>[**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
