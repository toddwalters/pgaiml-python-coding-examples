{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# **Sales Forecasting Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## **Project Context**\n",
    "-----------------------------\n",
    "\n",
    "Fresh Analytics, a data analytics company, aims to comprehend and predict the demand for various items across restaurants. The primary goal of the project is to determine the sales of items across different restaurants over the years. In an ever-changing competitive market, accurate forecasting is crucial for making correct decisions and plans related to sales, production, and other business aspects.\n",
    "\n",
    "-----------------------------\n",
    "## **Project Objectives**\n",
    "-----------------------------\n",
    "\n",
    "In ever-changing competitive market conditions, there is a need to make correct decisions and plans for future events related to business like sales, production, and many more. The effectiveness of a decision taken by business managers is influenced by the accuracy of the models used. Demand is the most important aspect of a business's ability to achieve its objectives. Many decisions in business depend on demand, like production, sales, and staff requirements. Forecasting is necessary for business at both international and domestic levels. \n",
    "\n",
    "-----------------------------\n",
    "## **Project Dataset Description**\n",
    "-----------------------------\n",
    "\n",
    "1. **restaurants.csv**: Contains information about the restaurants or stores.\n",
    "   - id: Unique identification of the restaurant or store\n",
    "   - name: Name of the restaurant\n",
    "\n",
    "2. **items.csv**: Provides details about the items sold.\n",
    "   - id: Unique identification of the item\n",
    "   - store_id: Unique identification of the store\n",
    "   - name: Name of the item\n",
    "   - kcal: A measure of energy nutrients (calories) in the item\n",
    "   - cost: The unit price of the item\n",
    "\n",
    "3. **sales.csv**: Contains sales data for items at different stores on various dates.\n",
    "   - date: Date of purchase\n",
    "   - item: Name of the item bought\n",
    "   - Price: Unit price of the item\n",
    "   - item_count: Total count of the items bought on that day\n",
    "\n",
    "-----------------------------------\n",
    "## **Project Analysis Steps To Perform**\n",
    "-----------------------------------\n",
    "\n",
    "4.1  Preliminary analysis:\n",
    "\n",
    "         4.1.1. Import the datasets into the Python environment\n",
    "         4.1.2. Examine the dataset's shape and structure, and look out for any outlier\n",
    "         4.1.3. Merge the datasets into a single dataset that includes the date, item id, price, item count, item names, kcal values, store id, and store name\n",
    "\n",
    "4.2  Exploratory data analysis:\n",
    "\n",
    "         4.2.1. Examine the overall date wise sales to understand the pattern\n",
    "         4.2.2. Find out how sales fluctuate across different days of the week\n",
    "         4.2.3. Look for any noticeable trends in the sales data for different months of the year\n",
    "         4.2.4. Examine the sales distribution across different quarters averaged over the years. Identify any noticeable patterns.\n",
    "         4.2.5. Compare the performances of the different restaurants. Find out which restaurant had the most sales and look at the sales for each restaurant across different years, months, and days.\n",
    "         4.2.6. Identify the most popular items overall and the stores where they are being sold. Also, find out the most popular item at each store.\n",
    "         4.2.7. Determine if the store with the highest sales volume is also making the most money per day\n",
    "         4.2.8. Identify the most expensive item at each restaurant and find out its calorie count\n",
    "\n",
    "4.3 Forecasting using machine learning algorithms\n",
    "\n",
    "         4.3.1. Forecasting using machine learning algorithms\n",
    "            4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on\n",
    "            4.3.1.2. Use the data from the last six months as the testing data\n",
    "            4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances\n",
    "            4.3.1.4. Use the best-performing models to make a forecast for the next year\n",
    "\n",
    "4.4 Forecasting using deep learning algorithms\n",
    "\n",
    "         4.4.1. Use sales amount for predictions instead of item count\n",
    "         4.4.2. Build a long short-term memory (LSTM) model for predictions\n",
    "            4.4.2.1. Define the train and test series\n",
    "            4.4.2.2. Generate synthetic data for the last 12 months\n",
    "            4.4.2.3. Build and train an LSTM model.\n",
    "            4.4.2.4. Use the model to make predictions for the test data.\n",
    "         4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance\n",
    "         4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Preliminary analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1.1. Import Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import calendar\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import andrews_curves, parallel_coordinates\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.fft import fft\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score,roc_auc_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample, to_categorical\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1971)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "\n",
    "DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "restaurants_ds_file = f'{DATASET_PATH}/resturants.csv'\n",
    "items_ds_file = f'{DATASET_PATH}/items.csv'\n",
    "sales_ds_file = f'{DATASET_PATH}/sales.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "restaurants_df = pd.read_csv(restaurants_ds_file)\n",
    "items_df = pd.read_csv(items_ds_file)\n",
    "sales_df = pd.read_csv(sales_ds_file)\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"Restaurants dataset:\")\n",
    "print(restaurants_df.head())\n",
    "print(\"\\nItems dataset:\")\n",
    "print(items_df.head())\n",
    "print(\"\\nSales dataset:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code block imports necessary libraries (`pandas`, `numpy`, `matplotlib`, and `seaborn`) and reads the three CSV files into pandas DataFrames. It then displays the first few rows of each dataset to give an initial view of the data.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Importing and examining the datasets is crucial as it allows us to understand the structure and content of our data. This step helps identify any immediate issues with data formatting or missing values and provides a foundation for all subsequent analyses.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1.2. Examine the dataset's shape and structure, and look out for any outlier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of each dataset\n",
    "print(\"Restaurants dataset shape:\", restaurants_df.shape)\n",
    "print(\"Items dataset shape:\", items_df.shape)\n",
    "print(\"Sales dataset shape:\", sales_df.shape)\n",
    "\n",
    "# Display info for each dataset\n",
    "print(\"\\nRestaurants dataset info:\")\n",
    "restaurants_df.info()\n",
    "print(\"\\nItems dataset info:\")\n",
    "items_df.info()\n",
    "print(\"\\nSales dataset info:\")\n",
    "sales_df.info()\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"\\nRestaurants Dataset Statistics:\")\n",
    "print(restaurants_df.describe())\n",
    "print(\"\\nItems Dataset Statistics:\")\n",
    "print(items_df.describe())\n",
    "print(\"\\nSales Dataset Statistics:\")\n",
    "print(sales_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in Restaurants dataset:\")\n",
    "print(restaurants_df.isnull().sum())\n",
    "print(\"\\nMissing values in Items dataset:\")\n",
    "print(items_df.isnull().sum())\n",
    "print(\"\\nMissing values in Sales dataset:\")\n",
    "print(sales_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicates in Restaurants dataset:\", restaurants_df.duplicated().sum())\n",
    "print(\"Duplicates in Items dataset:\", items_df.duplicated().sum())\n",
    "print(\"Duplicates in Sales dataset:\", sales_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "# Print info about the datasets\n",
    "print(f\"\\nSales_df information: \\n\")\n",
    "print(sales_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using box plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131)\n",
    "sns.boxplot(data=items_df, y='kcal')\n",
    "plt.title('Kcal Distribution')\n",
    "plt.subplot(132)\n",
    "sns.boxplot(data=items_df, y='cost')\n",
    "plt.title('Cost Distribution')\n",
    "plt.subplot(133)\n",
    "sns.boxplot(data=sales_df, y='item_count')\n",
    "plt.title('Item Count Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# datasets = [restaurants_df, items_df, sales_df]\n",
    "datasets = [items_df, sales_df]\n",
    "\n",
    "restaurants_df.attrs['name'] = 'Restaurants Dataset'\n",
    "items_df.attrs['name'] = 'Items Dataset'\n",
    "sales_df.attrs['name'] = 'Sales Dataset'\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "def plot_correlation_matrix(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    corr = df.select_dtypes(include=[np.number]).corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title(f'{dataset_name} Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_correlation_matrix(df)\n",
    "\n",
    "# 2. Pairplot\n",
    "def create_pairplot(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    sns.pairplot(df.select_dtypes(include=[np.number]))\n",
    "    plt.suptitle(f'{dataset_name} Pairplot of Numerical Variables', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    create_pairplot(df)\n",
    "\n",
    "# 5. Time Series Decomposition\n",
    "def plot_time_series_decomposition(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    df_temp = df.copy()\n",
    "    df_temp.set_index('date', inplace=True)\n",
    "    result = seasonal_decompose(df_temp['item_count'], model='additive', period=30)  # Adjust period as needed\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Time Series Decomposition of {dataset_name}', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series_decomposition(sales_df)\n",
    "\n",
    "# 6. Distribution Plots\n",
    "def plot_distributions(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    n_cols = 2\n",
    "    n_rows = (len(num_cols) + 1) // 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    for i, col in enumerate(num_cols):\n",
    "        ax = axes[i//n_cols, i%n_cols]\n",
    "        sns.histplot(df[col], kde=True, ax=ax)\n",
    "        ax.set_title(f'Distribution of {col} in {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_distributions(df)\n",
    "    \n",
    "# 11. Scatter Plot Matrix\n",
    "def plot_scatter_matrix(df):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    pd.plotting.scatter_matrix(df.select_dtypes(include=[np.number]), figsize=(15, 15), diagonal='kde')\n",
    "    plt.suptitle(f'Scatter Plot Matrix of {dataset_name}', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    plot_scatter_matrix(df)\n",
    "\n",
    "\n",
    "# 12. 3D Scatter Plot\n",
    "def plot_3d_scatter(df, x_col, y_col, z_col):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df[x_col], df[y_col], df[z_col])\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_zlabel(z_col)\n",
    "    plt.title(f'3D Scatter Plot of {dataset_name}')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_scatter(items_df, 'kcal', 'cost', 'id')  # Example\n",
    "\n",
    "# 15. Cluster Analysis\n",
    "from sklearn.cluster import KMeans\n",
    "def perform_cluster_analysis(df, n_clusters=3):\n",
    "    # Retrieve the dataset name from the DataFrame's attributes\n",
    "    dataset_name = df.attrs.get('name', 'Dataset')\n",
    "    numeric_data = df.select_dtypes(include=[np.number])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(numeric_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(numeric_data.iloc[:, 0], numeric_data.iloc[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.title(f'K-means Clustering of Numeric Variables in {dataset_name}')\n",
    "    plt.xlabel(numeric_data.columns[0])\n",
    "    plt.ylabel(numeric_data.columns[1])\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "for df in datasets:\n",
    "    perform_cluster_analysis(df)\n",
    "# perform_cluster_analysis(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code examines the shape, structure, and quality of each dataset. It checks the number of rows and columns, data types of each column, presence of missing values, and existence of duplicate entries.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Understanding the dataset's structure and quality is crucial for data preprocessing and analysis. It helps identify potential issues like missing data or duplicates that need to be addressed before proceeding with the analysis.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Dataset Sizes:**\n",
    "   - Restaurants: 6 entries, 2 columns\n",
    "   - Items: 100 entries, 5 columns\n",
    "   - Sales: 109,600 entries, 4 columns\n",
    "\n",
    "2. **Data Quality:**\n",
    "   - No missing values in any dataset\n",
    "   - No duplicates in any dataset\n",
    "\n",
    "3. **Correlations:**\n",
    "   - Items dataset: Moderate positive correlation (0.42) between kcal and cost\n",
    "   - Sales dataset: Weak negative correlation (-0.15) between item_id and item_count\n",
    "\n",
    "4. **Distributions:**\n",
    "   - Item costs are right-skewed, with most items clustered at lower cost levels\n",
    "   - Calorie (kcal) distribution is relatively normal but with some high-calorie outliers\n",
    "   - Sales prices show multiple peaks, suggesting different price tiers\n",
    "   - Item counts in sales are heavily right-skewed, with many low-count transactions\n",
    "\n",
    "5. **Time Series:**\n",
    "   - Sales data shows clear seasonal patterns with regular peaks and troughs\n",
    "   - There's an overall increasing trend in sales over time\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Menu Composition:**\n",
    "   - The restaurant chain offers a diverse menu with varying nutritional values and prices\n",
    "   - There's a slight tendency for higher-calorie items to cost more\n",
    "\n",
    "2. **Pricing Strategy:**\n",
    "   - The multi-modal price distribution suggests tiered pricing or distinct categories of items\n",
    "\n",
    "3. **Sales Patterns:**\n",
    "   - Strong seasonality in sales, possibly reflecting weekly or monthly patterns\n",
    "   - Overall positive trend in sales, indicating business growth\n",
    "\n",
    "4. **Customer Behavior:**\n",
    "   - Most transactions involve a small number of items, with some large orders as outliers\n",
    "\n",
    "5. **Restaurant Chain Size:**\n",
    "   - With only 6 restaurants, this appears to be a small to medium-sized chain\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Menu Optimization:**\n",
    "   - Analyze the relationship between item cost, price, and sales volume to optimize menu offerings\n",
    "   - Consider introducing more items in the mid-range of calories and cost to balance the menu\n",
    "\n",
    "2. **Pricing Strategy Review:**\n",
    "   - Evaluate the effectiveness of the current tiered pricing structure\n",
    "   - Consider dynamic pricing based on identified seasonal patterns to maximize revenue\n",
    "\n",
    "3. **Seasonal Promotions:**\n",
    "   - Develop targeted marketing campaigns aligned with the observed seasonal sales patterns\n",
    "   - Prepare inventory and staffing based on predicted busy periods\n",
    "\n",
    "4. **Upselling Initiatives:**\n",
    "   - Given the prevalence of small transactions, implement strategies to increase average order size\n",
    "\n",
    "5. **Expansion Consideration:**\n",
    "   - With positive sales trends, explore opportunities for opening new locations\n",
    "\n",
    "6. **Data Collection Enhancement:**\n",
    "   - Include more detailed time data (hour of day) to analyze intra-day sales patterns\n",
    "   - Collect customer demographic data to enable more targeted marketing and menu planning\n",
    "\n",
    "7. **Outlier Analysis:**\n",
    "   - Investigate high-calorie and high-cost menu items to ensure they are contributing positively to overall sales and profitability\n",
    "\n",
    "8. **Health-Conscious Options:**\n",
    "   - Given the wide range of calorie contents, ensure there are sufficient low-calorie options to cater to health-conscious customers\n",
    "\n",
    "9. **Loyalty Program:**\n",
    "   - Consider implementing a loyalty program to encourage repeat business and gather more customer-specific data\n",
    "\n",
    "10. **Operational Efficiency:**\n",
    "    - Use the sales pattern data to optimize staff scheduling and inventory management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1.2. Merge the datasets into a single dataset that includes the date, item id, price, item count, item names, kcal values, store id, and store name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with items\n",
    "merged_df = pd.merge(sales_df, items_df, left_on='item_id', right_on='id', how='left')\n",
    "\n",
    "# Merge with restaurants\n",
    "merged_df = pd.merge(merged_df, restaurants_df, left_on='store_id', right_on='id', how='left')\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "merged_df = merged_df.rename(columns={'name_x': 'item_name', 'name_y': 'restaurant_name'})\n",
    "\n",
    "# Select relevant columns\n",
    "final_df = merged_df[['date', 'item_id', 'price', 'item_count', 'item_name', 'kcal', 'store_id', 'restaurant_name']]\n",
    "\n",
    "# Convert date to datetime\n",
    "# final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "print(final_df.head())\n",
    "\n",
    "# Display info of the merged dataset\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code merges the three datasets based on common identifiers (item_id and store_id). It then selects relevant columns, renames them for clarity, and converts the date column to datetime format.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Merging the datasets is crucial for conducting comprehensive analyses that involve information from all three sources. It allows us to examine relationships between sales, item characteristics, and restaurant information in a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Exploratory data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.1. Examine the overall date wise sales to understand the pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of entries for each restaurant_name\n",
    "restaurant_counts = final_df['restaurant_name'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Restaurant Sales Transaction Counts From Combined Dataset: \\n\")\n",
    "print(restaurant_counts)\n",
    "print()\n",
    "\n",
    "restaurant_items = final_df.groupby('restaurant_name')['item_name'].apply(set)\n",
    "\n",
    "# Find the common item_names across all restaurants\n",
    "common_items = set.intersection(*restaurant_items)\n",
    "\n",
    "# Check if there are common items and print the appropriate message\n",
    "if common_items:\n",
    "    print(\"Common item_names between restaurants:\")\n",
    "    print(sorted(common_items))\n",
    "else:\n",
    "    print(f\"No shared items between restaurants.\\n\")\n",
    "\n",
    "# Group the DataFrame by restaurant_name and get unique item_names for each restaurant\n",
    "restaurant_items = final_df.groupby('restaurant_name')['item_name'].apply(set)\n",
    "\n",
    "# Print the unique item_names for each restaurant in alphabetical order\n",
    "for restaurant, items in restaurant_items.items():\n",
    "    sorted_items = sorted(items)\n",
    "    print(f\"Restaurant: {restaurant}\")\n",
    "    print(f\"Unique Items: {sorted_items}\")\n",
    "    print()\n",
    "    \n",
    "# Group the DataFrame by restaurant_name and item_name, and aggregate the prices\n",
    "restaurant_item_prices = final_df.groupby(['restaurant_name', 'item_name'])['price'].mean().reset_index()\n",
    "\n",
    "# Iterate over each restaurant and print the table of unique items and their prices\n",
    "for restaurant, group in restaurant_item_prices.groupby('restaurant_name'):\n",
    "    print(f\"Restaurant: {restaurant}\")\n",
    "    print(group[['item_name', 'price']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "# Group the DataFrame by item_name and calculate the standard deviation of prices for each item\n",
    "price_variability = final_df.groupby('item_name')['price'].std().reset_index()\n",
    "\n",
    "# Filter items with non-zero standard deviation\n",
    "variable_price_items = price_variability[price_variability['price'] > 0]\n",
    "\n",
    "# Check if there are any items with price variability and print the appropriate message\n",
    "if not variable_price_items.empty:\n",
    "    print(\"Items with price variability:\")\n",
    "    for _, row in variable_price_items.iterrows():\n",
    "        item_name = row['item_name']\n",
    "        std_dev = row['price']\n",
    "        restaurants_selling_item = final_df[final_df['item_name'] == item_name]['restaurant_name'].unique()\n",
    "        print(f\"Item: {item_name}, Std Dev: {std_dev:.2f}, Restaurants: {', '.join(restaurants_selling_item)}\")\n",
    "else:\n",
    "    print(\"No price variability found for any items.\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Group by date and calculate total sales\n",
    "daily_sales = final_df.groupby('date').agg({'price': 'sum', 'item_count': 'sum'})\n",
    "daily_sales['total_sales'] = daily_sales['price'] * daily_sales['item_count']\n",
    "\n",
    "# Plot daily sales\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales.index, daily_sales['total_sales'])\n",
    "plt.title('Daily Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot 7-day moving average\n",
    "daily_sales['7_day_ma'] = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales.index, daily_sales['total_sales'], alpha=0.5, label='Daily Sales')\n",
    "plt.plot(daily_sales.index, daily_sales['7_day_ma'], color='red', label='7-day Moving Average')\n",
    "plt.title('Daily Sales and 7-day Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# First, let's check the structure of your DataFrame\n",
    "# print(daily_sales.index)\n",
    "# print(daily_sales.columns)\n",
    "\n",
    "# If the index is not already a DatetimeIndex, we'll try to convert it\n",
    "if not isinstance(daily_sales.index, pd.DatetimeIndex):\n",
    "    # Check if there's a column that could be a date\n",
    "    date_columns = daily_sales.select_dtypes(include=[np.datetime64]).columns\n",
    "    if len(date_columns) > 0:\n",
    "        date_column = date_columns[0]\n",
    "        daily_sales.set_index(date_column, inplace=True)\n",
    "    else:\n",
    "        # If no date column is found, we'll create a date range\n",
    "        daily_sales.index = pd.date_range(start='2019-01-01', periods=len(daily_sales), freq='D')\n",
    "\n",
    "# Ensure the index is sorted\n",
    "daily_sales.sort_index(inplace=True)\n",
    "\n",
    "# 1. Decomposition Plot\n",
    "def plot_decomposition():\n",
    "    result = seasonal_decompose(daily_sales['total_sales'], model='additive', period=365)\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16))\n",
    "    result.observed.plot(ax=ax1)\n",
    "    ax1.set_title('Observed')\n",
    "    result.trend.plot(ax=ax2)\n",
    "    ax2.set_title('Trend')\n",
    "    result.seasonal.plot(ax=ax3)\n",
    "    ax3.set_title('Seasonal')\n",
    "    result.resid.plot(ax=ax4)\n",
    "    ax4.set_title('Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Heatmap of Daily Sales\n",
    "def plot_daily_heatmap():\n",
    "    daily_data = daily_sales['total_sales'].resample('D').sum()\n",
    "    heatmap_data = pd.DataFrame({\n",
    "        'year': daily_data.index.year,\n",
    "        'day_of_year': daily_data.index.dayofyear,\n",
    "        'sales': daily_data.values\n",
    "    })\n",
    "    heatmap_pivot = heatmap_data.pivot(index='year', columns='day_of_year', values='sales')\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(heatmap_pivot, cmap='YlOrRd')\n",
    "    plt.title('Daily Sales Heatmap')\n",
    "    plt.xlabel('Day of Year')\n",
    "    plt.ylabel('Year')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Year-over-Year Comparison\n",
    "def plot_year_over_year():\n",
    "    daily_sales['year'] = daily_sales.index.year\n",
    "    daily_sales['day_of_year'] = daily_sales.index.dayofyear\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for year in daily_sales['year'].unique():\n",
    "        year_data = daily_sales[daily_sales['year'] == year]\n",
    "        plt.plot(year_data['day_of_year'], year_data['total_sales'], label=str(year))\n",
    "    plt.title('Year-over-Year Sales Comparison')\n",
    "    plt.xlabel('Day of Year')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Autocorrelation and Partial Autocorrelation Plots\n",
    "def plot_acf_pacf():\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    plot_acf(daily_sales['total_sales'], ax=ax1, lags=50)\n",
    "    plot_pacf(daily_sales['total_sales'], ax=ax2, lags=50)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Cumulative Sales Plot\n",
    "def plot_cumulative_sales():\n",
    "    cumulative_sales = daily_sales['total_sales'].cumsum()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_sales.index, cumulative_sales)\n",
    "    plt.title('Cumulative Sales Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Sales')\n",
    "    plt.show()\n",
    "\n",
    "# 7. Seasonal Subseries Plot\n",
    "def plot_seasonal_subseries():\n",
    "    daily_sales['quarter'] = daily_sales.index.quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    for i, quarter in enumerate([1, 2, 3, 4]):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        quarter_data = daily_sales[daily_sales['quarter'] == quarter]\n",
    "        sns.lineplot(x=quarter_data.index.dayofyear, y='total_sales', hue=quarter_data.index.year, data=quarter_data, ax=ax)\n",
    "        ax.set_title(f'Q{quarter} Sales')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 8. Rolling Statistics\n",
    "def plot_rolling_stats():\n",
    "    rolling_mean = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "    rolling_std = daily_sales['total_sales'].rolling(window=7).std()\n",
    "    rolling_median = daily_sales['total_sales'].rolling(window=7).median()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_sales.index, daily_sales['total_sales'], label='Daily Sales')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label='7-day Moving Average')\n",
    "    plt.plot(rolling_std.index, rolling_std, label='7-day Moving Std')\n",
    "    plt.plot(rolling_median.index, rolling_median, label='7-day Moving Median')\n",
    "    plt.title('Rolling Statistics of Daily Sales')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 9. Fourier Transform\n",
    "def plot_fourier_transform():\n",
    "    sales_fft = fft(daily_sales['total_sales'].values)\n",
    "    frequencies = np.fft.fftfreq(len(sales_fft))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(frequencies, np.abs(sales_fft))\n",
    "    plt.title('Fourier Transform of Sales Data')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xlim(0, 0.5)  # Only show positive frequencies\n",
    "    plt.show()\n",
    "\n",
    "# 10. Anomaly Detection\n",
    "def detect_anomalies():\n",
    "    rolling_mean = daily_sales['total_sales'].rolling(window=7).mean()\n",
    "    rolling_std = daily_sales['total_sales'].rolling(window=7).std()\n",
    "    anomalies = daily_sales[(daily_sales['total_sales'] > rolling_mean + 2*rolling_std) | \n",
    "                            (daily_sales['total_sales'] < rolling_mean - 2*rolling_std)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_sales.index, daily_sales['total_sales'], label='Daily Sales')\n",
    "    plt.scatter(anomalies.index, anomalies['total_sales'], color='red', label='Anomalies')\n",
    "    plt.title('Daily Sales with Anomalies')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run all the functions\n",
    "plot_decomposition()\n",
    "plot_daily_heatmap()\n",
    "plot_year_over_year()\n",
    "# plot_boxplots()\n",
    "plot_acf_pacf()\n",
    "plot_cumulative_sales()\n",
    "plot_seasonal_subseries()\n",
    "plot_rolling_stats()\n",
    "plot_fourier_transform()\n",
    "detect_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. Data Overview and Aggregation\n",
    "\n",
    "   - Counts sales transactions per restaurant\n",
    "   - Identifies common items across restaurants\n",
    "   - Lists unique items for each restaurant\n",
    "   - Calculates average prices for items at each restaurant\n",
    "   - Analyzes price variability across restaurants\n",
    "\n",
    "2. Time Series Analysis\n",
    "\n",
    "   - Creates daily sales totals\n",
    "   - Plots daily sales over time\n",
    "   - Calculates and plots a 7-day moving average\n",
    "\n",
    "3. Advanced Time Series Analysis\n",
    "\n",
    "   - **Seasonal Decomposition:** Separates the time series into trend, seasonality, and residual components\n",
    "   - **Heatmap of Daily Sales:** Visualizes sales patterns across years and days\n",
    "   - **Year-over-Year Comparison:** Compares sales patterns across different years\n",
    "   - **Box Plots by Month and Day of Week:** Shows sales distribution patterns\n",
    "   - **Autocorrelation and Partial Autocorrelation Plots:** Identifies time-dependent patterns and potential forecasting models\n",
    "   - **Cumulative Sales Plot:** Visualizes overall growth trends\n",
    "   - **Seasonal Subseries Plot:** Compares seasonal patterns across years\n",
    "   - **Rolling Statistics:** Provides smoothed trends and variability measures\n",
    "   - **Fourier Transform:** Identifies dominant frequencies in the sales data\n",
    "   - **Anomaly Detection:** Highlights unusual sales days\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "In the data science process, this comprehensive EDA is crucial because it:\n",
    "\n",
    "1. Helps understand the underlying structure and patterns in the data\n",
    "2. Guides feature engineering for predictive modeling\n",
    "3. Informs the selection of appropriate forecasting models\n",
    "4. Identifies potential issues or anomalies in the data\n",
    "5. Provides business insights for strategic decision-making\n",
    "6. Helps in formulating hypotheses for further analysis\n",
    "7. Aids in communicating findings to stakeholders through visualizations\n",
    "\n",
    "- Provides a high-level understanding of the dataset structure, product offerings, and pricing strategies across different restaurants.\n",
    "- These visualizations help identify overall trends and smooth out daily fluctuations for clearer pattern recognition.\n",
    "- These analyses provide deep insights into various aspects of the sales data:\n",
    "   - Seasonal patterns and trends\n",
    "   - Weekly and monthly variations\n",
    "   - Year-over-year growth\n",
    "   - Potential forecasting model structures\n",
    "   - Unusual events or outliers\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Seasonal Patterns:**\n",
    "   - Strong yearly seasonality is evident, with peaks occurring around the same time each year.\n",
    "   - Weekly patterns are visible, with sales generally higher on weekends (Friday and Saturday) and lower on Sundays.\n",
    "\n",
    "2. **Long-term Trend:**\n",
    "   - There's a clear upward trend in sales over the three-year period from 2019 to 2021.\n",
    "\n",
    "3. **Daily Variations:**\n",
    "   - Significant day-to-day fluctuations in sales are present.\n",
    "\n",
    "4. **Yearly Comparison:**\n",
    "   - Sales patterns are similar across years, but with noticeable growth year-over-year.\n",
    "\n",
    "5. **Monthly Distribution:**\n",
    "   - Sales tend to be higher in mid-year months (June-August) and lower in early and late months of the year.\n",
    "\n",
    "6. **Autocorrelation:**\n",
    "   - Strong positive autocorrelation at lag 7, indicating weekly patterns.\n",
    "   - Decreasing but significant autocorrelation at larger lags, suggesting longer-term trends.\n",
    "\n",
    "7. **Fourier Transform:**\n",
    "   - Clear peaks at certain frequencies, confirming the presence of regular cyclical patterns.\n",
    "\n",
    "8. **Anomalies:**\n",
    "   - Few anomalies detected, mostly during peak sales periods.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Seasonality:** The business has strong seasonal components, both on a yearly and weekly basis.\n",
    "\n",
    "2. **Growth:** The company is experiencing consistent growth over the observed period.\n",
    "\n",
    "3. **Day of Week Impact:** Weekends are crucial for sales, while Sundays see a significant drop.\n",
    "\n",
    "4. **Yearly Consistency:** The overall sales pattern remains consistent year to year, despite growth.\n",
    "\n",
    "5. **Summer Peak:** Mid-year (summer) months generally outperform other seasons.\n",
    "\n",
    "6. **Predictable Patterns:** The strong autocorrelation and clear Fourier transform peaks indicate highly predictable sales patterns.\n",
    "\n",
    "7. **Stability:** Few anomalies suggest a relatively stable business model with predictable fluctuations.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Seasonal Staffing:** Adjust staffing levels to match the observed seasonal patterns, ensuring adequate coverage during peak periods.\n",
    "\n",
    "2. **Inventory Management:** Optimize inventory based on the predictable seasonal and weekly patterns to minimize waste and stockouts.\n",
    "\n",
    "3. **Marketing Campaigns:** Time marketing efforts to coincide with typically slower periods to boost sales during these times.\n",
    "\n",
    "4. **Weekend Focus:** Develop strategies to maximize revenue during peak weekend days (Friday and Saturday).\n",
    "\n",
    "5. **Sunday Specials:** Create promotions or events to increase Sunday sales and smooth out the weekly pattern.\n",
    "\n",
    "6. **Year-Round Growth Strategies:** While maintaining focus on peak seasons, develop strategies to boost sales during typically slower months.\n",
    "\n",
    "7. **Capacity Planning:** Use the observed growth trend to plan for increased capacity in the coming years.\n",
    "\n",
    "8. **Anomaly Investigation:** Analyze the few detected anomalies to understand their causes and potentially replicate positive outliers.\n",
    "\n",
    "9. **Forecasting Model:** Develop a sales forecasting model incorporating the observed seasonality and trends for better business planning.\n",
    "\n",
    "10. **Customer Behavior Analysis:** Conduct deeper analysis into customer behavior during different seasons and days of the week to tailor offerings.\n",
    "\n",
    "11. **Menu Optimization:** Adjust menu items seasonally based on observed sales patterns to maximize profitability.\n",
    "\n",
    "12. **Expansion Consideration:** Given the consistent growth, consider expanding to new locations or markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.2. Find out how sales fluctuate across different days of the week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week column\n",
    "daily_sales['day_of_week'] = daily_sales.index.dayofweek\n",
    "daily_sales['day_of_week_name'] = daily_sales.index.day_name()\n",
    "\n",
    "# Calculate average sales by day of week\n",
    "avg_sales_by_day = daily_sales.groupby('day_of_week_name')['total_sales'].mean().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "total_sales_by_day = daily_sales.groupby('day_of_week_name')['item_count'].sum().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# Plot average sales by day of week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=avg_sales_by_day.index, y=avg_sales_by_day.values, palette='Spectral')\n",
    "plt.title('Average Sales by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(avg_sales_by_day.values):\n",
    "    plt.text(index, value, f'${value:,.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=total_sales_by_day.index, y=total_sales_by_day.values, palette='Spectral')\n",
    "plt.title('Total Item Sales by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Total Items Sold')\n",
    "\n",
    "# Annotate each bar with the total items sold value formatted with commas\n",
    "for index, value in enumerate(total_sales_by_day.values):\n",
    "    plt.text(index, value, f'{value:,.0f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Define the order of the days\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Boxplot of sales by day of week\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='day_of_week_name', y='total_sales', data=daily_sales, order=days_order, palette='Spectral')\n",
    "plt.title('Sales Distribution by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Annotate the median values\n",
    "medians = daily_sales.groupby('day_of_week_name')['total_sales'].median().reindex(days_order)\n",
    "for index, median in enumerate(medians):\n",
    "    plt.text(index, median, f'${median:,.2f}', ha='center', va='bottom', color='black', weight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 3. Day of Week Trend Over Time\n",
    "def plot_day_of_week_trend():\n",
    "    daily_sales['year_month'] = daily_sales.index.to_period('M')\n",
    "    trend_data = daily_sales.groupby(['year_month', 'day_of_week_name'])['total_sales'].mean().unstack()\n",
    "    ax = trend_data.plot(figsize=(12, 6), colormap='Spectral')\n",
    "    plt.title('Day of Week Sales Trend Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sales')\n",
    "    plt.legend(title='Day of Week', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. Average Order Value by Day\n",
    "def plot_avg_order_value():\n",
    "    avg_order = daily_sales.groupby('day_of_week_name')['total_sales'].mean() / daily_sales.groupby('day_of_week_name')['item_count'].mean()\n",
    "    ax = avg_order.plot(kind='bar', figsize=(10, 6), color=sns.color_palette('Spectral', len(avg_order)))\n",
    "    plt.title('Average Order Value by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Average Order Value')\n",
    "\n",
    "    # Annotate each bar with the average order value formatted with commas and dollar signs\n",
    "    for index, value in enumerate(avg_order.values):\n",
    "        plt.text(index, value, f'${value:,.2f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# 6. Violin Plot of Sales Distribution\n",
    "def plot_sales_violin():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(x='day_of_week_name', y='total_sales', data=daily_sales, order=days_order, palette='Spectral')\n",
    "    plt.title('Sales Distribution by Day of Week (Violin Plot)')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Total Sales')\n",
    "\n",
    "    # Annotate the median values\n",
    "    medians = daily_sales.groupby('day_of_week_name')['total_sales'].median().reindex(days_order)\n",
    "    for index, median in enumerate(medians):\n",
    "        plt.text(index, median, f'${median:,.2f}', ha='center', va='bottom', color='black', weight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 9. Sales Variance Plot\n",
    "def plot_sales_variance():\n",
    "    cv = daily_sales.groupby('day_of_week_name')['total_sales'].agg(lambda x: x.std() / x.mean())\n",
    "    ax = cv.plot(kind='bar', figsize=(10, 6), color=sns.color_palette('Spectral', len(cv)))\n",
    "    plt.title('Coefficient of Variation of Sales by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Coefficient of Variation')\n",
    "\n",
    "    # Annotate each bar with the coefficient of variation value formatted with commas\n",
    "    for index, value in enumerate(cv.values):\n",
    "        plt.text(index, value, f'{value:,.2f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 10. Day-to-Day Sales Transition Heatmap\n",
    "def plot_day_to_day_transition():\n",
    "    daily_sales['next_day_sales'] = daily_sales['total_sales'].shift(-1)\n",
    "    transition = daily_sales.groupby(['day_of_week_name', daily_sales['day_of_week_name'].shift(-1)])[['total_sales', 'next_day_sales']].mean()\n",
    "    transition = transition['next_day_sales'].unstack()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(transition, annot=True, fmt=\".2f\", cmap=\"Spectral\")\n",
    "    plt.title('Day-to-Day Sales Transition')\n",
    "    plt.xlabel('Next Day')\n",
    "    plt.ylabel('Current Day')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the functions to generate the plots\n",
    "plot_day_of_week_trend()\n",
    "plot_avg_order_value()\n",
    "plot_sales_violin()\n",
    "plot_sales_variance()\n",
    "plot_day_to_day_transition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code analyzes sales fluctuations across different days of the week. It calculates average sales for each day and creates a bar plot to visualize these averages. Additionally, it generates a box plot to show the distribution of sales for each day of the week.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Understanding weekly sales patterns is crucial for inventory management, staffing decisions, and marketing strategies. It can help businesses optimize their operations based on expected demand for different days of the week.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Sales Progression:**\n",
    "   - Sales steadily increase from Monday to Saturday, with Friday and Saturday being the peak days.\n",
    "   - Sunday shows a significant drop, having the lowest sales of the week.\n",
    "\n",
    "2. **Peak Days:**\n",
    "   - Friday and Saturday consistently outperform other days in average sales, total items sold, and sales distribution.\n",
    "   - Saturday slightly edges out Friday in average sales ($971,591 vs $963,992).\n",
    "\n",
    "3. **Weekend Effect:**\n",
    "   - While Friday and Saturday show peak performance, Sunday unexpectedly has the lowest sales.\n",
    "\n",
    "4. **Mid-Week Performance:**\n",
    "   - Wednesday and Thursday show moderate sales, higher than early week but lower than Friday/Saturday.\n",
    "\n",
    "5. **Sales Variability:**\n",
    "   - Tuesday shows the highest coefficient of variation, indicating more inconsistent sales.\n",
    "   - Friday and Saturday, despite having highest sales, show relatively low variability.\n",
    "\n",
    "6. **Average Order Value:**\n",
    "   - Highest on Sundays ($7,542), despite having the lowest total sales.\n",
    "   - Relatively consistent across weekdays, with a slight increase on weekends.\n",
    "\n",
    "7. **Sales Distribution:**\n",
    "   - Violin plots show Friday and Saturday have not only higher sales but also wider distributions, indicating potential for very high sales days.\n",
    "   - Sunday's distribution is narrower, suggesting more consistent (though lower) sales.\n",
    "\n",
    "8. **Day-to-Day Transitions:**\n",
    "   - Largest positive transition is from Thursday to Friday.\n",
    "   - Largest negative transition is from Saturday to Sunday.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Weekend Dominance:** Friday and Saturday are crucial for business, generating significantly higher sales.\n",
    "   \n",
    "2. **Sunday Anomaly:** Despite being a weekend day, Sunday consistently underperforms all other days.\n",
    "   \n",
    "3. **Weekday Buildup:** There's a clear pattern of sales building up throughout the week.\n",
    "   \n",
    "4. **Consistent Patterns:** The trends are consistent across different metrics (average sales, total items, distribution).\n",
    "   \n",
    "5. **Order Value Inverse to Volume:** Sunday's high average order value contrasts with its low total sales, suggesting fewer but larger transactions.\n",
    "   \n",
    "6. **Mid-Week Stability:** Wednesday and Thursday show stable, moderate performance.\n",
    "   \n",
    "7. **Tuesday Volatility:** Tuesday sales are the most unpredictable, potentially due to varying promotional activities or external factors.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Maximize Weekend Potential:**\n",
    "   - Ensure optimal staffing and inventory for Fridays and Saturdays.\n",
    "   - Implement strategies to handle higher customer volume efficiently.\n",
    "\n",
    "2. **Sunday Revival Strategy:**\n",
    "   - Develop special Sunday promotions or events to boost foot traffic.\n",
    "   - Capitalize on the higher average order value by encouraging more transactions.\n",
    "\n",
    "3. **Early Week Boost:**\n",
    "   - Create targeted marketing campaigns for Monday and Tuesday to increase sales on slower days.\n",
    "   - Investigate the cause of Tuesday's high variability and develop strategies to stabilize sales.\n",
    "\n",
    "4. **Staffing Optimization:**\n",
    "   - Adjust staff schedules to align with the weekly sales pattern.\n",
    "   - Ensure experienced staff are available during peak times (Friday and Saturday).\n",
    "\n",
    "5. **Inventory Management:**\n",
    "   - Adjust stock levels and preparation to match the weekly sales cycle.\n",
    "   - Implement a just-in-time inventory system for perishables, considering the weekly pattern.\n",
    "\n",
    "6. **Promotional Calendar:**\n",
    "   - Design a promotional calendar that complements the natural weekly flow.\n",
    "   - Consider running promotions early in the week to boost slower days.\n",
    "\n",
    "7. **Customer Experience Focus:**\n",
    "   - Enhance customer experience during peak days to encourage repeat visits on slower days.\n",
    "   - Implement loyalty programs that incentivize visits on typically slower days.\n",
    "\n",
    "8. **Data-Driven Decision Making:**\n",
    "   - Continuously monitor these patterns and adjust strategies based on ongoing data analysis.\n",
    "   - Conduct customer surveys to understand preferences and behaviors driving these patterns.\n",
    "\n",
    "9. **Operational Efficiency:**\n",
    "   - Streamline operations for busy weekends to handle higher volumes efficiently.\n",
    "   - Use slower days for staff training, maintenance, and preparation for busier periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.3. Look for any noticeable trends in the sales data for different months of the year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month column\n",
    "daily_sales['month'] = daily_sales.index.month\n",
    "daily_sales['month_name'] = daily_sales.index.month_name()\n",
    "\n",
    "# 2. Identify trends in sales data for different months\n",
    "# merged_df['month'] = merged_df['date'].dt.month_name()\n",
    "# monthly_sales = merged_df.groupby('month')['item_count'].sum().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "\n",
    "# Calculate average sales by month\n",
    "avg_sales_by_month = daily_sales.groupby('month_name')['total_sales'].mean().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "total_sales_by_month = daily_sales.groupby('month_name')['item_count'].sum().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "# Plot average sales by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=avg_sales_by_month.index, y=avg_sales_by_month.values, palette='viridis')\n",
    "plt.title('Average Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(avg_sales_by_month.values):\n",
    "    plt.text(index, value, f'${value:,.2f}', ha='center', va='bottom', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot total sales by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=total_sales_by_month.index, y=total_sales_by_month.values, palette='viridis')\n",
    "plt.title('Total Item Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Annotate each bar with the average sales value formatted with commas and dollar signs\n",
    "for index, value in enumerate(total_sales_by_month.values):\n",
    "    plt.text(index, value, f'{value:,.0f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of sales by month and day of week\n",
    "# Pivot the data to create a heatmap-friendly format\n",
    "# Heatmap of sales by month and day of week\n",
    "monthly_dow_sales = daily_sales.groupby(['month_name', 'day_of_week_name'])['total_sales'].mean().unstack()\n",
    "\n",
    "# Define the correct order for the days of the week and months of the year\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Reindex the DataFrame to ensure the correct order\n",
    "monthly_dow_sales = monthly_dow_sales.reindex(index=months_order, columns=days_order)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(monthly_dow_sales, cmap='YlOrRd', annot=True, fmt='.0f')\n",
    "plt.title('Average Sales by Month and Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Month')\n",
    "plt.show()\n",
    "\n",
    "# Time Series Line Plot\n",
    "daily_sales['date'] = daily_sales.index\n",
    "monthly_sales = daily_sales.resample('M', on='date')['total_sales'].sum()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.lineplot(x=monthly_sales.index, y=monthly_sales.values, linewidth=2, color='#FF6B6B')\n",
    "plt.title('Monthly Sales Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Total Sales', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.fill_between(monthly_sales.index, monthly_sales.values, alpha=0.3, color='#FF6B6B')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "# Year-over-Year Comparison\n",
    "daily_sales['year'] = daily_sales.index.year\n",
    "yearly_monthly_sales = daily_sales.groupby(['year', 'month'])['total_sales'].sum().unstack(level=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for i, year in enumerate(yearly_monthly_sales.columns):\n",
    "    plt.plot(yearly_monthly_sales.index, yearly_monthly_sales[year], label=str(year), linewidth=2, color=colors[i])\n",
    "\n",
    "plt.title('Monthly Sales: Year-over-Year Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Total Sales', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot for Monthly Distribution\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the box plot\n",
    "ax = sns.boxplot(x='month_name', y='total_sales', data=daily_sales, order=months_order, palette='viridis')\n",
    "\n",
    "# Add value annotations for medians\n",
    "medians = daily_sales.groupby('month_name')['total_sales'].median()\n",
    "vertical_offset = daily_sales['total_sales'].median() * 0.05  # offset for median labels\n",
    "\n",
    "for xtick in ax.get_xticks():\n",
    "    month = months_order[xtick]\n",
    "    ax.text(xtick, medians[month] + vertical_offset, f'${medians[month]:,.0f}', \n",
    "            horizontalalignment='center', size='x-small', color='white', weight='semibold')\n",
    "\n",
    "plt.title('Distribution of Daily Sales by Month', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Daily Sales', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust y-axis to make room for annotations\n",
    "plt.ylim(0, daily_sales['total_sales'].max() * 1.1)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative Sales Plot\n",
    "yearly_cumulative = daily_sales.groupby('year')['total_sales'].cumsum().groupby(daily_sales.index.dayofyear).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=yearly_cumulative.index, y=yearly_cumulative.values, linewidth=2, color='#6C5B7B')\n",
    "plt.title('Average Cumulative Sales Throughout the Year', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day of Year', fontsize=12)\n",
    "plt.ylabel('Cumulative Sales', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.fill_between(yearly_cumulative.index, yearly_cumulative.values, alpha=0.3, color='#6C5B7B')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "\n",
    "# Ensure you have a datetime index\n",
    "# daily_sales['date'] = pd.to_datetime(daily_sales.index)\n",
    "monthly_sales = daily_sales.resample('M', on='date')['total_sales'].sum()\n",
    "\n",
    "result = seasonal_decompose(monthly_sales, model='additive', period=12)\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 20))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#6C5B7B']\n",
    "\n",
    "# Observed\n",
    "ax1.fill_between(result.observed.index, 0, result.observed.values, alpha=0.3, color=colors[0])\n",
    "ax1.plot(result.observed.index, result.observed.values, color=colors[0])\n",
    "ax1.set_title('Observed', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Trend\n",
    "ax2.fill_between(result.trend.index, 0, result.trend.values, alpha=0.3, color=colors[1])\n",
    "ax2.plot(result.trend.index, result.trend.values, color=colors[1])\n",
    "ax2.set_title('Trend', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Seasonal\n",
    "ax3.fill_between(result.seasonal.index, 0, result.seasonal.values, alpha=0.3, color=colors[2])\n",
    "ax3.plot(result.seasonal.index, result.seasonal.values, color=colors[2])\n",
    "ax3.set_title('Seasonal', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Residual\n",
    "ax4.fill_between(result.resid.index, 0, result.resid.values, alpha=0.3, color=colors[3])\n",
    "ax4.plot(result.resid.index, result.resid.values, color=colors[3])\n",
    "ax4.set_title('Residual', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel('')  # Remove x-label from each subplot\n",
    "    ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "# Add an overall x-axis label\n",
    "fig.text(0.5, 0.04, 'Date', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Add y-axis labels\n",
    "ax1.set_ylabel('Sales', fontsize=12)\n",
    "ax2.set_ylabel('Trend', fontsize=12)\n",
    "ax3.set_ylabel('Seasonal', fontsize=12)\n",
    "ax4.set_ylabel('Residual', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code analyzes sales trends across different months of the year. It calculates and plots average sales for each month. Additionally, it creates a heatmap to visualize the relationship between months, days of the week, and sales.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Analyzing monthly sales trends helps identify seasonal patterns in the business. This information is crucial for long-term planning, inventory management, and marketing strategies. It can also inform the development of more accurate forecasting models by accounting for seasonality.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Seasonal Pattern**: There is a clear seasonal pattern in sales, with peaks during summer months (June-August) and troughs during winter months (December-February).\n",
    "\n",
    "2. **Yearly Cycle**: The sales data shows a consistent yearly cycle across the three years presented.\n",
    "\n",
    "3. **Day of Week Impact**: Fridays and Saturdays consistently show higher sales across all months, as evident from the heatmap.\n",
    "\n",
    "4. **Growth Trend**: There appears to be a slight overall growth trend from year to year, particularly noticeable in the peak months.\n",
    "\n",
    "5. **Variability**: Winter months show lower variability in sales compared to summer months, as seen in the box plot.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Summer Peak**: The businesses experiences their highest sales during the summer season, likely due to increased outdoor activities or tourism.\n",
    "\n",
    "2. **Weekend Boost**: Weekend sales, particularly Fridays and Saturdays, drive a significant portion of revenue across all months.\n",
    "\n",
    "3. **Seasonal Dependency**: The businesses are highly seasonal, with performance strongly tied to the time of year.\n",
    "\n",
    "4. **Gradual Growth**: Despite seasonal fluctuations, there's an indication of overall business growth year-over-year.\n",
    "\n",
    "5. **Winter Slowdown**: The businesses faces a consistent slowdown during winter months, which might be challenging for cash flow and resource management.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Seasonal Staffing**: Adjust staffing levels to match the seasonal demand, increasing workforce during summer months and potentially reducing hours or using temporary staff in winter.\n",
    "\n",
    "2. **Inventory Management**: Optimize inventory based on the seasonal trends to ensure adequate stock during peak months and avoid overstocking during slower periods.\n",
    "\n",
    "3. **Marketing Campaigns**: Develop targeted marketing campaigns to boost sales during slower months, potentially focusing on indoor or winter-specific promotions.\n",
    "\n",
    "4. **Weekend Focus**: Given the higher sales on Fridays and Saturdays, consider extending hours or running special promotions on these days to maximize revenue.\n",
    "\n",
    "5. **Off-Season Strategies**: Develop strategies to attract customers during the off-peak season, such as introducing new products or services that are more relevant to winter months.\n",
    "\n",
    "6. **Cash Flow Management**: Plan for the seasonal fluctuations in cash flow, potentially setting aside reserves during high-performing months to cover expenses during slower periods.\n",
    "\n",
    "7. **Customer Retention**: Implement loyalty programs or other retention strategies to encourage repeat business, especially aimed at maintaining some level of engagement during slower months.\n",
    "\n",
    "8. **Data-Driven Decision Making**: Continue to analyze sales data regularly to identify any emerging trends or changes in patterns, allowing for quick adaptations in strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.4. Examine the sales distribution across different quarters averaged over the years. Identify any noticeable patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quarter column\n",
    "daily_sales['quarter'] = daily_sales.index.quarter\n",
    "\n",
    "# Calculate average sales by quarter\n",
    "avg_sales_by_quarter = daily_sales.groupby('quarter')['total_sales'].mean()\n",
    "\n",
    "# Plot average sales by quarter\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=avg_sales_by_quarter.index, y=avg_sales_by_quarter.values, palette='viridis')\n",
    "plt.title('Average Sales by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Sales')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for i, v in enumerate(avg_sales_by_quarter.values):\n",
    "    ax.text(i, v, f'${v:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of sales by quarter\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "plt.title('Sales Distribution by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the boxplot\n",
    "medians = daily_sales.groupby('quarter')['total_sales'].median()\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='bottom', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Violin plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "plt.title('Sales Distribution by Quarter (Violin Plot)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the violin plot\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Box plot with swarm\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(x='quarter', y='total_sales', data=daily_sales, palette='viridis')\n",
    "sns.swarmplot(x='quarter', y='total_sales', data=daily_sales, color=\".25\", size=3)\n",
    "plt.title('Sales Distribution by Quarter (Box Plot with Swarm)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "\n",
    "# Add median values on the boxplot\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'${median:,.0f}', ha='center', va='bottom', color='white', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Year-over-Year Quarterly Sales Growth heatmap\n",
    "quarterly_yoy = daily_sales.groupby([daily_sales.index.year, 'quarter'])['total_sales'].sum().unstack()\n",
    "quarterly_yoy_pct = quarterly_yoy.pct_change() * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.heatmap(quarterly_yoy_pct, cmap='RdYlGn', annot=True, fmt='.1f')\n",
    "plt.title('Year-over-Year Quarterly Sales Growth (%)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Year')\n",
    "\n",
    "# Rotate the tick labels\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code examines sales distribution across different quarters of the year. It calculates and plots average sales for each quarter, creates a box plot to show the distribution of sales by quarter, and generates a line plot to visualize quarterly sales trends over the years.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Analyzing quarterly sales patterns helps identify broader seasonal trends and potential fiscal year effects on the business. This information is valuable for strategic planning, budgeting, and setting sales targets. It can also reveal how the business performance has evolved over the years on a quarterly basis.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Quarterly Sales Pattern**: \n",
    "   - The \"Average Sales by Quarter\" bar chart clearly shows that Q2 (second quarter) has the highest average sales ($925,127), followed closely by Q3 ($874,876).\n",
    "   - Q4 has the lowest average sales ($596,461), with Q1 performing slightly better ($605,732).\n",
    "\n",
    "2. **Sales Distribution**: \n",
    "   - The box plots and violin plots show that Q2 and Q3 have wider distributions, indicating more variability in sales during these quarters.\n",
    "   - Q1 and Q4 have narrower distributions, suggesting more consistent (but lower) sales.\n",
    "\n",
    "3. **Outliers**: \n",
    "   - The box plot with swarm points shows numerous outliers, particularly in Q2 and Q3, indicating some exceptionally high sales days or periods.\n",
    "\n",
    "4. **Year-over-Year Growth**: \n",
    "   - The heatmap of \"Year-over-Year Quarterly Sales Growth (%)\" shows mostly positive growth, with some variations:\n",
    "     - 2022 showed positive growth across all quarters compared to 2021.\n",
    "     - 2023 had mixed results, with strong growth in Q4 but a decline in Q1 compared to 2022.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Seasonal Business**: The restaurants' sales are highly seasonal, with peak performance in spring (Q2) and summer (Q3).\n",
    "\n",
    "2. **Winter Slowdown**: There's a significant drop in sales during the winter months (Q4), which extends partially into Q1.\n",
    "\n",
    "3. **Variability in Peak Seasons**: While Q2 and Q3 bring higher average sales, they also come with greater variability, suggesting factors like weather or events might influence performance.\n",
    "\n",
    "4. **Growth Trend**: Despite seasonal fluctuations, there's an overall positive growth trend year-over-year, indicating business expansion or increasing popularity.\n",
    "\n",
    "5. **Consistent Low Seasons**: The narrower distributions in Q1 and Q4 suggest more predictable (though lower) sales during these periods.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Seasonal Strategies**: \n",
    "   - Develop specific strategies to capitalize on the high sales potential of Q2 and Q3. This could include seasonal menus, outdoor seating expansions, or special events.\n",
    "   - Create targeted marketing campaigns and promotions for Q1 and Q4 to boost sales during slower periods.\n",
    "\n",
    "2. **Staff Optimization**: \n",
    "   - Adjust staffing levels to match seasonal demand, potentially using more part-time or seasonal workers during peak quarters.\n",
    "   - Consider reduced hours or skeleton crews during slower periods to manage costs.\n",
    "\n",
    "3. **Inventory Management**: \n",
    "   - Implement dynamic inventory management to handle the higher variability in Q2 and Q3, ensuring sufficient stock for high-sales days without overstocking.\n",
    "   - Optimize inventory for Q1 and Q4 to minimize waste during slower periods.\n",
    "\n",
    "4. **Financial Planning**: \n",
    "   - Develop a financial strategy that accounts for the significant differences in quarterly performance. This might include setting aside surplus from high-performing quarters to cover expenses in lower-performing ones.\n",
    "\n",
    "5. **Off-Season Focus**: \n",
    "   - Investigate opportunities to increase sales in Q4 and Q1, such as holiday-themed events, comfort food specials, or indoor dining experiences that attract customers despite colder weather.\n",
    "\n",
    "6. **Growth Analysis**: \n",
    "   - Conduct a detailed analysis of the factors contributing to year-over-year growth, especially in Q4 of 2023, to replicate successful strategies across other quarters and locations.\n",
    "\n",
    "7. **Outlier Investigation**: \n",
    "   - Analyze the outlier days or periods, particularly in Q2 and Q3, to understand what drives exceptionally high sales. Use these insights to potentially recreate these conditions more frequently.\n",
    "\n",
    "8. **Customer Engagement**: \n",
    "   - Develop loyalty programs or other incentives to encourage repeat business, especially aimed at maintaining customer engagement during slower quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.5. Compare the performances of the different restaurants. Find out which restaurant had the most sales and look at the sales for each restaurant across different years, months, and days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming final_df is already created and contains the necessary data\n",
    "final_df['total_sales'] = final_df['price'] * final_df['item_count']\n",
    "\n",
    "# Calculate daily sales for each restaurant\n",
    "restaurant_daily_sales = final_df.groupby(['date', 'restaurant_name']).agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index().rename(columns={'price': 'daily_sales'})\n",
    "\n",
    "# Calculate total sales by restaurant\n",
    "restaurant_sales = final_df.groupby('restaurant_name').agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).rename(columns={'price': 'total_sales'}).sort_values('total_sales', ascending=False)\n",
    "\n",
    "# Function to create individual bar plots\n",
    "def plot_individual_bars(data, x, y, title, x_label, y_label):\n",
    "    palette = sns.color_palette(\"husl\", len(data['restaurant_name'].unique()))\n",
    "    restaurants = data['restaurant_name'].unique()\n",
    "    \n",
    "    for restaurant in restaurants:\n",
    "        restaurant_data = data[data['restaurant_name'] == restaurant]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(data=restaurant_data, x=x, y=y, palette=palette)\n",
    "        plt.title(f\"{title} - {restaurant}\")\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        \n",
    "        if restaurant != \"Bob's Diner\" or title != \"Average Daily Sales\":\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(f'${p.get_height():,.2f}', \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                            ha='center', va='center', \n",
    "                            xytext=(0, 9), \n",
    "                            textcoords='offset points')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Function to create individual boxplots for each restaurant\n",
    "def plot_individual_boxplots(data, x, y, title_prefix, x_label, y_label, is_monthly=False):\n",
    "    restaurants = data['restaurant_name'].unique()\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(restaurants)))\n",
    "    \n",
    "    for restaurant, color in zip(restaurants, colors):\n",
    "        restaurant_data = data[data['restaurant_name'] == restaurant]\n",
    "        \n",
    "        if is_monthly:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            ax = sns.boxplot(data=restaurant_data, x='month', y=y, order=range(1, 13), palette='viridis')\n",
    "            plt.title(f\"{title_prefix} - {restaurant}\")\n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            \n",
    "            month_names = [calendar.month_abbr[i] for i in range(1, 13)]\n",
    "            plt.xticks(range(12), month_names)\n",
    "            \n",
    "            medians = restaurant_data.groupby('month')[y].median()\n",
    "            for i, median in enumerate(medians):\n",
    "                ax.text(i, median, f'${median:,.2f}', \n",
    "                        horizontalalignment='center', size='small', color='white', \n",
    "                        weight='semibold', bbox=dict(facecolor='black', edgecolor='none', alpha=0.5))\n",
    "        else:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            ax = sns.boxplot(data=restaurant_data, x=x, y=y, color=color)\n",
    "            plt.title(f\"{title_prefix} - {restaurant}\")\n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            \n",
    "            medians = restaurant_data.groupby(x)[y].median().values\n",
    "            pos = range(len(medians))\n",
    "            for tick, label in zip(pos, ax.get_xticklabels()):\n",
    "                ax.text(pos[tick], medians[tick], f'${medians[tick]:,.2f}', \n",
    "                        horizontalalignment='center', size='small', color='black', \n",
    "                        weight='semibold', rotation=0, alpha=0.7, \n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 1. Total Sales by Restaurant (Dual Axis) Plot\n",
    "bobs_data = restaurant_sales[restaurant_sales.index == \"Bob's Diner\"]\n",
    "other_data = restaurant_sales[restaurant_sales.index != \"Bob's Diner\"]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "sns.barplot(x=other_data.index, y='total_sales', data=other_data, ax=ax1, palette='husl')\n",
    "ax1.set_xlabel('Restaurant')\n",
    "ax1.set_ylabel('Total Sales (Other Restaurants)')\n",
    "ax1.tick_params(axis='x')\n",
    "\n",
    "for p in ax1.patches:\n",
    "    ax1.annotate('${:,.2f}'.format(p.get_height()), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', va='center', \n",
    "                 xytext=(0, 9), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.barplot(x=bobs_data.index, y='total_sales', data=bobs_data, ax=ax2, color='red', alpha=0.5)\n",
    "ax2.set_ylabel(\"Total Sales (Bob's Diner)\")\n",
    "\n",
    "for p in ax2.patches:\n",
    "    ax2.annotate('${:,.2f}'.format(p.get_height()), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', va='center', \n",
    "                 xytext=(0, 9), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Total Sales by Restaurant (Dual Axis)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Yearly Plots\n",
    "yearly_sales = final_df.groupby([final_df['date'].dt.year, 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "yearly_sales = yearly_sales.rename(columns={'date': 'year'})\n",
    "plot_individual_bars(yearly_sales, 'year', 'total_sales', 'Yearly Sales', 'Year', 'Total Sales')\n",
    "plot_individual_boxplots(yearly_sales, 'year', 'total_sales', 'Yearly Sales Distribution', 'Year', 'Yearly Total Sales')\n",
    "\n",
    "# 3. Quarterly Plots\n",
    "final_df['year_quarter'] = final_df['date'].dt.to_period('Q')\n",
    "quarterly_sales = final_df.groupby(['year_quarter', 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "quarterly_sales['year_quarter'] = quarterly_sales['year_quarter'].astype(str)\n",
    "plot_individual_boxplots(quarterly_sales, 'year_quarter', 'total_sales', 'Quarterly Sales Distribution', 'Year-Quarter', 'Quarterly Total Sales')\n",
    "\n",
    "# 4. Monthly Plots\n",
    "monthly_sales = final_df.groupby([final_df['date'].dt.month, 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "monthly_sales = monthly_sales.rename(columns={'date': 'month'})\n",
    "monthly_sales['month_name'] = monthly_sales['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "plot_individual_bars(monthly_sales, 'month_name', 'total_sales', 'Average Monthly Sales', 'Month', 'Average Sales')\n",
    "\n",
    "final_df['month'] = final_df['date'].dt.month\n",
    "monthly_sales_box = final_df.groupby(['month', 'restaurant_name', final_df['date'].dt.to_period('M')])['total_sales'].sum().reset_index()\n",
    "plot_individual_boxplots(monthly_sales_box, 'month', 'total_sales', 'Monthly Sales Distribution', 'Month', 'Monthly Total Sales', is_monthly=True)\n",
    "\n",
    "# 5. Weekly Plots\n",
    "# If you have weekly data, add the weekly plots here\n",
    "\n",
    "# 6. Days of the Week Plots\n",
    "final_df['day_name'] = final_df['date'].dt.day_name()\n",
    "day_name_sales = final_df.groupby(['day_name', 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "day_name_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_name_sales['day_name'] = pd.Categorical(day_name_sales['day_name'], categories=day_name_order, ordered=True)\n",
    "day_name_sales = day_name_sales.sort_values('day_name')\n",
    "plot_individual_bars(day_name_sales, 'day_name', 'total_sales', 'Average Sales by Day of Week', 'Day of Week', 'Average Sales')\n",
    "\n",
    "day_of_week_sales = final_df.groupby(['day_name', 'restaurant_name', 'date'])['total_sales'].sum().reset_index()\n",
    "day_of_week_sales['day_name'] = pd.Categorical(day_of_week_sales['day_name'], categories=day_name_order, ordered=True)\n",
    "day_of_week_sales = day_of_week_sales.sort_values('day_name')\n",
    "plot_individual_boxplots(day_of_week_sales, 'day_name', 'total_sales', 'Day of Week Sales Distribution', 'Day of Week', 'Total Sales')\n",
    "\n",
    "# 7. Daily Sales Plots\n",
    "daily_sales = final_df.groupby([final_df['date'].dt.day, 'restaurant_name'])['total_sales'].mean().reset_index()\n",
    "daily_sales = daily_sales.rename(columns={'date': 'day'})\n",
    "plot_individual_bars(daily_sales, 'day', 'total_sales', 'Average Daily Sales', 'Day of Month', 'Average Sales')\n",
    "\n",
    "daily_sales_box = final_df.groupby(['date', 'restaurant_name'])['total_sales'].sum().reset_index()\n",
    "plot_individual_boxplots(daily_sales_box, 'restaurant_name', 'total_sales', 'Daily Sales Distribution', 'Restaurant', 'Daily Total Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "**Data Preparation**\n",
    "```python\n",
    "final_df['total_sales'] = final_df['price'] * final_df['item_count']\n",
    "```\n",
    "This line calculates the total sales for each transaction. It's crucial for EDA as it creates a key metric that will be used throughout the analysis.\n",
    "\n",
    "**Aggregation Functions**\n",
    "```python\n",
    "restaurant_daily_sales = final_df.groupby(['date', 'restaurant_name']).agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index().rename(columns={'price': 'daily_sales'})\n",
    "```\n",
    "This function aggregates data to calculate daily sales for each restaurant. Aggregation is essential in EDA as it helps summarize large datasets into manageable and meaningful metrics.\n",
    "\n",
    "**Visualization Functions**\n",
    "```python\n",
    "def plot_individual_bars(data, x, y, title, x_label, y_label):\n",
    "    # Function body...\n",
    "\n",
    "def plot_individual_boxplots(data, x, y, title_prefix, x_label, y_label, is_monthly=False):\n",
    "    # Function body...\n",
    "```\n",
    "These functions create bar plots and box plots for different time frames. Visualization is a cornerstone of EDA, allowing analysts to quickly identify patterns, trends, and anomalies in the data.\n",
    "\n",
    "**Time-based Analysis**\n",
    "The code includes analysis at various time scales:\n",
    "- Yearly: `yearly_sales = final_df.groupby([final_df['date'].dt.year, 'restaurant_name'])['total_sales'].sum().reset_index()`\n",
    "- Quarterly: `final_df['year_quarter'] = final_df['date'].dt.to_period('Q')`\n",
    "- Monthly: `monthly_sales = final_df.groupby([final_df['date'].dt.month, 'restaurant_name'])['total_sales'].mean().reset_index()`\n",
    "- Daily: `daily_sales = final_df.groupby([final_df['date'].dt.day, 'restaurant_name'])['total_sales'].mean().reset_index()`\n",
    "\n",
    "This multi-scale temporal analysis is crucial in EDA for identifying seasonality, trends, and cyclical patterns in the data.\n",
    "\n",
    "**Comparative Analysis**\n",
    "```python\n",
    "bobs_data = restaurant_sales[restaurant_sales.index == \"Bob's Diner\"]\n",
    "other_data = restaurant_sales[restaurant_sales.index != \"Bob's Diner\"]\n",
    "```\n",
    "This code separates one restaurant from the others for comparison. Comparative analysis in EDA helps in benchmarking and identifying outliers or unique performers.\n",
    "\n",
    "**Statistical Distribution**\n",
    "The use of box plots (`plot_individual_boxplots`) provides insights into the statistical distribution of sales, including median, quartiles, and outliers. This is vital in EDA for understanding the spread and central tendency of the data.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "1. **Data Understanding**: This code helps in gaining a comprehensive understanding of the sales data across different time scales and restaurants.\n",
    "\n",
    "2. **Pattern Identification**: The various plots enable the identification of patterns such as seasonality, weekly trends, and restaurant-specific performance.\n",
    "\n",
    "3. **Anomaly Detection**: Box plots and comparative analyses help in identifying outliers and unusual patterns in the data.\n",
    "\n",
    "4. **Hypothesis Generation**: The visualizations can lead to hypothesis formation about factors affecting sales, which can be further investigated.\n",
    "\n",
    "5. **Feature Engineering Insights**: Understanding time-based patterns can inform feature engineering for predictive modeling.\n",
    "\n",
    "6. **Stakeholder Communication**: Clear visualizations are essential for communicating findings to non-technical stakeholders.\n",
    "\n",
    "7. **Data Quality Assessment**: The process of creating these visualizations can reveal data quality issues or missing data.\n",
    "\n",
    "8. **Decision Support**: The insights gained from this EDA can directly inform business decisions, such as staffing, inventory management, and marketing strategies.\n",
    "\n",
    "**Detailed Analysis and Inferences:**\n",
    "\n",
    "1. **Overall Performance**:\n",
    "   - Bobby's Grill's consistently taller bars (around $17,000-$18,000 daily average) indicate it likely has a prime location, excellent brand recognition, or a unique offering that attracts significantly more customers or allows for higher pricing compared to other restaurants.\n",
    "   - Corner Cafe's shorter bars ($3,000-$3,500 daily average) suggest it might be a smaller establishment, possibly focusing on a niche market like breakfast and lunch crowds, or it could be struggling with location or menu appeal.\n",
    "\n",
    "2. **Yearly Trends**:\n",
    "   - The increase in bar heights from 2021 to 2022 across most restaurants suggests a general recovery or growth in the restaurant industry, possibly post-pandemic.\n",
    "   - The stagnation or slight decline in 2023 for Corner Cafe and Tasty Bites, shown by similar or shorter bars, might indicate market saturation, increased competition, or these specific restaurants reaching their capacity limits.\n",
    "\n",
    "3. **Seasonal Patterns**:\n",
    "   - Taller bars in summer months (June-August) across all restaurants imply a strong influence of tourism or seasonal activities on dining out. This could be due to better weather, vacations, or local events driving more foot traffic.\n",
    "   - Shorter bars in winter months (December-February) suggest reduced outdoor dining, fewer tourists, or changed consumer behavior (e.g., preference for home cooking in colder months).\n",
    "\n",
    "4. **Day of Week Trends**:\n",
    "   - Larger boxes and higher medians for Friday and Saturday across all restaurants indicate a strong weekend dining culture. This could be due to more leisure time, social dining habits, or targeted weekend promotions.\n",
    "   - Smaller and lower boxes for Sunday and Monday suggest these are typically slower days, possibly due to many people preparing for the work week or recovering from weekend activities.\n",
    "\n",
    "5. **Restaurant-Specific Observations**:\n",
    "   - Bobby's Grill: Largest interquartile ranges and highest median values in box plots suggest not only high sales but also high variability, indicating potential for both very busy and slower periods. This could be due to its popularity leading to rush times and quieter off-peak hours.\n",
    "   - Tasty Bites: Consistent median values across weekdays with a slight weekend increase suggest a stable customer base with a modest weekend boost. This could indicate a strong lunch business with some additional dinner traffic on weekends.\n",
    "   - Corner Cafe: Smallest interquartile ranges and consistent medians across all days imply a very steady, predictable business. This could be a popular local spot with regular clientele, or it might have limited seating causing consistent but capped sales.\n",
    "   - Soup's Up: Larger whiskers and more outliers in its box plots indicate higher day-to-day variability. This could suggest more susceptibility to factors like weather (for a soup-focused restaurant) or irregular events driving occasionally high sales.\n",
    "   - Beach Shack: Pronounced peak in summer and significant dip in winter in monthly sales charts suggest heavy reliance on seasonal tourism or outdoor dining. This implies a need for strong financial management to handle off-season periods.\n",
    "   - Fire Grill: Consistent increase in bar heights year-over-year indicates successful growth strategies or increasing popularity. This suggests potential for expansion or for applying its successful practices to other restaurants in the group.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Market Leader:** Bobby's Grill is clearly the top performer and likely has the strongest brand or most favorable location.\n",
    "   \n",
    "2. **Seasonal Dependency:** All restaurants are affected by seasonal trends, with summer being the peak season.\n",
    "   \n",
    "3. **Weekend Boost:** The significantly higher sales on Fridays and Saturdays indicate a strong reliance on weekend dining across all restaurants.\n",
    "   \n",
    "4. **Growth Potential:** While most restaurants showed growth from 2021 to 2022, the slowing or declining growth in 2023 for some restaurants suggests potential market saturation or external factors affecting performance.\n",
    "   \n",
    "5. **Consistency vs. Variability:** Some restaurants (e.g., Corner Cafe) show more consistent sales across the week, while others (e.g., Soup's Up) have higher variability, suggesting different target markets or dining concepts.\n",
    "\n",
    "**Comparative Inferences:**\n",
    "\n",
    "1. **Market Positioning**: The significant gap between Bobby's Grill's performance and others suggests it might be in a different market segment, possibly a higher-end dining experience or a extremely popular casual dining spot.\n",
    "\n",
    "2. **Operational Efficiency**: Corner Cafe's consistent but lower sales might indicate efficient operations scaled for smaller volume, while Bobby's Grill's high variability could suggest challenges in staffing and inventory management for peak times.\n",
    "\n",
    "3. **Target Demographics**: The varying patterns in weekday vs. weekend performance across restaurants imply different target demographics. Tasty Bites might cater more to a working lunch crowd, while Beach Shack seems more focused on leisure and tourist dining.\n",
    "\n",
    "4. **Scalability and Growth Potential**: Fire Grill's consistent growth suggests it might have the most scalable business model or room for expansion. In contrast, Corner Cafe's steady sales might indicate it has reached its optimal operational size for its current model.\n",
    "\n",
    "5. **Risk and Seasonality**: Beach Shack's high seasonal variability indicates higher risk and more complex financial management needs compared to more consistent performers like Corner Cafe or Tasty Bites.\n",
    "\n",
    "6. **Adaptability to Market Changes**: Soup's Up's high variability suggests it might be more adaptable to changing conditions (like introducing new menu items or responding to trends) but also more vulnerable to external factors.\n",
    "\n",
    "These detailed inferences provide a nuanced understanding of each restaurant's performance, market position, and potential strategies for improvement or growth. They highlight the diverse challenges and opportunities within the restaurant group, suggesting areas for focused attention and potential cross-pollination of successful strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.6. Identify the most popular items overall and the stores where they are being sold. Also, find out the most popular item at each store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total sales and quantity sold for each item, including the restaurant name\n",
    "item_sales = final_df.groupby(['item_id', 'item_name', 'restaurant_name']).agg({\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum(),\n",
    "    'item_count': 'sum'\n",
    "}).reset_index().rename(columns={'price': 'total_sales', 'item_count': 'quantity_sold'})\n",
    "\n",
    "# Sort items by total sales and get top 10\n",
    "top_10_items_sales = item_sales.sort_values('total_sales', ascending=False).head(10)\n",
    "\n",
    "# 1. Plot top 10 items by total sales\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='item_name', y='total_sales', data=top_10_items_sales, palette='viridis')\n",
    "plt.title('Top 10 Items by Total Sales', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Item', fontsize=12)\n",
    "plt.ylabel('Total Sales ($)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for i, v in enumerate(top_10_items_sales['total_sales']):\n",
    "    ax.text(i, v, f'${v:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Create a pretty table for top 10 items by total sales\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Rank\", \"Item Name\", \"Restaurant\", \"Total Sales\", \"Quantity Sold\"]\n",
    "for rank, (_, row) in enumerate(top_10_items_sales.iterrows(), 1):\n",
    "    table.add_row([\n",
    "        rank, \n",
    "        row['item_name'], \n",
    "        row['restaurant_name'],\n",
    "        f\"${row['total_sales']:,.2f}\", \n",
    "        f\"{row['quantity_sold']:,}\"\n",
    "    ])\n",
    "\n",
    "table.align[\"Item Name\"] = \"l\"\n",
    "table.align[\"Restaurant\"] = \"l\"\n",
    "table.align[\"Total Sales\"] = \"r\"\n",
    "table.align[\"Quantity Sold\"] = \"r\"\n",
    "\n",
    "print(\"\\nTop 10 Items by Total Sales:\")\n",
    "print(table)\n",
    "\n",
    "# New table: Top 10 Items by Total Sales (Excluding Bob's Diner)\n",
    "top_10_items_sales_no_bob = item_sales[item_sales['restaurant_name'] != \"Bob's Diner\"].sort_values('total_sales', ascending=False).head(10)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Rank\", \"Item Name\", \"Restaurant\", \"Total Sales\", \"Quantity Sold\"]\n",
    "for rank, (_, row) in enumerate(top_10_items_sales_no_bob.iterrows(), 1):\n",
    "    table.add_row([\n",
    "        rank, \n",
    "        row['item_name'], \n",
    "        row['restaurant_name'],\n",
    "        f\"${row['total_sales']:,.2f}\", \n",
    "        f\"{row['quantity_sold']:,}\"\n",
    "    ])\n",
    "\n",
    "table.align[\"Item Name\"] = \"l\"\n",
    "table.align[\"Restaurant\"] = \"l\"\n",
    "table.align[\"Total Sales\"] = \"r\"\n",
    "table.align[\"Quantity Sold\"] = \"r\"\n",
    "\n",
    "print(\"\\nTop 10 Items by Total Sales (Excluding Bob's Diner):\")\n",
    "print(table)\n",
    "\n",
    "# 3. Plot top 10 items by quantity sold\n",
    "top_10_items_quantity = item_sales.sort_values('quantity_sold', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='item_name', y='quantity_sold', data=top_10_items_quantity, palette='magma')\n",
    "plt.title('Top 10 Items by Quantity Sold', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Item', fontsize=12)\n",
    "plt.ylabel('Quantity Sold', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for i, v in enumerate(top_10_items_quantity['quantity_sold']):\n",
    "    ax.text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Create a pretty table for top 10 items by quantity sold\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Rank\", \"Item Name\", \"Restaurant\", \"Quantity Sold\", \"Total Sales\"]\n",
    "for rank, (_, row) in enumerate(top_10_items_quantity.iterrows(), 1):\n",
    "    table.add_row([\n",
    "        rank, \n",
    "        row['item_name'], \n",
    "        row['restaurant_name'],\n",
    "        f\"{row['quantity_sold']:,}\",\n",
    "        f\"${row['total_sales']:,.2f}\"\n",
    "    ])\n",
    "\n",
    "table.align[\"Item Name\"] = \"l\"\n",
    "table.align[\"Restaurant\"] = \"l\"\n",
    "table.align[\"Quantity Sold\"] = \"r\"\n",
    "table.align[\"Total Sales\"] = \"r\"\n",
    "\n",
    "print(\"\\nTop 10 Items by Quantity Sold:\")\n",
    "print(table)\n",
    "\n",
    "# New table: Top 10 Items by Quantity Sold (Excluding Bob's Diner)\n",
    "top_10_items_quantity_no_bob = item_sales[item_sales['restaurant_name'] != \"Bob's Diner\"].sort_values('quantity_sold', ascending=False).head(10)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Rank\", \"Item Name\", \"Restaurant\", \"Quantity Sold\", \"Total Sales\"]\n",
    "for rank, (_, row) in enumerate(top_10_items_quantity_no_bob.iterrows(), 1):\n",
    "    table.add_row([\n",
    "        rank, \n",
    "        row['item_name'], \n",
    "        row['restaurant_name'],\n",
    "        f\"{row['quantity_sold']:,}\",\n",
    "        f\"${row['total_sales']:,.2f}\"\n",
    "    ])\n",
    "\n",
    "table.align[\"Item Name\"] = \"l\"\n",
    "table.align[\"Restaurant\"] = \"l\"\n",
    "table.align[\"Quantity Sold\"] = \"r\"\n",
    "table.align[\"Total Sales\"] = \"r\"\n",
    "\n",
    "print(\"\\nTop 10 Items by Quantity Sold (Excluding Bob's Diner):\")\n",
    "print(table)\n",
    "\n",
    "# 5. Identify most popular item at each store\n",
    "store_popular_items = final_df.groupby(['store_id', 'restaurant_name', 'item_id', 'item_name']).agg({\n",
    "    'item_count': 'sum',\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index()\n",
    "\n",
    "store_popular_items = store_popular_items.loc[store_popular_items.groupby('store_id')['item_count'].idxmax()]\n",
    "\n",
    "# Create a pretty table for most popular items at each store\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Restaurant Name\", \"Most Popular Item\", \"Quantity Sold\", \"Total Sales\"]\n",
    "for _, row in store_popular_items.iterrows():\n",
    "    table.add_row([\n",
    "        row['restaurant_name'], \n",
    "        row['item_name'], \n",
    "        f\"{row['item_count']:,}\",\n",
    "        f\"${row['price']:,.2f}\"\n",
    "    ])\n",
    "\n",
    "table.align[\"Restaurant Name\"] = \"l\"\n",
    "table.align[\"Most Popular Item\"] = \"l\"\n",
    "table.align[\"Quantity Sold\"] = \"r\"\n",
    "table.align[\"Total Sales\"] = \"r\"\n",
    "\n",
    "print(\"\\nMost popular item at each store:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "1. **Bob's Diner Dominance:**\n",
    "   - Bob's Diner significantly outperforms other restaurants in both total sales and quantity sold.\n",
    "   - All top 10 items by total sales and quantity sold are from Bob's Diner when including all restaurants.\n",
    "\n",
    "2. **Menu Item Performance:**\n",
    "   - \"Amazing pork lunch\" is the highest-grossing item overall, generating $1,094,501 in sales.\n",
    "   - \"Strawberry Smoothy\" is the best-selling item by quantity, with 236,337 units sold.\n",
    "   - Smoothies and lunch items appear to be particularly popular at Bob's Diner.\n",
    "\n",
    "3. **Price Point Differences:**\n",
    "   - Some items have high sales despite lower quantities (e.g., \"Blue Ribbon Beef Entree\"), suggesting higher price points.\n",
    "   - Others have high quantities but lower total sales (e.g., \"Orange Juice\"), indicating lower price points.\n",
    "\n",
    "4. **Non-Bob's Diner Performance:**\n",
    "   - When excluding Bob's Diner, \"Blue Ribbon Fruity Vegi Lunch\" from Fou Cher is the top-selling item by revenue ($16,086.04).\n",
    "   - \"Awesome Smoothy\" from Sweet Shack leads in quantity sold (1,692 units) when Bob's Diner is excluded.\n",
    "\n",
    "5. **Restaurant Specialties:**\n",
    "   - Different restaurants seem to have signature items:\n",
    "     - Sweet Shack and Beachfront Bar excel in smoothies\n",
    "     - Fou Cher's top item is a vegetarian option\n",
    "     - Surfs Up has multiple popular seafood items (e.g., \"Oysters Rockefeller\")\n",
    "\n",
    "6. **Menu Diversity:**\n",
    "   - The top-selling items represent a mix of main courses, drinks, and desserts, suggesting diverse menu offerings.\n",
    "\n",
    "7. **Price Range Inference:**\n",
    "   - Based on the sales and quantity data, we can infer that Bob's Diner likely offers items at various price points, from affordable high-volume items to pricier entrees.\n",
    "\n",
    "8. **Customer Preferences:**\n",
    "   - Across different restaurants, smoothies and lunch items appear frequently in the top-selling lists, indicating a general customer preference for these types of items.\n",
    "\n",
    "9. **Marketing Opportunities:**\n",
    "   - The data suggests opportunities for cross-promotion or menu optimization at lower-performing restaurants, potentially by introducing popular item types from Bob's Diner.\n",
    "\n",
    "10. **Operational Insights:**\n",
    "    - Bob's Diner's success might indicate superior location, marketing, or operational efficiency, which could be studied and potentially applied to other restaurants in the group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.7. Determine if the store with the highest sales volume is also making the most money per day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily sales volume and revenue for each store\n",
    "daily_store_performance = final_df.groupby(['date', 'store_id', 'restaurant_name']).agg({\n",
    "    'item_count': 'sum',\n",
    "    'price': lambda x: (x * final_df.loc[x.index, 'item_count']).sum()\n",
    "}).reset_index().rename(columns={'item_count': 'daily_volume', 'price': 'daily_revenue'})\n",
    "\n",
    "# Calculate average daily volume and revenue\n",
    "avg_store_performance = daily_store_performance.groupby('restaurant_name').agg({\n",
    "    'daily_volume': 'mean',\n",
    "    'daily_revenue': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot relationship between average daily volume and revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='daily_volume', y='daily_revenue', data=avg_store_performance, s=100)\n",
    "\n",
    "for i, row in avg_store_performance.iterrows():\n",
    "    plt.annotate(row['restaurant_name'], (row['daily_volume'], row['daily_revenue']))\n",
    "\n",
    "plt.title('Average Daily Sales Volume vs Revenue by Restaurant')\n",
    "plt.xlabel('Average Daily Sales Volume')\n",
    "plt.ylabel('Average Daily Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between daily volume and revenue\n",
    "correlation = daily_store_performance['daily_volume'].corr(daily_store_performance['daily_revenue'])\n",
    "print(f\"Correlation between daily sales volume and revenue: {correlation:.2f}\")\n",
    "\n",
    "# Identify store with highest sales volume and its revenue ranking\n",
    "highest_volume_store = avg_store_performance.loc[avg_store_performance['daily_volume'].idxmax()]\n",
    "revenue_rank = avg_store_performance['daily_revenue'].rank(ascending=False)\n",
    "highest_volume_store_rank = revenue_rank[highest_volume_store.name]\n",
    "\n",
    "print(f\"Store with highest sales volume: {highest_volume_store['restaurant_name']}\")\n",
    "print(f\"Its revenue rank: {highest_volume_store_rank} out of {len(avg_store_performance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.8. Identify the most expensive item at each restaurant and find out its calorie count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most expensive item at each restaurant\n",
    "most_expensive_items = final_df.groupby(['store_id', 'restaurant_name', 'item_id', 'item_name', 'price', 'kcal']).size().reset_index(name='count')\n",
    "most_expensive_items = most_expensive_items.loc[most_expensive_items.groupby('store_id')['price'].idxmax()]\n",
    "\n",
    "# Sort by price in descending order\n",
    "most_expensive_items = most_expensive_items.sort_values('price', ascending=False)\n",
    "\n",
    "# Display the most expensive items and their calorie counts\n",
    "print(\"Most expensive item at each restaurant and its calorie count:\")\n",
    "print(most_expensive_items[['restaurant_name', 'item_name', 'price', 'kcal']])\n",
    "\n",
    "# Visualize price vs calorie count for the most expensive items\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='kcal', y='price', data=most_expensive_items, s=100)\n",
    "\n",
    "for i, row in most_expensive_items.iterrows():\n",
    "    plt.annotate(row['restaurant_name'], (row['kcal'], row['price']))\n",
    "\n",
    "plt.title('Price vs Calorie Count for Most Expensive Items')\n",
    "plt.xlabel('Calories')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between price and calorie count for all items\n",
    "price_calorie_correlation = final_df[['price', 'kcal']].drop_duplicates().corr().iloc[0, 1]\n",
    "print(f\"Correlation between price and calorie count for all items: {price_calorie_correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Forecasting using machine learning algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.1. Build and compare linear regression, random forest, and XGBoost models for predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final_df DataFrame Information:\")\n",
    "print(final_df.info())\n",
    "print()\n",
    "print(\"final_df DataFrame Description:\")\n",
    "print(final_df.describe().transpose())\n",
    "print()\n",
    "\n",
    "# Count the number of entries for each restaurant_name\n",
    "restaurant_counts = final_df['restaurant_name'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Restaurant Sales Transaction Counts From Combined Dataset: \\n\")\n",
    "print(restaurant_counts)\n",
    "print()\n",
    "\n",
    "restaurant_items = final_df.groupby('restaurant_name')['item_name'].apply(set)\n",
    "\n",
    "# Find the common item_names across all restaurants\n",
    "common_items = set.intersection(*restaurant_items)\n",
    "\n",
    "# Check if there are common items and print the appropriate message\n",
    "if common_items:\n",
    "    print(\"Common item_names between restaurants:\")\n",
    "    print(sorted(common_items))\n",
    "else:\n",
    "    print(f\"No shared items between restaurants.\\n\")\n",
    "\n",
    "# Group the DataFrame by restaurant_name and get unique item_names for each restaurant\n",
    "restaurant_items = final_df.groupby('restaurant_name')['item_name'].apply(set)\n",
    "\n",
    "# Print the unique item_names for each restaurant in alphabetical order\n",
    "for restaurant, items in restaurant_items.items():\n",
    "    sorted_items = sorted(items)\n",
    "    print(f\"Restaurant: {restaurant}\")\n",
    "    print(f\"Unique Items: {sorted_items}\")\n",
    "    print()\n",
    "    \n",
    "# Group the DataFrame by restaurant_name and item_name, and aggregate the prices\n",
    "restaurant_item_prices = final_df.groupby(['restaurant_name', 'item_name'])['price'].mean().reset_index()\n",
    "\n",
    "# Iterate over each restaurant and print the table of unique items and their prices\n",
    "for restaurant, group in restaurant_item_prices.groupby('restaurant_name'):\n",
    "    print(f\"Restaurant: {restaurant}\")\n",
    "    print(group[['item_name', 'price']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "# Group the DataFrame by item_name and calculate the standard deviation of prices for each item\n",
    "price_variability = final_df.groupby('item_name')['price'].std().reset_index()\n",
    "\n",
    "# Filter items with non-zero standard deviation\n",
    "variable_price_items = price_variability[price_variability['price'] > 0]\n",
    "\n",
    "# Check if there are any items with price variability and print the appropriate message\n",
    "if not variable_price_items.empty:\n",
    "    print(\"Items with price variability:\")\n",
    "    for _, row in variable_price_items.iterrows():\n",
    "        item_name = row['item_name']\n",
    "        std_dev = row['price']\n",
    "        restaurants_selling_item = final_df[final_df['item_name'] == item_name]['restaurant_name'].unique()\n",
    "        print(f\"Item: {item_name}, Std Dev: {std_dev:.2f}, Restaurants: {', '.join(restaurants_selling_item)}\")\n",
    "else:\n",
    "    print(\"No price variability found for any items.\")\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final_df to a CSV file\n",
    "# final_df.to_csv(f'{DATASET_PATH}/final_df_export.csv', index=False)\n",
    "\n",
    "# Extract additional date features\n",
    "final_df['day_of_week'] = final_df['date'].dt.dayofweek\n",
    "final_df['week_of_year'] = final_df['date'].dt.isocalendar().week\n",
    "final_df['quarter'] = final_df['date'].dt.quarter\n",
    "final_df['month'] = final_df['date'].dt.month\n",
    "final_df['year'] = final_df['date'].dt.year\n",
    "final_df['day_of_month'] = final_df['date'].dt.day\n",
    "\n",
    "# Handling outliers by capping them\n",
    "def cap_outliers(df, column):\n",
    "    upper_limit = df[column].quantile(0.95)\n",
    "    lower_limit = df[column].quantile(0.05)\n",
    "    df[column] = np.clip(df[column], lower_limit, upper_limit)\n",
    "    return df\n",
    "\n",
    "for column in ['price', 'item_count', 'total_sales', 'kcal']:\n",
    "    final_df = cap_outliers(final_df, column)\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "num_features = ['price', 'item_count', 'kcal', 'total_sales']\n",
    "date_features = ['day_of_week', 'week_of_year', 'quarter', 'month', 'year', 'day_of_month']\n",
    "cat_features = ['restaurant_name', 'item_name', 'day_name'] + date_features\n",
    "\n",
    "# Custom transformer for log transformation\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log', log_transformer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features)\n",
    "    ])\n",
    "\n",
    "# Apply the transformations\n",
    "df_preprocessed = preprocessor.fit_transform(final_df)\n",
    "\n",
    "# Check the shape of the preprocessed data\n",
    "print(f\"Shape of preprocessed data: {df_preprocessed.shape}\")\n",
    "\n",
    "# Generate column names\n",
    "num_columns = num_features\n",
    "cat_columns = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(cat_features)\n",
    "all_columns = num_columns + list(cat_columns)\n",
    "\n",
    "# Check the length of the columns\n",
    "print(f\"Number of columns: {len(all_columns)}\")\n",
    "\n",
    "# Ensure the shape matches the number of columns\n",
    "if df_preprocessed.shape[1] == len(all_columns):\n",
    "    df_preprocessed = pd.DataFrame(df_preprocessed, columns=all_columns)\n",
    "else:\n",
    "    print(\"Mismatch between the number of columns in preprocessed data and column names\")\n",
    "\n",
    "# Add the date column back to the preprocessed DataFrame\n",
    "df_preprocessed['date'] = final_df['date']\n",
    "\n",
    "# Display the preprocessed DataFrame if there is no mismatch\n",
    "if df_preprocessed.shape[1] == len(all_columns) + 1:\n",
    "    print(df_preprocessed.head())\n",
    "else:\n",
    "    print(\"Preprocessed data and column names do not match. Please check the transformations.\")\n",
    "\n",
    "# Export final_df to a CSV file\n",
    "# df_preprocessed.to_csv(f'{DATASET_PATH}/df_preprocessed_export.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.2. Use the Sales data from the last six months as the testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data to daily level\n",
    "daily_sales = df_preprocessed.groupby('date')['total_sales'].sum().reset_index()\n",
    "\n",
    "# Create lag features\n",
    "for i in range(1, 8):  # Create 7 lag features\n",
    "    daily_sales[f'lag_{i}'] = daily_sales['total_sales'].shift(i)\n",
    "\n",
    "# Drop rows with NaN values after creating lag features\n",
    "daily_sales = daily_sales.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X = daily_sales[['date'] + [f'lag_{i}' for i in range(1, 8)]]\n",
    "y = daily_sales['total_sales']\n",
    "\n",
    "# Extract additional date features for daily_sales\n",
    "X['day_of_week'] = X['date'].dt.dayofweek\n",
    "X['week_of_year'] = X['date'].dt.isocalendar().week\n",
    "X['quarter'] = X['date'].dt.quarter\n",
    "X['month'] = X['date'].dt.month\n",
    "X['year'] = X['date'].dt.year\n",
    "X['day_of_month'] = X['date'].dt.day\n",
    "\n",
    "# Drop the date column as it's no longer needed\n",
    "X = X.drop(columns=['date'])\n",
    "\n",
    "# Split the data - use last 6 months as test set\n",
    "split_date = daily_sales['date'].max() - timedelta(days=180)\n",
    "X_train = X[daily_sales['date'] <= split_date]\n",
    "X_test = X[daily_sales['date'] > split_date]\n",
    "y_train = y[daily_sales['date'] <= split_date]\n",
    "y_test = y[daily_sales['date'] > split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances in predicting sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    # Train on the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'model': model, 'cv_rmse': cv_rmse, 'test_rmse': rmse, 'r2': r2}\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse}\")\n",
    "    print(f\"  Cross-validation RMSE: {cv_rmse}\")\n",
    "    print(f\"  Test RMSE: {rmse}\")\n",
    "    print(f\"  R2 Score: {r2}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: This metric measures the average of the squares of the errorsthat is, the average squared difference between the estimated values and the actual value.\n",
    "2. **Root Mean Squared Error (RMSE)**: This is the square root of the mean squared error. It is more interpretable as it is in the same units as the original data.\n",
    "3. **R2 Score (Coefficient of Determination)**: This indicates how well data points fit a statistical model  an R2 score closer to 1 implies a better fit.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "The evaluation of different machine learning models and their respective performance metrics is crucial for several reasons, especially in a business or research context where data-driven decisions can have significant impacts. Here's why this process is important:\n",
    "\n",
    "1. **Accuracy and Reliability of Predictions**\n",
    "\n",
    "- **Business Decisions**: Accurate predictions are essential for making informed business decisions. For instance, in sales forecasting, precise predictions can help in inventory management, staffing, and financial planning.\n",
    "- **Minimizing Errors**: By evaluating models based on metrics like MSE, RMSE, and R2 Score, you can minimize prediction errors, which is critical in applications where errors can be costly or dangerous (e.g., healthcare, finance).\n",
    "\n",
    "2. **Understanding Data Relationships**\n",
    "\n",
    "- **Model Insights**: Different models can provide different insights into how features relate to the target variable. For example, Random Forest can highlight the importance of different features, helping you understand which factors drive sales the most.\n",
    "- **Feature Engineering**: Evaluating model performance helps in refining feature engineering efforts. If a model performs poorly, it might indicate the need for additional or different features.\n",
    "\n",
    "3. **Choosing the Right Model**\n",
    "\n",
    "- **Model Performance**: Different models perform differently based on the nature of the data. For example, Random Forest might handle complex, non-linear relationships better than Linear Regression.\n",
    "- **Context-Specific Needs**: Some models might be more interpretable but less accurate, while others might be more accurate but less interpretable. Depending on the context, you might prioritize one over the other.\n",
    "\n",
    "4. **Risk Management**\n",
    "\n",
    "- **Overfitting and Underfitting**: By comparing models, you can identify overfitting or underfitting issues. Overfitting occurs when a model performs well on training data but poorly on test data, while underfitting occurs when a model is too simple to capture the underlying patterns.\n",
    "- **Robustness**: Ensuring the selected model is robust and generalizes well to unseen data reduces the risk of unexpected performance drops when deployed in real-world scenarios.\n",
    "\n",
    "5. **Resource Allocation**\n",
    "\n",
    "- **Efficient Use of Resources**: Developing, tuning, and deploying models can be resource-intensive. By thoroughly evaluating models, you ensure that resources are allocated to the most promising models, optimizing the return on investment.\n",
    "- **Automation**: Accurate models can automate decision-making processes, freeing up human resources for more strategic tasks.\n",
    "\n",
    "6. **Continuous Improvement**\n",
    "\n",
    "- **Iterative Refinement**: Model evaluation is part of an iterative process where models are continuously improved based on performance feedback. This iterative process leads to better and more refined models over time.\n",
    "- **Benchmarking**: Evaluating models against each other sets benchmarks for future model improvements. It helps in establishing a performance baseline against which new models can be compared.\n",
    "\n",
    "7. **Real-World Implications**\n",
    "\n",
    "- **Customer Satisfaction**: In customer-facing applications, accurate predictions can enhance user experiences and satisfaction. For example, personalized recommendations based on accurate models can increase user engagement and sales.\n",
    "- **Operational Efficiency**: Accurate models can improve operational efficiency by optimizing processes such as supply chain management, predictive maintenance, and demand forecasting.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The process of evaluating and selecting the right model is crucial because it ensures that the predictions made by the model are accurate, reliable, and actionable. This leads to better decision-making, optimized operations, risk management, efficient resource use, and continuous improvement, all of which are vital for the success of any data-driven initiative.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - MSE: 12.74\n",
    "   - Cross-validation RMSE: 3.23\n",
    "   - Test RMSE: 3.57\n",
    "   - R2 Score: 0.40\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - MSE: 10.82\n",
    "   - Cross-validation RMSE: 3.25\n",
    "   - Test RMSE: 3.29\n",
    "   - R2 Score: 0.49\n",
    "\n",
    "3. **XGBoost**:\n",
    "   - MSE: 11.80\n",
    "   - Cross-validation RMSE: 3.45\n",
    "   - Test RMSE: 3.44\n",
    "   - R2 Score: 0.44\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Model Performance**: \n",
    "   - Random Forest performed the best among the three models with the lowest MSE and RMSE, and the highest R2 Score.\n",
    "   - Linear Regression showed moderate performance with the highest test RMSE and the lowest R2 Score.\n",
    "   - XGBoost performed better than Linear Regression but was outperformed by Random Forest.\n",
    "\n",
    "2. **Overfitting and Underfitting**:\n",
    "   - Random Forest, with its ensemble method, tends to handle overfitting better and captures the variability in the data more effectively.\n",
    "   - Linear Regression, being a simple model, might not capture complex patterns in the data, leading to lower performance.\n",
    "   - XGBoost, while powerful, might require more hyperparameter tuning to reach its full potential.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - Random Forest's superior performance suggests that the relationship between the features and the target variable is non-linear and complex.\n",
    "   - Linear Regression's poorer performance indicates that a simple linear relationship is insufficient to capture the patterns in the data.\n",
    "\n",
    "4. **Model Robustness**:\n",
    "   - Random Forest appears more robust and less sensitive to overfitting compared to the other models.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Based on the metrics, Random Forest is the recommended model due to its better performance in terms of MSE, RMSE, and R2 Score.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Further tuning of the Random Forest and XGBoost models might yield better results. Techniques like Grid Search or Random Search should be considered to optimize their performance.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Explore additional feature engineering techniques to enhance model performance. For example, interactions between features, polynomial features, or more sophisticated temporal features.\n",
    "   - Investigate feature importance from the Random Forest model to understand which features contribute most to the predictions.\n",
    "\n",
    "4. **Cross-validation Strategy**:\n",
    "   - Use more robust cross-validation strategies, such as time-series split, to ensure the model's performance generalizes well to unseen data.\n",
    "\n",
    "5. **Ensemble Methods**:\n",
    "   - Consider using ensemble methods combining the strengths of different models. Stacking or blending techniques could improve predictive performance.\n",
    "\n",
    "6. **Model Monitoring**:\n",
    "   - Continuously monitor the model's performance on new data to ensure it remains effective over time. Implement a retraining strategy if model performance degrades.\n",
    "   \n",
    "7. **Deployment**:\n",
    "   - Deploy the model into a production environment where it can be used to make real-time predictions and integrate it with decision-making processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.4. Use the best-performing models to make a sales forecast for the next year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on test RMSE\n",
    "best_model_name = min(results, key=lambda x: results[x]['test_rmse'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Forecast for the next year\n",
    "last_date = daily_sales['date'].max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=365)\n",
    "future_df = pd.DataFrame({'date': future_dates})\n",
    "future_df['day_of_week'] = future_df['date'].dt.dayofweek\n",
    "future_df['week_of_year'] = future_df['date'].dt.isocalendar().week\n",
    "future_df['quarter'] = future_df['date'].dt.quarter\n",
    "future_df['month'] = future_df['date'].dt.month\n",
    "future_df['year'] = future_df['date'].dt.year\n",
    "future_df['day_of_month'] = future_df['date'].dt.day\n",
    "\n",
    "# Create lag features for future data\n",
    "for i in range(1, 8):\n",
    "    future_df[f'lag_{i}'] = daily_sales['total_sales'].iloc[-i]\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = best_model.predict(future_df[X.columns])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales['date'], daily_sales['total_sales'], label='Historical Data')\n",
    "plt.plot(future_dates, future_predictions, label='Forecast', color='red')\n",
    "plt.title('Sales Forecast for Next Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the forecast for the next year\n",
    "forecast_df = pd.DataFrame({'date': future_dates, 'forecast': future_predictions})\n",
    "print(\"\\nForecast for the next year:\")\n",
    "print(forecast_df)\n",
    "\n",
    "# Feature importance for Random Forest and XGBoost\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'pink']\n",
    "    plt.bar(feature_importance['feature'], feature_importance['importance'], color=colors)\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. **Sales Forecast Plot**:\n",
    "   - The plot shows historical sales data (in cyan) and the forecasted sales data (in red) for the next year.\n",
    "   - The historical data includes fluctuations over time, reflecting seasonality and possible trends.\n",
    "\n",
    "2. **Forecast Data**:\n",
    "   - The forecast data is shown for each day of the next year, indicating the predicted sales totals.\n",
    "\n",
    "3. **Feature Importance Plot**:\n",
    "   - This bar plot displays the importance of each feature used in the Random Forest model. Higher bars indicate more important features for the model.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "1. **Inventory Management**:\n",
    "   - **Explanation**: Accurate sales forecasts help in managing inventory levels effectively.\n",
    "   - **Observation**: Overstocking or understocking can be avoided, leading to optimized storage costs and reduced wastage.\n",
    "   - **Conclusion**: Proper inventory management ensures products are available when needed, improving customer satisfaction.\n",
    "   - **Inference**: Predictable sales allow for better planning and procurement strategies.\n",
    "   - **Recommendation**: Use the forecast to set reorder points and safety stock levels.\n",
    "\n",
    "2. **Financial Planning**:\n",
    "   - **Explanation**: Sales forecasts are critical for financial planning and budgeting.\n",
    "   - **Observation**: Predicting revenue helps in planning for expenses, investments, and cash flow management.\n",
    "   - **Conclusion**: Reliable forecasts lead to better financial health and the ability to make informed investment decisions.\n",
    "   - **Inference**: Accurate revenue projections reduce financial risks.\n",
    "   - **Recommendation**: Integrate sales forecasts into financial models to plan for growth and expansion.\n",
    "\n",
    "3. **Staffing and Resource Allocation**:\n",
    "   - **Explanation**: Anticipating sales helps in staffing appropriately to meet demand.\n",
    "   - **Observation**: Ensuring the right number of employees are scheduled can improve service levels and reduce labor costs.\n",
    "   - **Conclusion**: Efficient resource allocation enhances operational efficiency.\n",
    "   - **Inference**: Overstaffing or understaffing can be minimized.\n",
    "   - **Recommendation**: Use forecasts to plan shifts, hire temporary staff, or manage training schedules.\n",
    "\n",
    "4. **Marketing and Promotions**:\n",
    "   - **Explanation**: Forecasts inform marketing strategies and promotional activities.\n",
    "   - **Observation**: Identifying periods of high or low sales can guide when to run promotions or marketing campaigns.\n",
    "   - **Conclusion**: Targeted marketing efforts increase ROI and drive sales.\n",
    "   - **Inference**: Data-driven marketing is more effective.\n",
    "   - **Recommendation**: Align marketing budgets and campaigns with forecasted sales trends.\n",
    "\n",
    "5. **Customer Satisfaction**:\n",
    "   - **Explanation**: Meeting customer demand consistently improves satisfaction and loyalty.\n",
    "   - **Observation**: Accurate forecasts ensure product availability, reducing the likelihood of stockouts.\n",
    "   - **Conclusion**: Happy customers are more likely to return and recommend the business.\n",
    "   - **Inference**: Sales forecasts are directly linked to customer experience.\n",
    "   - **Recommendation**: Use forecasts to maintain service levels and product availability.\n",
    "\n",
    "6. **Operational Efficiency**:\n",
    "   - **Explanation**: Forecasting helps streamline operations and reduce inefficiencies.\n",
    "   - **Observation**: Businesses can plan production schedules, logistics, and supply chain activities more effectively.\n",
    "   - **Conclusion**: Improved operational efficiency leads to cost savings and better margins.\n",
    "   - **Inference**: Predictable operations reduce the chances of last-minute adjustments and disruptions.\n",
    "   - **Recommendation**: Incorporate forecasts into operational planning and scheduling tools.\n",
    "\n",
    "Accurate sales forecasting using advanced models like Random Forest provides a competitive edge by enabling better planning, resource allocation, and strategic decision-making. The insights gained from forecasting allow businesses to optimize operations, improve financial health, enhance customer satisfaction, and drive growth. By understanding and leveraging the importance of sales forecasting, businesses can navigate market uncertainties more effectively and position themselves for long-term success.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Historical vs. Forecasted Data**:\n",
    "   - The forecasted sales totals (red) are more stable and less variable compared to the historical data (cyan).\n",
    "   - The historical data shows significant fluctuations, while the forecasted data remains within a narrower range.\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - The most important feature is `lag_7`, followed by other lag features like `lag_1`, `lag_6`, etc.\n",
    "   - Date-related features such as `day_of_week`, `day_of_month`, and `week_of_year` have relatively lower importance.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Stability of Forecast**:\n",
    "   - The Random Forest model predicts more stable sales totals for the next year. This could be due to the model averaging out the historical fluctuations.\n",
    "   \n",
    "2. **Importance of Lag Features**:\n",
    "   - Lag features (e.g., sales from 7 days ago) are crucial for making accurate sales predictions. This suggests that recent sales data significantly impacts future sales.\n",
    "\n",
    "3. **Lower Importance of Date Features**:\n",
    "   - Features like `day_of_week`, `month`, and `quarter` are less important in the model. This implies that the sales patterns are more influenced by recent past sales rather than specific dates or periods.\n",
    "\n",
    "4. **Seasonal Patterns**:\n",
    "   - The historical data shows clear seasonal patterns, which are likely captured by the lag features in the Random Forest model.\n",
    "   \n",
    "5. **Model Reliability**:\n",
    "   - The Random Forest model appears reliable for forecasting sales, given its ability to stabilize predictions and focus on recent trends.\n",
    "\n",
    "6. **Potential Overfitting**:\n",
    "   - The forecast being too stable might suggest some level of overfitting or underfitting. Real-world sales might still experience some variability.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Use Lag Features in Forecasting**:\n",
    "   - Continue using lag features for sales forecasting, as they are shown to be the most important predictors.\n",
    "\n",
    "2. **Monitor Real-World Data**:\n",
    "   - Regularly compare the forecasted sales with actual sales data to ensure the model remains accurate over time. Adjust the model if significant discrepancies are observed.\n",
    "\n",
    "3. **Incorporate Additional Features**:\n",
    "   - Consider incorporating other external factors that might affect sales (e.g., promotions, holidays, economic indicators) to potentially improve model accuracy.\n",
    "\n",
    "4. **Model Updates**:\n",
    "   - Periodically retrain the model with new data to capture any changes in sales patterns and improve forecasting accuracy.\n",
    "\n",
    "5. **Analyze Stability**:\n",
    "   - Investigate why the forecasted sales are so stable. Ensure the model is not smoothing out important fluctuations that might be critical for business operations.\n",
    "\n",
    "6. **Business Planning**:\n",
    "   - Use the stable forecast to plan inventory, staffing, and other operational needs. The forecast can help ensure resources are allocated efficiently.\n",
    "\n",
    "7. **Refinement**:\n",
    "   - Fine-tune the Random Forest model's hyperparameters to ensure optimal performance.\n",
    "   - Explore other models or ensemble methods to see if they can capture variability better while maintaining accuracy.\n",
    "\n",
    "8. **Visualization**:\n",
    "   - Create more detailed visualizations to compare forecasted vs. actual sales over different periods to better understand model performance.\n",
    "\n",
    "9. **Stakeholder Communication**:\n",
    "   - Communicate the forecast and its implications to stakeholders to inform strategic decisions and planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.5. Use the item count data from the last six months as the testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate data to daily level\n",
    "daily_item_count = df_preprocessed.groupby('date')['item_count'].sum().reset_index()\n",
    "\n",
    "# Create lag features\n",
    "for i in range(1, 8):  # Create 7 lag features\n",
    "    daily_item_count[f'lag_{i}'] = daily_item_count['item_count'].shift(i)\n",
    "\n",
    "# Drop rows with NaN values after creating lag features\n",
    "daily_item_count = daily_item_count.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X = daily_item_count[['date'] + [f'lag_{i}' for i in range(1, 8)]]\n",
    "y = daily_item_count['item_count']\n",
    "\n",
    "# Extract additional date features for daily_item_count\n",
    "X['day_of_week'] = X['date'].dt.dayofweek\n",
    "X['week_of_year'] = X['date'].dt.isocalendar().week\n",
    "X['quarter'] = X['date'].dt.quarter\n",
    "X['month'] = X['date'].dt.month\n",
    "X['year'] = X['date'].dt.year\n",
    "X['day_of_month'] = X['date'].dt.day\n",
    "\n",
    "# Drop the date column as it's no longer needed\n",
    "X = X.drop(columns=['date'])\n",
    "\n",
    "# Split the data - use last 6 months as test set\n",
    "split_date = daily_item_count['date'].max() - timedelta(days=180)\n",
    "X_train = X[daily_item_count['date'] <= split_date]\n",
    "X_test = X[daily_item_count['date'] > split_date]\n",
    "y_train = y[daily_item_count['date'] <= split_date]\n",
    "y_test = y[daily_item_count['date'] > split_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.6. Compute the root mean square error (RMSE) values for each model to compare their performances in item count prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    # Train on the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'model': model, 'cv_rmse': cv_rmse, 'test_rmse': rmse, 'r2': r2}\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse}\")\n",
    "    print(f\"  Cross-validation RMSE: {cv_rmse}\")\n",
    "    print(f\"  Test RMSE: {rmse}\")\n",
    "    print(f\"  R2 Score: {r2}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values. Lower values indicate better model performance.\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**: The square root of MSE, which is in the same units as the target variable. It provides an interpretable measure of prediction error.\n",
    "\n",
    "3. **R2 Score (Coefficient of Determination)**: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. A value closer to 1 indicates a better fit.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "Accurately predicting `item_count` is critical for several reasons, as it impacts various facets of business operations, strategy, and financial planning. Heres why it is important, along with detailed explanations, observations, conclusions, inferences, and recommendations based on the given model outputs:\n",
    "\n",
    "1. **Inventory Management**:\n",
    "   - **Explanation**: Accurate item count forecasts help manage inventory levels effectively.\n",
    "   - **Observation**: Avoiding overstocking or understocking reduces storage costs and minimizes wastage.\n",
    "   - **Conclusion**: Proper inventory management ensures that products are available when needed, improving customer satisfaction.\n",
    "   - **Inference**: Predictable item counts allow for better planning and procurement strategies.\n",
    "   - **Recommendation**: Use the forecast to set reorder points and safety stock levels, ensuring optimal inventory management.\n",
    "\n",
    "2. **Financial Planning**:\n",
    "   - **Explanation**: Forecasts provide a basis for financial planning and budgeting.\n",
    "   - **Observation**: Predicting item counts helps in planning for expenses, investments, and cash flow management.\n",
    "   - **Conclusion**: Reliable forecasts lead to better financial health and the ability to make informed investment decisions.\n",
    "   - **Inference**: Accurate item count projections reduce financial risks.\n",
    "   - **Recommendation**: Integrate item count forecasts into financial models to plan for growth and expansion.\n",
    "\n",
    "3. **Staffing and Resource Allocation**:\n",
    "   - **Explanation**: Knowing future item counts helps in staffing appropriately to meet demand.\n",
    "   - **Observation**: Ensuring the right number of employees are scheduled can improve service levels and reduce labor costs.\n",
    "   - **Conclusion**: Efficient resource allocation enhances operational efficiency.\n",
    "   - **Inference**: Overstaffing or understaffing can be minimized.\n",
    "   - **Recommendation**: Use forecasts to plan shifts, hire temporary staff, or manage training schedules.\n",
    "\n",
    "4. **Marketing and Promotions**:\n",
    "   - **Explanation**: Forecasts inform marketing strategies and promotional activities.\n",
    "   - **Observation**: Identifying periods of high or low item counts can guide when to run promotions or marketing campaigns.\n",
    "   - **Conclusion**: Targeted marketing efforts increase ROI and drive item counts.\n",
    "   - **Inference**: Data-driven marketing is more effective.\n",
    "   - **Recommendation**: Align marketing budgets and campaigns with forecasted item count trends.\n",
    "\n",
    "5. **Customer Satisfaction**:\n",
    "   - **Explanation**: Meeting customer demand consistently improves satisfaction and loyalty.\n",
    "   - **Observation**: Accurate forecasts ensure product availability, reducing the likelihood of stockouts.\n",
    "   - **Conclusion**: Happy customers are more likely to return and recommend the business.\n",
    "   - **Inference**: Item count forecasts are directly linked to customer experience.\n",
    "   - **Recommendation**: Use forecasts to maintain service levels and product availability.\n",
    "\n",
    "6. **Operational Efficiency**:\n",
    "   - **Explanation**: Forecasting helps streamline operations and reduce inefficiencies.\n",
    "   - **Observation**: Businesses can plan production schedules, logistics, and supply chain activities more effectively.\n",
    "   - **Conclusion**: Improved operational efficiency leads to cost savings and better margins.\n",
    "   - **Inference**: Predictable operations reduce the chances of last-minute adjustments and disruptions.\n",
    "   - **Recommendation**: Incorporate forecasts into operational planning and scheduling tools.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - MSE: 7.28\n",
    "   - Cross-validation RMSE: 2.44\n",
    "   - Test RMSE: 2.70\n",
    "   - R2 Score: 0.66\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - MSE: 6.73\n",
    "   - Cross-validation RMSE: 2.32\n",
    "   - Test RMSE: 2.59\n",
    "   - R2 Score: 0.69\n",
    "\n",
    "3. **XGBoost**:\n",
    "   - MSE: 7.15\n",
    "   - Cross-validation RMSE: 2.51\n",
    "   - Test RMSE: 2.67\n",
    "   - R2 Score: 0.67\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - **Random Forest**: Exhibits the lowest MSE and RMSE, and the highest R2 Score, indicating it performs the best among the three models.\n",
    "   - **XGBoost**: Performs slightly better than Linear Regression but is not as effective as Random Forest.\n",
    "   - **Linear Regression**: While performing reasonably well, it is outperformed by both Random Forest and XGBoost.\n",
    "\n",
    "2. **Consistency**:\n",
    "   - All three models show a relatively high R2 Score, indicating that the majority of the variance in `item_count` can be explained by the features used.\n",
    "\n",
    "3. **Importance of Non-linear Relationships**:\n",
    "   - The superior performance of Random Forest suggests that non-linear relationships are important for predicting `item_count`.\n",
    "   - Linear Regressions performance, while good, indicates that it might not be capturing some of the more complex relationships in the data.\n",
    "\n",
    "4. **Robustness and Flexibility**:\n",
    "   - Random Forests ensemble approach appears to provide robustness and flexibility in capturing the patterns in the data.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Adopt Random Forest for Predictions**:\n",
    "   - Given its performance, Random Forest should be adopted for predicting `item_count`. It provides the most accurate and reliable predictions among the evaluated models.\n",
    "\n",
    "2. **Further Model Tuning**:\n",
    "   - Consider hyperparameter tuning for Random Forest and XGBoost to further enhance their performance. Techniques such as Grid Search or Random Search can be employed to find optimal parameters.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Explore additional feature engineering to capture more nuanced patterns in the data. This could include interaction terms, polynomial features, or external factors like promotions, holidays, or weather conditions.\n",
    "\n",
    "4. **Regular Model Updates**:\n",
    "   - Regularly update the model with new data to ensure it adapts to any changes in item count patterns over time.\n",
    "\n",
    "5. **Cross-validation Strategy**:\n",
    "   - Continue using robust cross-validation strategies to ensure the models performance generalizes well to unseen data.\n",
    "\n",
    "6. **Scenario Analysis**:\n",
    "   - Use the model to run various scenarios (e.g., changes in marketing spend, introduction of new products) to understand their potential impact on item count.\n",
    "\n",
    "7. **Operational Integration**:\n",
    "   - Integrate the model into business operations for inventory management, staffing, and resource allocation to optimize efficiency based on predicted item counts.\n",
    "\n",
    "8. **Visualization and Reporting**:\n",
    "   - Create detailed visualizations to compare actual vs. predicted item counts over time to provide actionable insights to stakeholders.\n",
    "   - Generate reports summarizing model performance and forecasts to inform decision-making processes.\n",
    "\n",
    "9. **Exploring Additional Models**:\n",
    "   - While Random Forest performs well, exploring other advanced models like LightGBM or neural networks might yield even better results, especially with more complex patterns.\n",
    "\n",
    "10. **Real-time Forecasting**:\n",
    "    - Consider implementing real-time forecasting capabilities to adjust predictions dynamically based on new incoming data, enhancing responsiveness to changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3.1.7. Use the best-performing models to make an item count forecast for the next year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast for the next year using the best model (Random Forest)\n",
    "best_model = models['Random Forest']\n",
    "\n",
    "# Create a DataFrame for the forecast\n",
    "forecast_dates = pd.date_range(start=daily_item_count['date'].max() + timedelta(days=1), periods=365, freq='D')\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates})\n",
    "\n",
    "# Generate lag features for the forecast\n",
    "for i in range(1, 8):\n",
    "    forecast_df[f'lag_{i}'] = daily_item_count['item_count'].shift(i).iloc[-365:].values\n",
    "\n",
    "# Extract additional date features for the forecast\n",
    "forecast_df['day_of_week'] = forecast_df['date'].dt.dayofweek\n",
    "forecast_df['week_of_year'] = forecast_df['date'].dt.isocalendar().week\n",
    "forecast_df['quarter'] = forecast_df['date'].dt.quarter\n",
    "forecast_df['month'] = forecast_df['date'].dt.month\n",
    "forecast_df['year'] = forecast_df['date'].dt.year\n",
    "forecast_df['day_of_month'] = forecast_df['date'].dt.day\n",
    "\n",
    "# Prepare forecast features (drop date)\n",
    "forecast_X = forecast_df.drop(columns=['date'])\n",
    "\n",
    "# Predict the item count for the next year\n",
    "forecast_df['forecast'] = best_model.predict(forecast_X)\n",
    "\n",
    "# Plot the historical and forecasted item counts\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(daily_item_count['date'], daily_item_count['item_count'], label='Historical Data')\n",
    "plt.plot(forecast_df['date'], forecast_df['forecast'], label='Forecast', color='red')\n",
    "plt.title('Item Count Forecast for Next Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Item Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display forecast\n",
    "print(forecast_df[['date', 'forecast']])\n",
    "\n",
    "# Feature importance\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'pink', 'gray']\n",
    "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'], color=colors)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.show()\n",
    "\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. **Item Count Forecast Plot**:\n",
    "   - The plot shows historical item count data (in cyan) and the forecasted item counts (in red) for the next year.\n",
    "   - The historical data includes fluctuations over time, reflecting seasonality and possible trends.\n",
    "\n",
    "2. **Forecast Data**:\n",
    "   - The forecast data is shown for each day of the next year, indicating the predicted item counts.\n",
    "\n",
    "3. **Feature Importance Plot**:\n",
    "   - This bar plot displays the importance of each feature used in the Random Forest model. Higher bars indicate more important features for the model.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "Accurately predicting item counts is crucial for a variety of reasons that impact business operations, financial performance, and customer satisfaction. Here are some detailed explanations, observations, conclusions, inferences, and recommendations based on the importance of accurate item count forecasting:\n",
    "\n",
    "1. **Inventory Management**:\n",
    "   - **Explanation**: Accurate item count forecasts enable businesses to manage inventory levels effectively.\n",
    "   - **Observation**: Overstocking or understocking can be minimized, reducing storage costs and avoiding wastage.\n",
    "   - **Conclusion**: Efficient inventory management ensures product availability, meeting customer demand without excessive holding costs.\n",
    "   - **Inference**: Better inventory planning leads to reduced operational costs and increased profitability.\n",
    "   - **Recommendation**: Use item count forecasts to set reorder points and maintain optimal stock levels.\n",
    "\n",
    "2. **Financial Planning**:\n",
    "   - **Explanation**: Forecasting item counts aids in precise financial planning and budgeting.\n",
    "   - **Observation**: Predicting item counts helps in forecasting revenue, planning expenses, and managing cash flow.\n",
    "   - **Conclusion**: Accurate forecasts improve financial health and allow for informed investment decisions.\n",
    "   - **Inference**: Predictable item counts reduce financial uncertainty and risks.\n",
    "   - **Recommendation**: Integrate item count forecasts into financial models to plan for growth and mitigate risks.\n",
    "\n",
    "3. **Staffing and Resource Allocation**:\n",
    "   - **Explanation**: Anticipating item counts helps in planning appropriate staffing levels to meet demand.\n",
    "   - **Observation**: Ensuring the right number of employees are scheduled improves service levels and reduces labor costs.\n",
    "   - **Conclusion**: Optimal resource allocation enhances operational efficiency and customer service.\n",
    "   - **Inference**: Accurate forecasts prevent overstaffing or understaffing, leading to cost savings.\n",
    "   - **Recommendation**: Use item count forecasts to plan shifts and allocate resources effectively.\n",
    "\n",
    "4. **Marketing and Promotions**:\n",
    "   - **Explanation**: Forecasting informs marketing strategies and promotional activities.\n",
    "   - **Observation**: Identifying periods of high or low item counts helps plan promotions and marketing campaigns effectively.\n",
    "   - **Conclusion**: Targeted marketing efforts increase ROI and drive item counts.\n",
    "   - **Inference**: Data-driven marketing strategies are more effective and efficient.\n",
    "   - **Recommendation**: Align marketing budgets and campaigns with forecasted item count trends.\n",
    "\n",
    "5. **Customer Satisfaction**:\n",
    "   - **Explanation**: Meeting customer demand consistently improves satisfaction and loyalty.\n",
    "   - **Observation**: Accurate forecasts ensure product availability, reducing stockouts and backorders.\n",
    "   - **Conclusion**: Satisfied customers are more likely to return and recommend the business.\n",
    "   - **Inference**: Reliable item count forecasts are directly linked to enhanced customer experiences.\n",
    "   - **Recommendation**: Use forecasts to maintain high service levels and ensure product availability.\n",
    "\n",
    "6. **Operational Efficiency**:\n",
    "   - **Explanation**: Forecasting helps streamline operations and reduce inefficiencies.\n",
    "   - **Observation**: Businesses can plan production schedules, logistics, and supply chain activities more effectively.\n",
    "   - **Conclusion**: Improved operational efficiency leads to cost savings and better margins.\n",
    "   - **Inference**: Predictable operations reduce the likelihood of last-minute adjustments and disruptions.\n",
    "   - **Recommendation**: Incorporate item count forecasts into operational planning and scheduling tools.\n",
    "\n",
    "Accurately forecasting item counts using models like Random Forest provides a competitive edge by enabling better planning, resource allocation, and strategic decision-making. The insights gained from forecasting allow businesses to optimize operations, improve financial health, enhance customer satisfaction, and drive growth. Understanding and leveraging the importance of item count forecasting ensures that businesses can navigate market uncertainties more effectively and position themselves for long-term success.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Historical vs. Forecasted Data**:\n",
    "   - The forecasted item counts (red) exhibit a clear seasonal pattern and seem to capture the cyclical nature observed in the historical data (cyan).\n",
    "   - The forecast data shows fluctuations similar to the historical patterns, indicating the model's ability to capture seasonality.\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - The most important feature by a large margin is `lag_7`, followed by other lag features like `lag_1`, `lag_6`, etc.\n",
    "   - Date-related features such as `quarter`, `year`, `month`, and `day_of_week` have relatively lower importance.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - Random Forest effectively captures the seasonality and trends in the item count data, as evidenced by the forecast plot and the use of lag features.\n",
    "\n",
    "2. **Reliance on Recent Data**:\n",
    "   - The heavy reliance on lag features (`lag_7`, `lag_1`, `lag_6`) indicates that recent item counts are the most significant predictors for future item counts.\n",
    "   - Date-related features, although less important, still contribute to the model, suggesting that temporal aspects do play a role, albeit smaller.\n",
    "\n",
    "### Inferences\n",
    "\n",
    "1. **Seasonal Patterns**:\n",
    "   - The model captures seasonal patterns effectively, as seen in the forecast plot where periodic fluctuations are evident.\n",
    "   \n",
    "2. **Predictive Power of Lag Features**:\n",
    "   - Recent past item counts are strong predictors of future item counts, highlighting the importance of short-term historical data.\n",
    "\n",
    "3. **Lower Importance of Date Features**:\n",
    "   - While date features like `quarter`, `month`, and `day_of_week` contribute to the model, their lower importance suggests that the item count trends are more influenced by recent activity than by specific dates.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Operational Use of Forecasts**:\n",
    "   - Utilize the item count forecasts for inventory management, ensuring that stock levels are optimized to meet predicted demand.\n",
    "   - Adjust staffing and resource allocation based on forecasted item counts to improve operational efficiency and customer satisfaction.\n",
    "\n",
    "2. **Focus on Recent Data**:\n",
    "   - Given the high importance of lag features, continue to emphasize the use of recent item count data in forecasting models.\n",
    "   - Regularly update the model with the most recent data to maintain its predictive accuracy.\n",
    "\n",
    "3. **Explore Additional Lag Features**:\n",
    "   - Consider experimenting with more lag features or different lag intervals to see if they can further enhance model performance.\n",
    "\n",
    "4. **Monitor and Update**:\n",
    "   - Continuously monitor the model's performance and update it with new data to adapt to any changes in item count patterns.\n",
    "   - Implement a feedback loop where actual item counts are compared to forecasts to refine the model.\n",
    "\n",
    "5. **Scenario Planning**:\n",
    "   - Use the model to run different scenarios (e.g., promotional events, seasonal changes) to understand their potential impact on item counts.\n",
    "   - Leverage these insights for strategic planning and decision-making.\n",
    "\n",
    "6. **Communication with Stakeholders**:\n",
    "   - Clearly communicate the forecasts and their implications to stakeholders, ensuring they understand the expected item count trends and can plan accordingly.\n",
    "\n",
    "7. **Refinement**:\n",
    "   - Fine-tune the Random Forest model's hyperparameters to ensure optimal performance.\n",
    "   - Explore other advanced models like LightGBM or neural networks to see if they can provide additional improvements.\n",
    "\n",
    "8. **Visualization and Reporting**:\n",
    "   - Create detailed visualizations to compare actual vs. predicted item counts over different periods.\n",
    "   - Generate comprehensive reports summarizing model performance and forecasts for stakeholder review.\n",
    "\n",
    "9. **Integration with Business Processes**:\n",
    "   - Integrate the forecasting model with business systems to automate inventory and staffing decisions based on predicted item counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Forecasting using deep learning algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4.1. Use sales amount for predictions instead of item count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily sales amount\n",
    "daily_sales = df_preprocessed.groupby('date').agg({'item_count': 'sum', 'price': 'mean'}).reset_index()\n",
    "daily_sales['sales_amount'] = daily_sales['item_count'] * daily_sales['price']\n",
    "\n",
    "# Sort the data by date\n",
    "daily_sales = daily_sales.sort_values('date')\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(daily_sales['sales_amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Create the normalized DataFrame\n",
    "daily_sales['scaled_sales_amount'] = scaled_sales\n",
    "\n",
    "daily_sales.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4.2. Build a long short-term memory (LSTM) model for predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.4.2.1. Define the train and test series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test series\n",
    "train_size = int(len(daily_sales) * 0.8)\n",
    "train, test = daily_sales[:train_size], daily_sales[train_size:]\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        label = data[i+seq_length]\n",
    "        sequences.append((seq, label))\n",
    "    return sequences\n",
    "\n",
    "seq_length = 30\n",
    "train_sequences = create_sequences(train['scaled_sales_amount'].values, seq_length)\n",
    "test_sequences = create_sequences(test['scaled_sales_amount'].values, seq_length)\n",
    "\n",
    "# Split sequences into X and y\n",
    "X_train, y_train = zip(*train_sequences)\n",
    "X_test, y_test = zip(*test_sequences)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.4.2.2. Generate synthetic data for the last 12 months**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we want to generate synthetic data for the last 12 months (365 days)\n",
    "num_synthetic_days = 365\n",
    "synthetic_data = resample(daily_sales[-num_synthetic_days:], n_samples=num_synthetic_days, replace=True)\n",
    "\n",
    "# Add synthetic data to the training set\n",
    "train_with_synthetic = pd.concat([train, synthetic_data]).sort_values('date')\n",
    "\n",
    "# Normalize the synthetic data\n",
    "synthetic_scaled_sales = scaler.fit_transform(train_with_synthetic['sales_amount'].values.reshape(-1, 1))\n",
    "train_with_synthetic['scaled_sales_amount'] = synthetic_scaled_sales\n",
    "\n",
    "# Create sequences with synthetic data\n",
    "train_synthetic_sequences = create_sequences(train_with_synthetic['scaled_sales_amount'].values, seq_length)\n",
    "\n",
    "# Split sequences into X and y\n",
    "X_train_synthetic, y_train_synthetic = zip(*train_synthetic_sequences)\n",
    "\n",
    "X_train_synthetic = np.array(X_train_synthetic)\n",
    "y_train_synthetic = np.array(y_train_synthetic)\n",
    "\n",
    "X_train_synthetic.shape, y_train_synthetic.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.4.2.3. Build and train an LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress additional warnings and info messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(units=50, dropout_rate=0.2, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(seq_length, 1)))\n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=units, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasRegressor\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_dist = {\n",
    "    'model__units': [50, 100, 150],\n",
    "    'model__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
    "random_search_result = random_search.fit(X_train_synthetic, y_train_synthetic)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search_result.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print()\n",
    "print('________________________________________________________')\n",
    "print()\n",
    "print(\"BEST HYPERPARAMETERS:\", random_search_result.best_params_)\n",
    "print()\n",
    "print('________________________________________________________')\n",
    "print()\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = best_model.fit(X_train_synthetic, y_train_synthetic, validation_split=0.1, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history_['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history_['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "An LSTM (Long Short-Term Memory) model is a type of recurrent neural network (RNN) designed to model sequential data and capture long-term dependencies. In this analysis, the LSTM model is employed to forecast daily sales amounts based on historical data. The model architecture includes multiple LSTM layers, dropout layers for regularization, and a dense layer for output. Key steps include data normalization, sequence creation, synthetic data generation, hyperparameter tuning, and training with early stopping.\n",
    "\n",
    "1. **Model Structure**:\n",
    "   - The model consists of an LSTM layer with 50 units, followed by a dropout layer (0.2 dropout rate), another LSTM layer with 50 units, another dropout layer, and finally a dense layer.\n",
    "   - The model is compiled using the Adam optimizer and mean squared error loss function, with accuracy as the metric for monitoring training progress.\n",
    "\n",
    "2. **Training Results**:\n",
    "   - The model was trained for 50 epochs with a batch size of 32 and a validation split of 0.2.\n",
    "   - The training and validation loss are plotted to monitor the model's performance over epochs.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- **Hyperparameter Tuning**: Crucial for optimizing model performance. Techniques like RandomizedSearchCV enable systematic exploration of hyperparameters.\n",
    "- **Early Stopping**: Prevents overfitting by halting training when validation performance stops improving.\n",
    "- **Synthetic Data**: Augments the training set, enhancing the model's ability to generalize.\n",
    "- **Normalization**: Essential for ensuring consistent input feature scales, aiding in model convergence.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Training and Validation Loss**:\n",
    "   - The training loss decreases steadily, indicating that the model is learning and fitting the training data well.\n",
    "   - The validation loss fluctuates and remains higher than the training loss throughout the epochs.\n",
    "\n",
    "2. **Validation Loss Behavior**:\n",
    "   - The validation loss shows significant variance, suggesting potential overfitting. The model performs well on training data but struggles with unseen data.\n",
    "\n",
    "3. **Hyperparameters**:\n",
    "   - The best hyperparameters identified are: `{'model__units': 50, 'model__optimizer': 'adam', 'model__dropout_rate': 0.4, 'epochs': 150, 'batch_size': 32}`.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- The model effectively learns from the training data, but the higher validation loss indicates overfitting.\n",
    "- The model's generalization ability to unseen data is suboptimal, as evidenced by the higher and fluctuating validation loss.\n",
    "- The LSTM model benefits from the use of synthetic data in the training set, but further improvements are necessary to enhance its generalization capability.\n",
    "- Additional regularization or tuning might be required to stabilize the validation loss and improve model performance.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Further Hyperparameter Tuning**:\n",
    "   - Expand the range of hyperparameters and use Grid Search for more exhaustive tuning.\n",
    "\n",
    "2. **Additional Regularization Techniques**:\n",
    "   - Incorporate L2 regularization or increase dropout rates to reduce overfitting.\n",
    "\n",
    "3. **Data Augmentation**:\n",
    "   - Generate more synthetic data or apply techniques like SMOTE to balance the dataset further.\n",
    "\n",
    "4. **Model Architecture**:\n",
    "   - Experiment with deeper LSTM layers or more complex architectures to capture intricate patterns in the data.\n",
    "\n",
    "5. **Feature Engineering**:\n",
    "   - Add more features such as moving averages, lag features, or external factors like holidays to provide more context to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.4.2.4. Use the model to make predictions for the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "predicted_sales = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "actual_sales = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs actual sales\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sales['date'][train_size+seq_length:], actual_sales, label='Actual Sales', color='green')\n",
    "plt.plot(daily_sales['date'][train_size+seq_length:], predicted_sales, label='Predicted Sales', color='red')\n",
    "plt.title('Sales Predictions vs Actual Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Amount')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "**Data Preparation and Normalization**\n",
    "1. **Calculating Daily Sales Amount**:\n",
    "   - The daily sales amount is calculated by multiplying the item count by the price.\n",
    "2. **Data Sorting**:\n",
    "   - The data is sorted by date to ensure temporal sequence.\n",
    "3. **Normalization**:\n",
    "   - MinMaxScaler is used to scale the sales amount between 0 and 1 for better training performance.\n",
    "\n",
    "**Sequence Creation**\n",
    "1. **Train-Test Split**:\n",
    "   - The data is split into training and test sets (80%-20%).\n",
    "2. **Sequence Generation**:\n",
    "   - Sequences of 30 days are created for both training and test sets to capture temporal dependencies.\n",
    "\n",
    "**Synthetic Data Generation**\n",
    "6. **Synthetic Data**:\n",
    "   - Synthetic data for the last 12 months is generated and added to the training set to increase data variability and size.\n",
    "7. **Normalization of Synthetic Data**:\n",
    "   - The synthetic data is normalized in the same manner as the original data.\n",
    "\n",
    "**Model Architecture and Training**\n",
    "8. **LSTM Model Definition**:\n",
    "   - A Sequential LSTM model is defined with three LSTM layers, dropout layers to prevent overfitting, and a dense output layer.\n",
    "9. **Hyperparameter Tuning**:\n",
    "   - RandomizedSearchCV is used to find the best hyperparameters.\n",
    "10. **Early Stopping**:\n",
    "    - Early stopping is used during training to prevent overfitting by monitoring the validation loss.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- **Data Preparation**:\n",
    "  - Proper normalization and sequence generation are crucial for training effective deep learning models.\n",
    "- **Model Architecture**:\n",
    "  - Defining an appropriate model architecture is critical for capturing the underlying patterns in the data.\n",
    "- **Hyperparameter Tuning**:\n",
    "  - Systematic tuning of hyperparameters significantly impacts the model's performance.\n",
    "- **Model Evaluation**:\n",
    "  - Evaluating the model with appropriate metrics and visualizations helps in understanding its strengths and weaknesses.\n",
    "\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Sales Prediction vs Actual Sales Plot**:\n",
    "   - The plot shows a comparison between the actual sales and the predicted sales.\n",
    "   - The predicted sales (red line) have a smoother pattern compared to the actual sales (green line), which shows high volatility.\n",
    "2. **Model Performance**:\n",
    "   - The model captures the overall trend of the sales but fails to capture the high-frequency fluctuations.\n",
    "3. **Prediction Accuracy**:\n",
    "   - The predicted values tend to hover around a central trend and do not capture extreme variations\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Trend Capturing**:\n",
    "   - The LSTM model is effective in capturing the general trend of the sales data.\n",
    "2. **Volatility**:\n",
    "   - The model struggles with capturing the high volatility and extreme values in the sales data.\n",
    "3. **Overfitting**:\n",
    "   - Early stopping and dropout layers help in preventing overfitting, but further tuning might be needed to capture more complex patterns.\n",
    "4. **Model Sufficiency**:\n",
    "   - While the model is good for understanding the overall trend, it may not be suitable for applications requiring precise prediction of daily sales.\n",
    "5. **Data Complexity**:\n",
    "   - The high volatility in the actual sales data indicates that the model might benefit from additional features or a more complex architecture.\n",
    "6. **Synthetic Data Impact**:\n",
    "   - Adding synthetic data helps in increasing the training data size, which is beneficial, but it might not add much value if the synthetic data does not capture the variability present in the actual data.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Introduce additional features such as promotions, holidays, and other external factors that might affect sales.\n",
    "2. **Model Complexity**:\n",
    "   - Experiment with deeper architectures or ensemble methods to capture more complex patterns.\n",
    "3. **Hyperparameter Optimization**:\n",
    "   - Use more sophisticated techniques like Bayesian Optimization for hyperparameter tuning.\n",
    "4. **Data Augmentation**:\n",
    "   - Use advanced data augmentation techniques to generate more realistic synthetic data.\n",
    "5. **Additional Metrics**:\n",
    "   - Evaluate the model using additional metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to get a comprehensive understanding of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((actual_sales - predicted_sales) / actual_sales)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "\n",
    "**Model Evaluation**\n",
    "1.  **Mean Absolute Percentage Error (MAPE)**:\n",
    "    - MAPE is calculated to evaluate the model's performance. The formula used is:\n",
    "      \\[\n",
    "      \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y_i}}{y_i} \\right| \\times 100\n",
    "      \\]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- **Data Preparation**:\n",
    "  - Proper normalization and sequence generation are crucial for training effective deep learning models.\n",
    "- **Model Architecture**:\n",
    "  - Defining an appropriate model architecture is critical for capturing the underlying patterns in the data.\n",
    "- **Hyperparameter Tuning**:\n",
    "  - Systematic tuning of hyperparameters significantly impacts the model's performance.\n",
    "- **Model Evaluation**:\n",
    "  - Evaluating the model with appropriate metrics and visualizations helps in understanding its strengths and weaknesses.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **MAPE Value**:\n",
    "   - The MAPE value is 185.97%, which indicates a high error rate between the predicted and actual sales values.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Trend Capturing**:\n",
    "   - The LSTM model is effective in capturing the general trend of the sales data but fails to accurately predict the exact sales amounts.\n",
    "2. **Volatility**:\n",
    "   - The model struggles with capturing the high volatility and extreme values in the sales data, as evidenced by the high MAPE value.\n",
    "3. **Model Performance**:\n",
    "   - The high MAPE indicates that the model is not performing well in terms of predicting sales accurately. The error margin is too high for practical purposes.\n",
    "4. **Model Sufficiency**:\n",
    "   - While the model is good for understanding the overall trend, it is not suitable for applications requiring precise prediction of daily sales.\n",
    "5. **Data Complexity**:\n",
    "   - The high volatility in the actual sales data indicates that the model might benefit from additional features or a more complex architecture.\n",
    "6. **Synthetic Data Impact**:\n",
    "   - Adding synthetic data helps in increasing the training data size, which is beneficial, but it might not add much value if the synthetic data does not capture the variability present in the actual data.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Introduce additional features such as promotions, holidays, and other external factors that might affect sales.\n",
    "2. **Model Complexity**:\n",
    "   - Experiment with deeper architectures or ensemble methods to capture more complex patterns.\n",
    "3. **Hyperparameter Optimization**:\n",
    "   - Use more sophisticated techniques like Bayesian Optimization for hyperparameter tuning.\n",
    "4. **Data Augmentation**:\n",
    "   - Use advanced data augmentation techniques to generate more realistic synthetic data.\n",
    "5. **Additional Metrics**:\n",
    "   - Evaluate the model using additional metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to get a comprehensive understanding of performance.\n",
    "6. **Fine-Tuning**:\n",
    "   - Perform additional fine-tuning of the model parameters and architecture to improve its ability to capture the high volatility in the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for binary classification\n",
    "# Assuming y_test contains actual sales and y_pred contains predicted sales amounts\n",
    "# Let's convert these continuous values to binary classification for simplicity\n",
    "threshold = np.median(actual_sales)\n",
    "y_test_binary = (actual_sales > threshold).astype(int)\n",
    "predicted_sales_binary = (predicted_sales > threshold).astype(int)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_binary, predicted_sales_binary)\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, predicted_sales_binary)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_binary, predicted_sales_binary)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Sensitivity (Recall) calculation\n",
    "tp = cm[1, 1]\n",
    "fn = cm[1, 0]\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The evaluation of the LSTM model involves analyzing the ROC AUC metrics, sensitivity (recall), and confusion matrix to understand the model's performance in binary classification. These metrics help in understanding how well the model is distinguishing between two classes and how accurate its predictions are.\n",
    "\n",
    "1. **ROC AUC Score**\n",
    "   - The ROC AUC score represents the model's ability to distinguish between positive and negative classes. A higher AUC indicates better model performance.\n",
    "\n",
    "2. **Sensitivity (Recall)**\n",
    "   - Sensitivity measures the proportion of actual positives that are correctly identified by the model. A higher sensitivity indicates that the model is good at identifying positive cases.\n",
    "\n",
    "3. **Confusion Matrix**\n",
    "- The confusion matrix provides a detailed breakdown of true positives, false positives, true negatives, and false negatives, giving insights into the model's classification accuracy.\n",
    "\n",
    "**Observations**\n",
    "\n",
    "1. **ROC AUC Score**:\n",
    "   - The ROC AUC score is 0.66, which indicates a moderate ability of the model to distinguish between positive and negative classes. This score suggests that the model's performance is better than random guessing but still has room for improvement.\n",
    "\n",
    "2. **Sensitivity (Recall)**:\n",
    "   - The sensitivity is 0.77, meaning that 77% of actual positive cases are correctly identified by the model. This indicates that the model is reasonably good at identifying positive cases.\n",
    "\n",
    "3. **Confusion Matrix**:\n",
    "   - True Positives (TP): 73\n",
    "   - True Negatives (TN): 63\n",
    "   - False Positives (FP): 42\n",
    "   - False Negatives (FN): 22\n",
    "   - The confusion matrix shows that the model has a higher number of true positives and true negatives, but there are also a significant number of false positives and false negatives.\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - The ROC AUC score of 0.66 indicates that the model has a moderate discriminatory power. While it performs better than random guessing, there is still significant room for improvement.\n",
    "   - The sensitivity of 0.77 shows that the model is fairly good at identifying positive cases but needs improvement to reduce false negatives.\n",
    "   - The confusion matrix highlights that while the model has correctly identified a good number of true positives and true negatives, it also has a notable number of false positives and false negatives.\n",
    "\n",
    "2. **Model Strengths**:\n",
    "   - The model demonstrates a reasonable ability to identify positive cases, as shown by the sensitivity score.\n",
    "   - The confusion matrix indicates that the model has learned some patterns to distinguish between classes.\n",
    "\n",
    "3. **Model Weaknesses**:\n",
    "   - The ROC AUC score and the confusion matrix suggest that the model struggles with correctly classifying some cases, leading to both false positives and false negatives.\n",
    "   - The moderate ROC AUC score indicates that the model's overall classification performance needs to be improved.\n",
    "\n",
    "**Why It Is Important**\n",
    "\n",
    "Understanding and evaluating model performance using metrics like ROC AUC, sensitivity, and the confusion matrix is crucial for the following reasons:\n",
    "\n",
    "1. **Model Validation**:\n",
    "   - These metrics help validate the model's effectiveness in real-world scenarios.\n",
    "   - They provide insights into the model's strengths and weaknesses, guiding further optimization efforts.\n",
    "\n",
    "2. **Decision Making**:\n",
    "   - Accurate performance metrics are essential for making informed decisions about model deployment and areas requiring improvement.\n",
    "\n",
    "3. **Model Improvement**:\n",
    "   - Identifying areas of weakness, such as false positives and false negatives, helps in refining the model architecture and training process.\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Incorporate additional features that may capture more relevant patterns in the data.\n",
    "   - Consider domain-specific knowledge to enhance feature selection and engineering.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Further optimize hyperparameters using techniques like grid search or Bayesian optimization.\n",
    "   - Experiment with different model architectures and regularization techniques.\n",
    "\n",
    "3. **Addressing Class Imbalance**:\n",
    "   - Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.\n",
    "   - Adjust class weights during training to mitigate the impact of class imbalance.\n",
    "\n",
    "4. **Model Complexity**:\n",
    "   - Increase model complexity by adding more layers or units to capture complex patterns.\n",
    "   - Experiment with different types of layers, such as convolutional layers, to enhance feature extraction.\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "   - Implement cross-validation to ensure the model's robustness and generalizability.\n",
    "   - Use k-fold cross-validation to evaluate model performance on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire series for training\n",
    "full_sequences = create_sequences(daily_sales['scaled_sales_amount'].values, seq_length)\n",
    "X_full, y_full = zip(*full_sequences)\n",
    "X_full = np.array(X_full)\n",
    "y_full = np.array(y_full)\n",
    "\n",
    "# Re-build and train the LSTM model on the entire series with best hyperparameters\n",
    "model_full = Sequential()\n",
    "model_full.add(LSTM(units=random_search_result.best_params_['model__units'], return_sequences=True, input_shape=(seq_length, 1)))\n",
    "model_full.add(Dropout(random_search_result.best_params_['model__dropout_rate']))\n",
    "model_full.add(LSTM(units=random_search_result.best_params_['model__units'], return_sequences=False))\n",
    "model_full.add(Dropout(random_search_result.best_params_['model__dropout_rate']))\n",
    "model_full.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model_full.compile(optimizer=random_search_result.best_params_['model__optimizer'], loss='mean_squared_error')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping_full = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_full = model_full.fit(X_full, y_full, epochs=random_search_result.best_params_['epochs'], batch_size=random_search_result.best_params_['batch_size'], callbacks=[early_stopping_full])\n",
    "\n",
    "# Forecast for the next three months\n",
    "forecast_period = 90\n",
    "forecast = []\n",
    "\n",
    "last_sequence = X_full[-1]\n",
    "\n",
    "for _ in range(forecast_period):\n",
    "    pred = model_full.predict(last_sequence.reshape(1, seq_length, 1))\n",
    "    forecast.append(pred[0, 0])\n",
    "    last_sequence = np.append(last_sequence[1:], pred)\n",
    "\n",
    "# Inverse transform the forecast\n",
    "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
    "\n",
    "# Create a DataFrame for the forecasted values\n",
    "forecast_dates = pd.date_range(start=daily_sales['date'].iloc[-1] + pd.Timedelta(days=1), periods=forecast_period)\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast_sales_amount': forecast.reshape(-1)})\n",
    "\n",
    "# Plot the forecasted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sales['date'], daily_sales['sales_amount'], label='Historical Sales', color='blue')\n",
    "plt.plot(forecast_df['date'], forecast_df['forecast_sales_amount'], label='Forecasted Sales', color='orange')\n",
    "plt.title('Sales Forecast for the Next Three Months')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Amount')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_full.history['loss'], label='Training Loss', color='blue')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations:**\n",
    "\n",
    "1. The code develops a new Sequential model using the entire available time series data for training.\n",
    "2. The model architecture includes LSTM layers with dropout for regularization.\n",
    "3. Early stopping is implemented to prevent overfitting.\n",
    "4. The model forecasts sales for the next three months (90 days).\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "1. **Data Utilization:** Using the full series allows the model to learn from all available historical patterns, potentially improving its ability to capture long-term trends and seasonality.\n",
    "\n",
    "2. **Overfitting Prevention:** The use of dropout and early stopping helps prevent overfitting, which is crucial when training on a larger dataset.\n",
    "\n",
    "3. **Model Complexity:** The ability to handle a larger dataset without overfitting suggests the model's complexity is appropriate for the task.\n",
    "\n",
    "4. **Forecast Stability:** The smoother forecast from this model may provide more reliable long-term predictions, which can be valuable for strategic planning.\n",
    "\n",
    "5. **Performance Benchmark:** This full-series model serves as a benchmark for comparing with other modeling approaches or shorter training periods.\n",
    "\n",
    "6. **Adaptation to Data Volume:** Successfully training on the full series demonstrates the model's scalability and ability to handle larger datasets, which is crucial in many real-world applications.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Sales Forecast Plot:**\n",
    "   - Historical sales data shows high volatility and seasonality.\n",
    "   - The forecasted sales (in orange) appear more stable compared to historical fluctuations.\n",
    "   - The forecast predicts a slight upward trend for the next three months.\n",
    "\n",
    "2. **Training Loss Plot:**\n",
    "   - The training loss decreases rapidly in the first few epochs.\n",
    "   - It continues to decline gradually, stabilizing around 0.01 after about 60 epochs.\n",
    "   - The final loss value is approximately 0.01, indicating a good fit to the training data.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. The model has learned to capture the overall trend and some seasonal patterns in the sales data.\n",
    "2. The smoother forecast suggests the model is averaging out some of the historical volatility.\n",
    "3. The low and stable training loss indicates the model has converged well on the training data.\n",
    "4. The model expects a moderate increase in sales over the next three months.\n",
    "5. The reduced volatility in the forecast might indicate the model is conservative in its predictions.\n",
    "6. The model appears to have captured long-term trends well but may have smoothed out short-term fluctuations.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. Compare this full-series model with models trained on shorter time frames to assess the impact of using more historical data.\n",
    "2. Implement cross-validation to ensure the model's performance is consistent across different data subsets.\n",
    "3. Consider adding external factors (e.g., economic indicators, marketing spend) to potentially improve forecast accuracy.\n",
    "4. Regularly retrain the model with new data to maintain its relevance and accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
